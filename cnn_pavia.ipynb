{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMNP5XdKzRxXHvI4cK4AOMV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sujith-kumara/CNN-for-Pavia/blob/master/cnn_pavia.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rT9Zl_2PSEKE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c33c9801-1b3f-40a0-c5c4-dc7328de1875"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Source and destination paths\n",
        "source_path = '/content/drive/MyDrive/Pavia'\n",
        "destination_path = '/content/PaviaU'\n",
        "\n",
        "# Use the !cp command to copy the folder\n",
        "!cp -r \"$source_path\" \"$destination_path\"\n",
        "\n",
        "# Verify the copied folder\n",
        "print(f\"The folder '{source_path}' has been copied to '{destination_path}'.\")\n"
      ],
      "metadata": {
        "id": "KdcpoANcTQlG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be21c772-6c10-44bb-9bcc-e68c4370726d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The folder '/content/drive/MyDrive/Pavia' has been copied to '/content/PaviaU'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorboardX"
      ],
      "metadata": {
        "id": "zOJVPjvqXGDr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a3b7d1b-3119-4664-8129-6ec5951c3b67"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (1.25.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (24.0)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (3.20.3)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.6.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import scipy.io\n",
        "import scipy.ndimage\n",
        "import numpy as np\n",
        "from random import shuffle\n",
        "\n",
        "import torch\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn import preprocessing\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torch.optim as optim\n",
        "from datetime import datetime\n",
        "from tensorboardX import SummaryWriter\n"
      ],
      "metadata": {
        "id": "KGOsR5XsTUU0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define global variables\n",
        "PATCH_SIZE = 17  # Patch size\n",
        "OUTPUT_CLASSES = 9  # Output classes (representing different land cover types)\n",
        "TEST_FRAC = 0.50  # Percentage of data used for testing\n",
        "NEW_DATA_PATH = os.path.join(os.getcwd(), \"patch\")  # Data storage path; \"patch\" is the folder name\n"
      ],
      "metadata": {
        "id": "5tF47aCYTfBa"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir patch"
      ],
      "metadata": {
        "id": "UWzSth7hUjJU"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pca(X, k):  # k is the number of features that should be retained\n",
        "    newX = np.reshape(X, (-1, X.shape[2]))\n",
        "    pca = PCA(n_components=k, whiten=True)\n",
        "    newX = pca.fit_transform(newX)\n",
        "    newX = np.reshape(newX, (k, X.shape[0], X.shape[1]))\n",
        "    return newX\n",
        "def pad(X, margin):\n",
        "    newX = np.zeros((X.shape[0], X.shape[1]+margin*2, X.shape[2]+margin*2))\n",
        "    newX[:, margin:X.shape[1]+margin, margin:X.shape[2]+margin] = X\n",
        "    return newX\n",
        "# Generate a patch and centerize it\n",
        "def patch(X, patch_size, height_index, width_index):\n",
        "    # The slice function is used for slicing\n",
        "    height_slice = slice(height_index, height_index + patch_size)\n",
        "    width_slice = slice(width_index, width_index + patch_size)\n",
        "    patch = X[:, height_slice, width_slice]  # The patch includes all bands\n",
        "    for i in range(X.shape[0]):\n",
        "        mean = np.mean(patch[i, :, :])\n",
        "        patch[i] = patch[i]-mean\n",
        "    return patch\n",
        "def standartize(X):\n",
        "    newX = np.reshape(X, (-1,1))\n",
        "    scaler = preprocessing.StandardScaler().fit(newX)\n",
        "    newX = scaler.transform(newX)\n",
        "    newX = np.reshape(newX, (X.shape[0], X.shape[1], X.shape[2]))\n",
        "    return newX"
      ],
      "metadata": {
        "id": "lNVQPQEuTil7"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Data\n",
        "def load_data(file_name, data_name, label_name):\n",
        "    # Original data path\n",
        "    DATA_PATH = os.path.join(os.getcwd(), file_name)\n",
        "    data = scipy.io.loadmat(os.path.join(DATA_PATH, data_name))\n",
        "    data = data[list(data.keys())[-1]]\n",
        "    label = scipy.io.loadmat(os.path.join(DATA_PATH, label_name))\n",
        "    label = np.int32(label[list(label.keys())[-1]])\n",
        "    data = np.transpose(data, (2, 0, 1))  # Move the channel dimension to the front for easier array manipulation\n",
        "    return data, label\n",
        "\n",
        "\n",
        "# Generate sliced data and store it\n",
        "def create_data(X, label):\n",
        "    for c in range(OUTPUT_CLASSES):\n",
        "        PATCH, LABEL, TEST_PATCH, TRAIN_PATCH, TEST_LABEL, TRAIN_LABEL = [], [], [], [], [], []\n",
        "        for h in range(X.shape[1] - PATCH_SIZE + 1):\n",
        "            for w in range(X.shape[2] - PATCH_SIZE + 1):\n",
        "                gt = label[h, w]\n",
        "                if(gt == c + 1):\n",
        "                    img = patch(X, PATCH_SIZE, h, w)\n",
        "                    PATCH.append(img)\n",
        "                    LABEL.append(gt - 1)\n",
        "        # Shuffle slices\n",
        "        shuffle(PATCH)\n",
        "        # Split into test set and training set\n",
        "        split_size = int(len(PATCH) * TEST_FRAC)\n",
        "        TEST_PATCH.extend(PATCH[:split_size])  # 0 ~ split_size\n",
        "        TRAIN_PATCH.extend(PATCH[split_size:])  # split_size ~ len(class)\n",
        "        TEST_LABEL.extend(LABEL[:split_size])\n",
        "        TRAIN_LABEL.extend(LABEL[split_size:])\n",
        "        # Write to folders\n",
        "        train_dict, test_dict = {}, {}\n",
        "        train_dict[\"train_patches\"] = TRAIN_PATCH\n",
        "        train_dict[\"train_labels\"] = TRAIN_LABEL\n",
        "        file_name = \"Training_class(%d).mat\" % c\n",
        "        scipy.io.savemat(os.path.join(NEW_DATA_PATH, file_name), train_dict)\n",
        "        test_dict[\"testing_patches\"] = TEST_PATCH\n",
        "        test_dict[\"testing_labels\"] = TEST_LABEL\n",
        "        file_name = \"Testing_class(%d).mat\" % c\n",
        "        scipy.io.savemat(os.path.join(NEW_DATA_PATH, file_name), test_dict)\n",
        "\n",
        "\n",
        "data, label = load_data(\"PaviaU\", \"PaviaU.mat\", \"PaviaU_gt.mat\")\n",
        "data = standartize(data)\n",
        "data = pad(data, int((PATCH_SIZE - 1) / 2))\n",
        "create_data(data, label)\n"
      ],
      "metadata": {
        "id": "ReBoMCmDT0ZD"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def xZero(X):\n",
        "    X = X.cpu()\n",
        "    newX = np.zeros(\n",
        "        (X.shape[0], X.shape[1], X.shape[2], X.shape[3]))\n",
        "    newX[:, :, int((X.shape[2]+1)/2), int((X.shape[3]+1)/2)] = X[:,\n",
        "                                                                 :, int((X.shape[2]+1)/2), int((X.shape[3]+1)/2)]\n",
        "    newX = torch.from_numpy(newX).double()\n",
        "    if torch.cuda.is_available():\n",
        "        newX = newX.cuda()\n",
        "    return newX"
      ],
      "metadata": {
        "id": "TiGTJa6FWF2G"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SeparableConv2d(nn.Module): # Depth wise separable conv\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=1, stride=1, padding=0, dilation=1, bias=False):\n",
        "        # Each input channel undergoes its own filter convolution operation\n",
        "        super(SeparableConv2d, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, in_channels, kernel_size,\n",
        "                               stride, padding, dilation, groups=in_channels, bias=bias)\n",
        "        self.pointwise = nn.Conv2d(\n",
        "            in_channels, out_channels, 1, 1, 0, 1, 1, bias=bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.pointwise(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "s6Uk_st8WQo9"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        # in_channels, out_channels, kernel_size, stride (default: 1), padding (default: 0)\n",
        "        self.conv1 = torch.nn.Sequential(\n",
        "            SeparableConv2d(103, 16, 1, 1, 0),  # 1*1 convolutional kernel\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.GroupNorm(16, 16)\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(103, 256, 1, 1, 0),\n",
        "            nn.GroupNorm(256, 256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.4),\n",
        "            SeparableConv2d(256, 256, 3, 1, 1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.4),\n",
        "            SeparableConv2d(256, 128, 3, 1, 1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.4),\n",
        "            SeparableConv2d(128, 64, 3, 1, 1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.GroupNorm(64, 64)\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(80*17*17, 2048),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(2048, 256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(256, 16),\n",
        "            nn.LogSoftmax(dim=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x0 = xZero(x)\n",
        "        x1 = self.conv1(x0)\n",
        "        x2 = self.conv2(x)\n",
        "        x12 = torch.cat((x1, x2), 1)\n",
        "        xCat = x12.view(-1, self.numFeatures(x12))  # Flatten the feature map into 1D\n",
        "        output = self.classifier(xCat)\n",
        "        return output\n",
        "\n",
        "    def numFeatures(self, x):\n",
        "        size = x.size()[1:]  # Get the height, width, and depth of the convolutional image\n",
        "        num = 1\n",
        "        for s in size:\n",
        "            num *= s\n",
        "        return num\n",
        "\n",
        "    def init_weights(self):  # Initialize weights\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                torch.nn.init.xavier_normal_(m.weight.data)\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.GroupNorm):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                torch.nn.init.normal_(m.weight.data, 0, 0.01)\n",
        "                m.bias.data.zero_()\n"
      ],
      "metadata": {
        "id": "kRfVrwfhWoiU"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class DataSet(Dataset):\n",
        "    def __init__(self, path, train, transform=None):\n",
        "        if train:\n",
        "            select = \"Training\"\n",
        "            patch_type = \"train\"\n",
        "        else:\n",
        "            select = \"Testing\"\n",
        "            patch_type = \"testing\"\n",
        "        self.tensors = []\n",
        "        self.labels = []\n",
        "        self.transform = transform\n",
        "        # Iterate through each class of tensors and add patches and labels\n",
        "        # Corresponding to lists\n",
        "        for file in os.listdir(path):\n",
        "            if os.path.isfile(os.path.join(path, file)) and select in file:\n",
        "                temp = scipy.io.loadmat(os.path.join(\n",
        "                    path, file))  # Load mat dictionary\n",
        "                # Filter dictionary to keep items without an underscore \"_\"\n",
        "                temp = {k: v for k, v in temp.items() if k[0] != '_'}\n",
        "\n",
        "                for i in range(len(temp[patch_type+\"_patches\"])):\n",
        "                    self.tensors.append(temp[patch_type+\"_patches\"][i])\n",
        "                    self.labels.append(temp[patch_type+\"_labels\"][0][i])\n",
        "        self.tensors = np.array(self.tensors)\n",
        "        self.labels = np.array(self.labels)\n",
        "\n",
        "    def __len__(self):\n",
        "        try:\n",
        "            if len(self.tensors) != len(self.labels):\n",
        "                raise Exception(\n",
        "                    \"Lengths of the tensor and labels list are not the same\")\n",
        "        except Exception as e:\n",
        "            print(e.args[0])\n",
        "        return len(self.tensors)\n",
        "\n",
        "    # Return a patch and label\n",
        "    def __getitem__(self, idx):\n",
        "        sample = (self.tensors[idx], self.labels[idx])\n",
        "        sample = (torch.from_numpy(self.tensors[idx]), torch.from_numpy(\n",
        "            np.array(self.labels[idx])).long())\n",
        "        return sample\n",
        "    # Tuple containing a patch image and its corresponding label\n"
      ],
      "metadata": {
        "id": "oTOtOKSrXjiv"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(net, data_loader, set_name, classes_name):\n",
        "    \"\"\"\n",
        "    Validate a batch of data, return the confusion matrix and accuracy\n",
        "    :param net:\n",
        "    :param data_loader:\n",
        "    :param set_name:  eg: 'valid' 'train' 'test'\n",
        "    :param classes_name:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    net.eval()\n",
        "    cls_num = len(classes_name)\n",
        "    conf_mat = np.zeros([cls_num, cls_num])\n",
        "\n",
        "    for data in data_loader:\n",
        "        images, labels = data\n",
        "        if torch.cuda.is_available():\n",
        "            images, labels = images.cuda(), labels.cuda()\n",
        "        outputs = net(images)\n",
        "        outputs.detach_()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "        # Update the confusion matrix\n",
        "        for i in range(len(labels)):\n",
        "            cate_i = labels[i]\n",
        "            pre_i = predicted[i]\n",
        "            conf_mat[cate_i, pre_i] += 1.0\n",
        "\n",
        "    for i in range(cls_num):\n",
        "        print('class:{:<10}, total num:{:<6}, correct num:{:<5}  Recall: {:.2%} Precision: {:.2%}'.format(\n",
        "            classes_name[i], np.sum(\n",
        "                conf_mat[i, :]), conf_mat[i, i], conf_mat[i, i] / (1 + np.sum(conf_mat[i, :])),\n",
        "            conf_mat[i, i] / (1 + np.sum(conf_mat[:, i]))))\n",
        "\n",
        "    print('{} set Accuracy:{:.2%}'.format(\n",
        "        set_name, np.trace(conf_mat) / np.sum(conf_mat)))\n",
        "\n",
        "    return conf_mat, '{:.2}'.format(np.trace(conf_mat) / np.sum(conf_mat))\n",
        "\n",
        "\n",
        "# Generate an image\n",
        "def show_confMat(confusion_mat, classes, set_name, out_dir):\n",
        "\n",
        "    # Normalize\n",
        "    confusion_mat_N = confusion_mat.copy()\n",
        "    for i in range(len(classes)):\n",
        "        confusion_mat_N[i, :] = confusion_mat[i, :] / confusion_mat[i, :].sum()\n",
        "\n",
        "    # Get the colormap\n",
        "    # More colormaps: http://matplotlib.org/examples/color/colormaps_reference.html\n",
        "    cmap = plt.cm.get_cmap('Greys')\n",
        "    plt.imshow(confusion_mat_N, cmap=cmap)\n",
        "    plt.colorbar()\n",
        "\n",
        "    # Set labels\n",
        "    xlocations = np.array(range(len(classes)))\n",
        "    plt.xticks(xlocations, list(classes), rotation=60)\n",
        "    plt.yticks(xlocations, list(classes))\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.ylabel('True label')\n",
        "    plt.title('Confusion Matrix ' + set_name)\n",
        "\n",
        "    # Display numbers\n",
        "    for i in range(confusion_mat_N.shape[0]):\n",
        "        for j in range(confusion_mat_N.shape[1]):\n",
        "            plt.text(x=j, y=i, s=int(\n",
        "                confusion_mat[i, j]), va='center', ha='center', color='red', fontsize=10)\n",
        "    # Save the image\n",
        "    plt.savefig(os.path.join(out_dir, 'Confusion_Matrix' + set_name + '.png'))\n",
        "    plt.close()\n"
      ],
      "metadata": {
        "id": "A4r_R8D2YAwn"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Define hyperparameters\n",
        "# EPOCH = 5\n",
        "# BATCH_SIZE = 24\n",
        "# classes_name = [str(c) for c in range(9)]  # Number of land cover classes\n",
        "\n",
        "# # --------------------Load Data---------------------\n",
        "# # Indian Pines .mat file path (each file is a separate class)\n",
        "# path = os.path.join(os.getcwd(), \"patch\")\n",
        "# training_dataset = DataSet(path=path, train=True)\n",
        "# testing_dataset = DataSet(path=path, train=False)\n",
        "\n",
        "# # Data Loaders\n",
        "# train_loader = torch.utils.data.DataLoader(\n",
        "#     dataset=training_dataset,\n",
        "#     batch_size=BATCH_SIZE,\n",
        "#     shuffle=True\n",
        "# )\n",
        "\n",
        "# test_loader = torch.utils.data.DataLoader(\n",
        "#     dataset=testing_dataset,\n",
        "#     batch_size=BATCH_SIZE,\n",
        "#     shuffle=True\n",
        "# )\n",
        "\n",
        "\n",
        "# Define hyperparameters\n",
        "EPOCH = 5\n",
        "BATCH_SIZE = 8\n",
        "classes_name = [str(c) for c in range(9)]  # Number of land cover classes\n",
        "\n",
        "# --------------------Load Data---------------------\n",
        "# Indian Pines .mat file path (each file is a separate class)\n",
        "path = os.path.join(os.getcwd(), \"patch\")\n",
        "training_dataset = DataSet(path=path, train=True)\n",
        "\n",
        "\n",
        "# Data Loaders\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    dataset=training_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "VfbrvQsjYnGS"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# testing_dataset = DataSet(path=path, train=False)\n",
        "# test_loader = torch.utils.data.DataLoader(\n",
        "#     dataset=testing_dataset,\n",
        "#     batch_size=BATCH_SIZE,\n",
        "#     shuffle=True\n",
        "# )"
      ],
      "metadata": {
        "id": "UO3TyvePgqqc"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Source and destination paths\n",
        "# source_path = '/content/patch' #'/content/drive/MyDrive/Pavia'\n",
        "# destination_path ='/content/drive/MyDrive/Pavia' #'/content/PaviaU'\n",
        "\n",
        "# # Use the !cp command to copy the folder\n",
        "# !cp -r \"$source_path\" \"$destination_path\"\n",
        "\n",
        "# # Verify the copied folder\n",
        "# print(f\"The folder '{source_path}' has been copied to '{destination_path}'.\")"
      ],
      "metadata": {
        "id": "eMA3zz8oexpE"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if CUDA is available\n",
        "use_cuda = torch.cuda.is_available()\n",
        "\n",
        "# Generate log\n",
        "now_time = datetime.now()\n",
        "time_str = datetime.strftime(now_time, '%m-%d_%H-%M-%S')\n",
        "log_path = os.path.join(os.getcwd(), \"log\")\n",
        "log_dir = os.path.join(log_path, time_str)\n",
        "\n",
        "# Create the log directory if it doesn't exist\n",
        "if not os.path.exists(log_dir):\n",
        "    os.makedirs(log_dir)\n",
        "\n",
        "# Initialize the SummaryWriter for logging\n",
        "writer = SummaryWriter(log_dir)\n"
      ],
      "metadata": {
        "id": "pJGzilIMiCU1"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------Build the Network--------------------------\n",
        "cnn = Net()  # Create CNN\n",
        "cnn.init_weights()  # Initialize weights\n",
        "cnn = cnn.double()\n",
        "\n",
        "# --------------------Set Loss Function and Optimizer----------------------\n",
        "optimizer = optim.Adam(cnn.parameters())  # Optimizer with default learning rate (1e-3)\n",
        "criterion = nn.CrossEntropyLoss()  # Loss function\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(\n",
        "    optimizer, step_size=EPOCH/2, gamma=0.5)  # Set the learning rate decay strategy\n"
      ],
      "metadata": {
        "id": "WtP3D7sBjGhz"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# testing_dataset = DataSet(path=path, train=False)\n",
        "# test_loader = torch.utils.data.DataLoader(\n",
        "#     dataset=testing_dataset,\n",
        "#     batch_size=BATCH_SIZE,\n",
        "#     shuffle=True\n",
        "# )"
      ],
      "metadata": {
        "id": "A_yBa5b2kBnr"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------Training------------------------------\n",
        "if use_cuda:  # Use GPU\n",
        "    cnn = cnn.cuda()\n",
        "\n",
        "for epoch in range(EPOCH):\n",
        "    loss_sigma = 0.0    # Record the sum of losses for one epoch\n",
        "    correct = 0.0\n",
        "    total = 0.0\n",
        "    scheduler.step()  # Update learning rate\n",
        "\n",
        "    for batch_idx, data in enumerate(train_loader):\n",
        "        # Get images and labels\n",
        "        inputs, labels = data\n",
        "        if use_cuda:\n",
        "            inputs, labels = inputs.cuda(), labels.cuda()\n",
        "        optimizer.zero_grad()  # Clear gradients\n",
        "        cnn = cnn.train()\n",
        "        outputs = cnn(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()  # Backward propagation\n",
        "        optimizer.step()  # Update weights\n",
        "\n",
        "        # Statistics for predictions\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += ((predicted == labels).squeeze().sum()).item()\n",
        "        loss_sigma += loss.item()\n",
        "\n",
        "        # Print training information every BATCH_SIZE iterations; loss is the average over BATCH_SIZE iterations\n",
        "        if batch_idx % BATCH_SIZE == BATCH_SIZE-1:\n",
        "            loss_avg = loss_sigma / BATCH_SIZE\n",
        "            loss_sigma = 0.0\n",
        "            print(\"Training: Epoch[{:0>3}/{:0>3}] Iteration[{:0>3}/{:0>3}] Loss: {:.4f} Acc:{:.2%}\".format(\n",
        "                epoch + 1, EPOCH, batch_idx + 1, len(train_loader), loss_avg, correct / total))\n",
        "            # Record training loss\n",
        "            writer.add_scalars(\n",
        "                'Loss_group', {'train_loss': loss_avg}, epoch)\n",
        "            # Record learning rate\n",
        "            writer.add_scalar(\n",
        "                'learning rate', scheduler.get_lr()[0], epoch)\n",
        "            # Record accuracy\n",
        "            writer.add_scalars('Accuracy_group', {\n",
        "                               'train_acc': correct / total}, epoch)\n",
        "\n",
        "    # Record gradients and weights for each epoch\n",
        "    for name, layer in cnn.named_parameters():\n",
        "        writer.add_histogram(\n",
        "            name + '_grad', layer.grad.cpu().data.numpy(), epoch)\n",
        "        writer.add_histogram(name + '_data', layer.cpu().data.numpy(), epoch)\n",
        "\n",
        "    # ------------------------------------ Evaluate model on the validation set ------------------------------------\n",
        "    if epoch % 1 == 0:\n",
        "        loss_sigma = 0.0\n",
        "        cls_num = len(classes_name)\n",
        "        conf_mat = np.zeros([cls_num, cls_num])  # Confusion matrix\n",
        "        cnn.eval()\n",
        "        # for batch_idx, data in enumerate(test_loader):\n",
        "        #     images, labels = data\n",
        "        #     if use_cuda:\n",
        "        #         images, labels = images.cuda(), labels.cuda()\n",
        "        #     cnn = cnn.train()\n",
        "        #     outputs = cnn(images)  # Forward pass\n",
        "        #     outputs.detach_()  # Detach from gradients\n",
        "        #     loss = criterion(outputs, labels)  # Compute loss\n",
        "        #     loss_sigma += loss.item()\n",
        "\n",
        "        #     _, predicted = torch.max(outputs.data, 1)  # Statistics\n",
        "        #     # labels = labels.data    # Variable --> tensor\n",
        "        #     # Update confusion matrix\n",
        "        #     for j in range(len(labels)):\n",
        "        #         cate_i = labels[j]\n",
        "        #         pre_i = predicted[j]\n",
        "        #         conf_mat[cate_i, pre_i] += 1.0\n",
        "\n",
        "        # print('{} set Accuracy:{:.2%}'.format(\n",
        "        #     'Valid', conf_mat.trace() / conf_mat.sum()))\n",
        "        # Record loss and accuracy\n",
        "        # writer.add_scalars(\n",
        "        #     'Loss_group', {'valid_loss': loss_sigma / len(test_loader)}, epoch)\n",
        "        # writer.add_scalars('Accuracy_group', {\n",
        "        #                    'valid_acc': conf_mat.trace() / conf_mat.sum()}, epoch)\n",
        "\n",
        "print('Finished Training')\n"
      ],
      "metadata": {
        "id": "w0GUYWAFjfxD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "777ca433-97d9-4566-e725-302a41d1d0fa"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training: Epoch[001/005] Iteration[008/2674] Loss: 2.4350 Acc:26.56%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:402: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training: Epoch[001/005] Iteration[016/2674] Loss: 1.6516 Acc:34.38%\n",
            "Training: Epoch[001/005] Iteration[024/2674] Loss: 1.6652 Acc:40.62%\n",
            "Training: Epoch[001/005] Iteration[032/2674] Loss: 1.4401 Acc:43.75%\n",
            "Training: Epoch[001/005] Iteration[040/2674] Loss: 1.2662 Acc:46.88%\n",
            "Training: Epoch[001/005] Iteration[048/2674] Loss: 1.3708 Acc:48.44%\n",
            "Training: Epoch[001/005] Iteration[056/2674] Loss: 1.2923 Acc:49.33%\n",
            "Training: Epoch[001/005] Iteration[064/2674] Loss: 1.3279 Acc:49.80%\n",
            "Training: Epoch[001/005] Iteration[072/2674] Loss: 1.5630 Acc:49.83%\n",
            "Training: Epoch[001/005] Iteration[080/2674] Loss: 1.3280 Acc:49.53%\n",
            "Training: Epoch[001/005] Iteration[088/2674] Loss: 1.5177 Acc:49.57%\n",
            "Training: Epoch[001/005] Iteration[096/2674] Loss: 1.4831 Acc:49.09%\n",
            "Training: Epoch[001/005] Iteration[104/2674] Loss: 1.0550 Acc:50.00%\n",
            "Training: Epoch[001/005] Iteration[112/2674] Loss: 1.0440 Acc:50.78%\n",
            "Training: Epoch[001/005] Iteration[120/2674] Loss: 1.0974 Acc:51.15%\n",
            "Training: Epoch[001/005] Iteration[128/2674] Loss: 1.3016 Acc:51.76%\n",
            "Training: Epoch[001/005] Iteration[136/2674] Loss: 1.1064 Acc:52.30%\n",
            "Training: Epoch[001/005] Iteration[144/2674] Loss: 1.5684 Acc:51.74%\n",
            "Training: Epoch[001/005] Iteration[152/2674] Loss: 1.3265 Acc:51.81%\n",
            "Training: Epoch[001/005] Iteration[160/2674] Loss: 1.0375 Acc:52.42%\n",
            "Training: Epoch[001/005] Iteration[168/2674] Loss: 1.0538 Acc:53.05%\n",
            "Training: Epoch[001/005] Iteration[176/2674] Loss: 1.2466 Acc:53.34%\n",
            "Training: Epoch[001/005] Iteration[184/2674] Loss: 0.8947 Acc:53.87%\n",
            "Training: Epoch[001/005] Iteration[192/2674] Loss: 1.1315 Acc:54.49%\n",
            "Training: Epoch[001/005] Iteration[200/2674] Loss: 1.1186 Acc:54.69%\n",
            "Training: Epoch[001/005] Iteration[208/2674] Loss: 1.2338 Acc:54.75%\n",
            "Training: Epoch[001/005] Iteration[216/2674] Loss: 1.2270 Acc:54.57%\n",
            "Training: Epoch[001/005] Iteration[224/2674] Loss: 0.9908 Acc:54.80%\n",
            "Training: Epoch[001/005] Iteration[232/2674] Loss: 0.9146 Acc:55.23%\n",
            "Training: Epoch[001/005] Iteration[240/2674] Loss: 1.0497 Acc:55.47%\n",
            "Training: Epoch[001/005] Iteration[248/2674] Loss: 1.0052 Acc:55.65%\n",
            "Training: Epoch[001/005] Iteration[256/2674] Loss: 0.8879 Acc:56.05%\n",
            "Training: Epoch[001/005] Iteration[264/2674] Loss: 1.1586 Acc:56.06%\n",
            "Training: Epoch[001/005] Iteration[272/2674] Loss: 1.4452 Acc:55.74%\n",
            "Training: Epoch[001/005] Iteration[280/2674] Loss: 1.2009 Acc:55.80%\n",
            "Training: Epoch[001/005] Iteration[288/2674] Loss: 1.2661 Acc:55.82%\n",
            "Training: Epoch[001/005] Iteration[296/2674] Loss: 1.2413 Acc:55.87%\n",
            "Training: Epoch[001/005] Iteration[304/2674] Loss: 0.9117 Acc:56.29%\n",
            "Training: Epoch[001/005] Iteration[312/2674] Loss: 1.0937 Acc:56.61%\n",
            "Training: Epoch[001/005] Iteration[320/2674] Loss: 0.9024 Acc:56.99%\n",
            "Training: Epoch[001/005] Iteration[328/2674] Loss: 0.9362 Acc:57.20%\n",
            "Training: Epoch[001/005] Iteration[336/2674] Loss: 1.1423 Acc:57.33%\n",
            "Training: Epoch[001/005] Iteration[344/2674] Loss: 0.9351 Acc:57.59%\n",
            "Training: Epoch[001/005] Iteration[352/2674] Loss: 1.1399 Acc:57.71%\n",
            "Training: Epoch[001/005] Iteration[360/2674] Loss: 1.2602 Acc:57.88%\n",
            "Training: Epoch[001/005] Iteration[368/2674] Loss: 1.1544 Acc:57.74%\n",
            "Training: Epoch[001/005] Iteration[376/2674] Loss: 1.0075 Acc:57.81%\n",
            "Training: Epoch[001/005] Iteration[384/2674] Loss: 0.7765 Acc:57.98%\n",
            "Training: Epoch[001/005] Iteration[392/2674] Loss: 1.1856 Acc:57.97%\n",
            "Training: Epoch[001/005] Iteration[400/2674] Loss: 0.8845 Acc:58.19%\n",
            "Training: Epoch[001/005] Iteration[408/2674] Loss: 0.8278 Acc:58.43%\n",
            "Training: Epoch[001/005] Iteration[416/2674] Loss: 0.9370 Acc:58.62%\n",
            "Training: Epoch[001/005] Iteration[424/2674] Loss: 0.8216 Acc:58.81%\n",
            "Training: Epoch[001/005] Iteration[432/2674] Loss: 0.8477 Acc:58.94%\n",
            "Training: Epoch[001/005] Iteration[440/2674] Loss: 0.7044 Acc:59.32%\n",
            "Training: Epoch[001/005] Iteration[448/2674] Loss: 0.8313 Acc:59.54%\n",
            "Training: Epoch[001/005] Iteration[456/2674] Loss: 0.9836 Acc:59.65%\n",
            "Training: Epoch[001/005] Iteration[464/2674] Loss: 0.6629 Acc:59.86%\n",
            "Training: Epoch[001/005] Iteration[472/2674] Loss: 0.8497 Acc:59.98%\n",
            "Training: Epoch[001/005] Iteration[480/2674] Loss: 0.8472 Acc:60.13%\n",
            "Training: Epoch[001/005] Iteration[488/2674] Loss: 0.8992 Acc:60.35%\n",
            "Training: Epoch[001/005] Iteration[496/2674] Loss: 0.9640 Acc:60.46%\n",
            "Training: Epoch[001/005] Iteration[504/2674] Loss: 0.9441 Acc:60.54%\n",
            "Training: Epoch[001/005] Iteration[512/2674] Loss: 0.8546 Acc:60.69%\n",
            "Training: Epoch[001/005] Iteration[520/2674] Loss: 0.8397 Acc:60.89%\n",
            "Training: Epoch[001/005] Iteration[528/2674] Loss: 0.7706 Acc:61.06%\n",
            "Training: Epoch[001/005] Iteration[536/2674] Loss: 0.9069 Acc:61.19%\n",
            "Training: Epoch[001/005] Iteration[544/2674] Loss: 0.5976 Acc:61.42%\n",
            "Training: Epoch[001/005] Iteration[552/2674] Loss: 0.7426 Acc:61.59%\n",
            "Training: Epoch[001/005] Iteration[560/2674] Loss: 0.9351 Acc:61.65%\n",
            "Training: Epoch[001/005] Iteration[568/2674] Loss: 0.7750 Acc:61.80%\n",
            "Training: Epoch[001/005] Iteration[576/2674] Loss: 0.6277 Acc:62.02%\n",
            "Training: Epoch[001/005] Iteration[584/2674] Loss: 0.8223 Acc:62.22%\n",
            "Training: Epoch[001/005] Iteration[592/2674] Loss: 0.9139 Acc:62.29%\n",
            "Training: Epoch[001/005] Iteration[600/2674] Loss: 0.8704 Acc:62.40%\n",
            "Training: Epoch[001/005] Iteration[608/2674] Loss: 0.9911 Acc:62.46%\n",
            "Training: Epoch[001/005] Iteration[616/2674] Loss: 0.9000 Acc:62.62%\n",
            "Training: Epoch[001/005] Iteration[624/2674] Loss: 0.5379 Acc:62.92%\n",
            "Training: Epoch[001/005] Iteration[632/2674] Loss: 0.8970 Acc:63.01%\n",
            "Training: Epoch[001/005] Iteration[640/2674] Loss: 0.9752 Acc:63.14%\n",
            "Training: Epoch[001/005] Iteration[648/2674] Loss: 0.8449 Acc:63.23%\n",
            "Training: Epoch[001/005] Iteration[656/2674] Loss: 0.9208 Acc:63.36%\n",
            "Training: Epoch[001/005] Iteration[664/2674] Loss: 0.7980 Acc:63.54%\n",
            "Training: Epoch[001/005] Iteration[672/2674] Loss: 0.7080 Acc:63.60%\n",
            "Training: Epoch[001/005] Iteration[680/2674] Loss: 0.7204 Acc:63.75%\n",
            "Training: Epoch[001/005] Iteration[688/2674] Loss: 0.5107 Acc:63.94%\n",
            "Training: Epoch[001/005] Iteration[696/2674] Loss: 0.4822 Acc:64.17%\n",
            "Training: Epoch[001/005] Iteration[704/2674] Loss: 1.0536 Acc:64.20%\n",
            "Training: Epoch[001/005] Iteration[712/2674] Loss: 0.9799 Acc:64.24%\n",
            "Training: Epoch[001/005] Iteration[720/2674] Loss: 0.6555 Acc:64.36%\n",
            "Training: Epoch[001/005] Iteration[728/2674] Loss: 0.7988 Acc:64.46%\n",
            "Training: Epoch[001/005] Iteration[736/2674] Loss: 0.7246 Acc:64.54%\n",
            "Training: Epoch[001/005] Iteration[744/2674] Loss: 0.8123 Acc:64.60%\n",
            "Training: Epoch[001/005] Iteration[752/2674] Loss: 0.5768 Acc:64.74%\n",
            "Training: Epoch[001/005] Iteration[760/2674] Loss: 1.0219 Acc:64.84%\n",
            "Training: Epoch[001/005] Iteration[768/2674] Loss: 0.5890 Acc:64.99%\n",
            "Training: Epoch[001/005] Iteration[776/2674] Loss: 0.6727 Acc:65.09%\n",
            "Training: Epoch[001/005] Iteration[784/2674] Loss: 0.8750 Acc:65.13%\n",
            "Training: Epoch[001/005] Iteration[792/2674] Loss: 0.7971 Acc:65.15%\n",
            "Training: Epoch[001/005] Iteration[800/2674] Loss: 0.6281 Acc:65.38%\n",
            "Training: Epoch[001/005] Iteration[808/2674] Loss: 0.7176 Acc:65.53%\n",
            "Training: Epoch[001/005] Iteration[816/2674] Loss: 0.4278 Acc:65.72%\n",
            "Training: Epoch[001/005] Iteration[824/2674] Loss: 0.9571 Acc:65.79%\n",
            "Training: Epoch[001/005] Iteration[832/2674] Loss: 0.7625 Acc:65.87%\n",
            "Training: Epoch[001/005] Iteration[840/2674] Loss: 0.6852 Acc:65.97%\n",
            "Training: Epoch[001/005] Iteration[848/2674] Loss: 0.5111 Acc:66.08%\n",
            "Training: Epoch[001/005] Iteration[856/2674] Loss: 0.6518 Acc:66.18%\n",
            "Training: Epoch[001/005] Iteration[864/2674] Loss: 1.0007 Acc:66.16%\n",
            "Training: Epoch[001/005] Iteration[872/2674] Loss: 0.8180 Acc:66.20%\n",
            "Training: Epoch[001/005] Iteration[880/2674] Loss: 0.5629 Acc:66.34%\n",
            "Training: Epoch[001/005] Iteration[888/2674] Loss: 0.7591 Acc:66.34%\n",
            "Training: Epoch[001/005] Iteration[896/2674] Loss: 0.6351 Acc:66.43%\n",
            "Training: Epoch[001/005] Iteration[904/2674] Loss: 0.7198 Acc:66.50%\n",
            "Training: Epoch[001/005] Iteration[912/2674] Loss: 0.6720 Acc:66.52%\n",
            "Training: Epoch[001/005] Iteration[920/2674] Loss: 0.8045 Acc:66.55%\n",
            "Training: Epoch[001/005] Iteration[928/2674] Loss: 0.6281 Acc:66.64%\n",
            "Training: Epoch[001/005] Iteration[936/2674] Loss: 0.8060 Acc:66.64%\n",
            "Training: Epoch[001/005] Iteration[944/2674] Loss: 0.4414 Acc:66.79%\n",
            "Training: Epoch[001/005] Iteration[952/2674] Loss: 0.3904 Acc:66.96%\n",
            "Training: Epoch[001/005] Iteration[960/2674] Loss: 0.8140 Acc:67.08%\n",
            "Training: Epoch[001/005] Iteration[968/2674] Loss: 0.5612 Acc:67.20%\n",
            "Training: Epoch[001/005] Iteration[976/2674] Loss: 0.6675 Acc:67.25%\n",
            "Training: Epoch[001/005] Iteration[984/2674] Loss: 0.8595 Acc:67.31%\n",
            "Training: Epoch[001/005] Iteration[992/2674] Loss: 0.6036 Acc:67.36%\n",
            "Training: Epoch[001/005] Iteration[1000/2674] Loss: 0.7103 Acc:67.38%\n",
            "Training: Epoch[001/005] Iteration[1008/2674] Loss: 0.3858 Acc:67.56%\n",
            "Training: Epoch[001/005] Iteration[1016/2674] Loss: 0.5774 Acc:67.68%\n",
            "Training: Epoch[001/005] Iteration[1024/2674] Loss: 0.6257 Acc:67.76%\n",
            "Training: Epoch[001/005] Iteration[1032/2674] Loss: 0.6993 Acc:67.84%\n",
            "Training: Epoch[001/005] Iteration[1040/2674] Loss: 0.6340 Acc:67.94%\n",
            "Training: Epoch[001/005] Iteration[1048/2674] Loss: 0.6365 Acc:68.03%\n",
            "Training: Epoch[001/005] Iteration[1056/2674] Loss: 0.6891 Acc:68.15%\n",
            "Training: Epoch[001/005] Iteration[1064/2674] Loss: 0.5316 Acc:68.29%\n",
            "Training: Epoch[001/005] Iteration[1072/2674] Loss: 0.5386 Acc:68.37%\n",
            "Training: Epoch[001/005] Iteration[1080/2674] Loss: 0.8067 Acc:68.39%\n",
            "Training: Epoch[001/005] Iteration[1088/2674] Loss: 0.8513 Acc:68.43%\n",
            "Training: Epoch[001/005] Iteration[1096/2674] Loss: 0.6257 Acc:68.52%\n",
            "Training: Epoch[001/005] Iteration[1104/2674] Loss: 0.5847 Acc:68.60%\n",
            "Training: Epoch[001/005] Iteration[1112/2674] Loss: 0.5191 Acc:68.71%\n",
            "Training: Epoch[001/005] Iteration[1120/2674] Loss: 0.4557 Acc:68.77%\n",
            "Training: Epoch[001/005] Iteration[1128/2674] Loss: 0.8214 Acc:68.82%\n",
            "Training: Epoch[001/005] Iteration[1136/2674] Loss: 0.7388 Acc:68.87%\n",
            "Training: Epoch[001/005] Iteration[1144/2674] Loss: 0.7151 Acc:68.88%\n",
            "Training: Epoch[001/005] Iteration[1152/2674] Loss: 0.7014 Acc:68.88%\n",
            "Training: Epoch[001/005] Iteration[1160/2674] Loss: 0.6794 Acc:68.91%\n",
            "Training: Epoch[001/005] Iteration[1168/2674] Loss: 0.5401 Acc:69.01%\n",
            "Training: Epoch[001/005] Iteration[1176/2674] Loss: 0.5014 Acc:69.11%\n",
            "Training: Epoch[001/005] Iteration[1184/2674] Loss: 0.5589 Acc:69.21%\n",
            "Training: Epoch[001/005] Iteration[1192/2674] Loss: 1.0024 Acc:69.22%\n",
            "Training: Epoch[001/005] Iteration[1200/2674] Loss: 0.5506 Acc:69.28%\n",
            "Training: Epoch[001/005] Iteration[1208/2674] Loss: 0.6527 Acc:69.35%\n",
            "Training: Epoch[001/005] Iteration[1216/2674] Loss: 0.6444 Acc:69.36%\n",
            "Training: Epoch[001/005] Iteration[1224/2674] Loss: 0.4518 Acc:69.46%\n",
            "Training: Epoch[001/005] Iteration[1232/2674] Loss: 0.5322 Acc:69.51%\n",
            "Training: Epoch[001/005] Iteration[1240/2674] Loss: 0.5053 Acc:69.62%\n",
            "Training: Epoch[001/005] Iteration[1248/2674] Loss: 0.5136 Acc:69.67%\n",
            "Training: Epoch[001/005] Iteration[1256/2674] Loss: 0.5730 Acc:69.76%\n",
            "Training: Epoch[001/005] Iteration[1264/2674] Loss: 0.9250 Acc:69.78%\n",
            "Training: Epoch[001/005] Iteration[1272/2674] Loss: 0.5858 Acc:69.83%\n",
            "Training: Epoch[001/005] Iteration[1280/2674] Loss: 0.5283 Acc:69.87%\n",
            "Training: Epoch[001/005] Iteration[1288/2674] Loss: 0.4578 Acc:69.94%\n",
            "Training: Epoch[001/005] Iteration[1296/2674] Loss: 0.6531 Acc:70.02%\n",
            "Training: Epoch[001/005] Iteration[1304/2674] Loss: 0.9035 Acc:70.08%\n",
            "Training: Epoch[001/005] Iteration[1312/2674] Loss: 0.4565 Acc:70.15%\n",
            "Training: Epoch[001/005] Iteration[1320/2674] Loss: 0.5313 Acc:70.23%\n",
            "Training: Epoch[001/005] Iteration[1328/2674] Loss: 0.7676 Acc:70.26%\n",
            "Training: Epoch[001/005] Iteration[1336/2674] Loss: 0.5981 Acc:70.32%\n",
            "Training: Epoch[001/005] Iteration[1344/2674] Loss: 0.9578 Acc:70.36%\n",
            "Training: Epoch[001/005] Iteration[1352/2674] Loss: 0.5606 Acc:70.42%\n",
            "Training: Epoch[001/005] Iteration[1360/2674] Loss: 0.5511 Acc:70.49%\n",
            "Training: Epoch[001/005] Iteration[1368/2674] Loss: 0.5434 Acc:70.52%\n",
            "Training: Epoch[001/005] Iteration[1376/2674] Loss: 0.7732 Acc:70.57%\n",
            "Training: Epoch[001/005] Iteration[1384/2674] Loss: 0.6260 Acc:70.64%\n",
            "Training: Epoch[001/005] Iteration[1392/2674] Loss: 0.5716 Acc:70.73%\n",
            "Training: Epoch[001/005] Iteration[1400/2674] Loss: 0.7202 Acc:70.76%\n",
            "Training: Epoch[001/005] Iteration[1408/2674] Loss: 0.4247 Acc:70.83%\n",
            "Training: Epoch[001/005] Iteration[1416/2674] Loss: 0.4878 Acc:70.88%\n",
            "Training: Epoch[001/005] Iteration[1424/2674] Loss: 0.3102 Acc:70.99%\n",
            "Training: Epoch[001/005] Iteration[1432/2674] Loss: 0.5552 Acc:71.03%\n",
            "Training: Epoch[001/005] Iteration[1440/2674] Loss: 0.8304 Acc:71.01%\n",
            "Training: Epoch[001/005] Iteration[1448/2674] Loss: 0.5898 Acc:71.07%\n",
            "Training: Epoch[001/005] Iteration[1456/2674] Loss: 0.7533 Acc:71.10%\n",
            "Training: Epoch[001/005] Iteration[1464/2674] Loss: 0.3488 Acc:71.17%\n",
            "Training: Epoch[001/005] Iteration[1472/2674] Loss: 0.4348 Acc:71.24%\n",
            "Training: Epoch[001/005] Iteration[1480/2674] Loss: 0.7889 Acc:71.28%\n",
            "Training: Epoch[001/005] Iteration[1488/2674] Loss: 0.5615 Acc:71.36%\n",
            "Training: Epoch[001/005] Iteration[1496/2674] Loss: 0.6342 Acc:71.39%\n",
            "Training: Epoch[001/005] Iteration[1504/2674] Loss: 0.3830 Acc:71.48%\n",
            "Training: Epoch[001/005] Iteration[1512/2674] Loss: 0.4803 Acc:71.56%\n",
            "Training: Epoch[001/005] Iteration[1520/2674] Loss: 0.4441 Acc:71.60%\n",
            "Training: Epoch[001/005] Iteration[1528/2674] Loss: 0.9549 Acc:71.61%\n",
            "Training: Epoch[001/005] Iteration[1536/2674] Loss: 0.4246 Acc:71.67%\n",
            "Training: Epoch[001/005] Iteration[1544/2674] Loss: 0.8212 Acc:71.69%\n",
            "Training: Epoch[001/005] Iteration[1552/2674] Loss: 0.7859 Acc:71.72%\n",
            "Training: Epoch[001/005] Iteration[1560/2674] Loss: 0.4670 Acc:71.75%\n",
            "Training: Epoch[001/005] Iteration[1568/2674] Loss: 0.3257 Acc:71.85%\n",
            "Training: Epoch[001/005] Iteration[1576/2674] Loss: 0.9341 Acc:71.81%\n",
            "Training: Epoch[001/005] Iteration[1584/2674] Loss: 0.5354 Acc:71.83%\n",
            "Training: Epoch[001/005] Iteration[1592/2674] Loss: 0.4441 Acc:71.88%\n",
            "Training: Epoch[001/005] Iteration[1600/2674] Loss: 0.4542 Acc:71.96%\n",
            "Training: Epoch[001/005] Iteration[1608/2674] Loss: 0.2686 Acc:72.05%\n",
            "Training: Epoch[001/005] Iteration[1616/2674] Loss: 0.5001 Acc:72.12%\n",
            "Training: Epoch[001/005] Iteration[1624/2674] Loss: 0.5376 Acc:72.15%\n",
            "Training: Epoch[001/005] Iteration[1632/2674] Loss: 0.3420 Acc:72.24%\n",
            "Training: Epoch[001/005] Iteration[1640/2674] Loss: 0.4179 Acc:72.31%\n",
            "Training: Epoch[001/005] Iteration[1648/2674] Loss: 0.5761 Acc:72.36%\n",
            "Training: Epoch[001/005] Iteration[1656/2674] Loss: 0.7190 Acc:72.36%\n",
            "Training: Epoch[001/005] Iteration[1664/2674] Loss: 0.8445 Acc:72.38%\n",
            "Training: Epoch[001/005] Iteration[1672/2674] Loss: 0.5874 Acc:72.41%\n",
            "Training: Epoch[001/005] Iteration[1680/2674] Loss: 0.3861 Acc:72.49%\n",
            "Training: Epoch[001/005] Iteration[1688/2674] Loss: 0.4087 Acc:72.56%\n",
            "Training: Epoch[001/005] Iteration[1696/2674] Loss: 0.3249 Acc:72.63%\n",
            "Training: Epoch[001/005] Iteration[1704/2674] Loss: 0.4902 Acc:72.71%\n",
            "Training: Epoch[001/005] Iteration[1712/2674] Loss: 0.3255 Acc:72.77%\n",
            "Training: Epoch[001/005] Iteration[1720/2674] Loss: 0.5671 Acc:72.81%\n",
            "Training: Epoch[001/005] Iteration[1728/2674] Loss: 0.6012 Acc:72.84%\n",
            "Training: Epoch[001/005] Iteration[1736/2674] Loss: 0.5795 Acc:72.89%\n",
            "Training: Epoch[001/005] Iteration[1744/2674] Loss: 0.6222 Acc:72.91%\n",
            "Training: Epoch[001/005] Iteration[1752/2674] Loss: 0.5613 Acc:72.97%\n",
            "Training: Epoch[001/005] Iteration[1760/2674] Loss: 0.6645 Acc:72.98%\n",
            "Training: Epoch[001/005] Iteration[1768/2674] Loss: 0.6027 Acc:73.02%\n",
            "Training: Epoch[001/005] Iteration[1776/2674] Loss: 0.6804 Acc:73.06%\n",
            "Training: Epoch[001/005] Iteration[1784/2674] Loss: 0.6162 Acc:73.12%\n",
            "Training: Epoch[001/005] Iteration[1792/2674] Loss: 0.4389 Acc:73.18%\n",
            "Training: Epoch[001/005] Iteration[1800/2674] Loss: 0.7382 Acc:73.20%\n",
            "Training: Epoch[001/005] Iteration[1808/2674] Loss: 0.4555 Acc:73.25%\n",
            "Training: Epoch[001/005] Iteration[1816/2674] Loss: 0.2721 Acc:73.33%\n",
            "Training: Epoch[001/005] Iteration[1824/2674] Loss: 0.5078 Acc:73.38%\n",
            "Training: Epoch[001/005] Iteration[1832/2674] Loss: 0.5446 Acc:73.40%\n",
            "Training: Epoch[001/005] Iteration[1840/2674] Loss: 0.5896 Acc:73.42%\n",
            "Training: Epoch[001/005] Iteration[1848/2674] Loss: 0.4992 Acc:73.46%\n",
            "Training: Epoch[001/005] Iteration[1856/2674] Loss: 0.4314 Acc:73.53%\n",
            "Training: Epoch[001/005] Iteration[1864/2674] Loss: 0.2943 Acc:73.61%\n",
            "Training: Epoch[001/005] Iteration[1872/2674] Loss: 0.6251 Acc:73.66%\n",
            "Training: Epoch[001/005] Iteration[1880/2674] Loss: 0.4569 Acc:73.69%\n",
            "Training: Epoch[001/005] Iteration[1888/2674] Loss: 0.3420 Acc:73.74%\n",
            "Training: Epoch[001/005] Iteration[1896/2674] Loss: 0.6795 Acc:73.77%\n",
            "Training: Epoch[001/005] Iteration[1904/2674] Loss: 0.3969 Acc:73.83%\n",
            "Training: Epoch[001/005] Iteration[1912/2674] Loss: 0.4849 Acc:73.87%\n",
            "Training: Epoch[001/005] Iteration[1920/2674] Loss: 0.2587 Acc:73.94%\n",
            "Training: Epoch[001/005] Iteration[1928/2674] Loss: 0.5159 Acc:73.98%\n",
            "Training: Epoch[001/005] Iteration[1936/2674] Loss: 0.6866 Acc:74.02%\n",
            "Training: Epoch[001/005] Iteration[1944/2674] Loss: 0.5751 Acc:74.07%\n",
            "Training: Epoch[001/005] Iteration[1952/2674] Loss: 0.5296 Acc:74.10%\n",
            "Training: Epoch[001/005] Iteration[1960/2674] Loss: 0.3471 Acc:74.15%\n",
            "Training: Epoch[001/005] Iteration[1968/2674] Loss: 0.3512 Acc:74.21%\n",
            "Training: Epoch[001/005] Iteration[1976/2674] Loss: 0.5459 Acc:74.23%\n",
            "Training: Epoch[001/005] Iteration[1984/2674] Loss: 0.6905 Acc:74.25%\n",
            "Training: Epoch[001/005] Iteration[1992/2674] Loss: 0.4297 Acc:74.30%\n",
            "Training: Epoch[001/005] Iteration[2000/2674] Loss: 0.3752 Acc:74.34%\n",
            "Training: Epoch[001/005] Iteration[2008/2674] Loss: 0.4963 Acc:74.37%\n",
            "Training: Epoch[001/005] Iteration[2016/2674] Loss: 0.4938 Acc:74.42%\n",
            "Training: Epoch[001/005] Iteration[2024/2674] Loss: 0.3303 Acc:74.47%\n",
            "Training: Epoch[001/005] Iteration[2032/2674] Loss: 0.4919 Acc:74.51%\n",
            "Training: Epoch[001/005] Iteration[2040/2674] Loss: 0.4729 Acc:74.56%\n",
            "Training: Epoch[001/005] Iteration[2048/2674] Loss: 0.7017 Acc:74.58%\n",
            "Training: Epoch[001/005] Iteration[2056/2674] Loss: 0.7488 Acc:74.61%\n",
            "Training: Epoch[001/005] Iteration[2064/2674] Loss: 0.4405 Acc:74.64%\n",
            "Training: Epoch[001/005] Iteration[2072/2674] Loss: 0.2182 Acc:74.71%\n",
            "Training: Epoch[001/005] Iteration[2080/2674] Loss: 0.3523 Acc:74.77%\n",
            "Training: Epoch[001/005] Iteration[2088/2674] Loss: 0.4882 Acc:74.82%\n",
            "Training: Epoch[001/005] Iteration[2096/2674] Loss: 0.5169 Acc:74.86%\n",
            "Training: Epoch[001/005] Iteration[2104/2674] Loss: 0.5208 Acc:74.89%\n",
            "Training: Epoch[001/005] Iteration[2112/2674] Loss: 0.5791 Acc:74.89%\n",
            "Training: Epoch[001/005] Iteration[2120/2674] Loss: 0.7085 Acc:74.89%\n",
            "Training: Epoch[001/005] Iteration[2128/2674] Loss: 0.7106 Acc:74.89%\n",
            "Training: Epoch[001/005] Iteration[2136/2674] Loss: 0.4807 Acc:74.92%\n",
            "Training: Epoch[001/005] Iteration[2144/2674] Loss: 0.5793 Acc:74.92%\n",
            "Training: Epoch[001/005] Iteration[2152/2674] Loss: 1.0096 Acc:74.95%\n",
            "Training: Epoch[001/005] Iteration[2160/2674] Loss: 0.7136 Acc:74.92%\n",
            "Training: Epoch[001/005] Iteration[2168/2674] Loss: 0.5187 Acc:74.97%\n",
            "Training: Epoch[001/005] Iteration[2176/2674] Loss: 0.6798 Acc:74.97%\n",
            "Training: Epoch[001/005] Iteration[2184/2674] Loss: 0.5863 Acc:74.98%\n",
            "Training: Epoch[001/005] Iteration[2192/2674] Loss: 0.5702 Acc:74.99%\n",
            "Training: Epoch[001/005] Iteration[2200/2674] Loss: 0.5017 Acc:75.02%\n",
            "Training: Epoch[001/005] Iteration[2208/2674] Loss: 0.5495 Acc:75.05%\n",
            "Training: Epoch[001/005] Iteration[2216/2674] Loss: 0.4992 Acc:75.10%\n",
            "Training: Epoch[001/005] Iteration[2224/2674] Loss: 0.1336 Acc:75.17%\n",
            "Training: Epoch[001/005] Iteration[2232/2674] Loss: 0.3818 Acc:75.22%\n",
            "Training: Epoch[001/005] Iteration[2240/2674] Loss: 0.3667 Acc:75.28%\n",
            "Training: Epoch[001/005] Iteration[2248/2674] Loss: 0.4371 Acc:75.31%\n",
            "Training: Epoch[001/005] Iteration[2256/2674] Loss: 0.4194 Acc:75.35%\n",
            "Training: Epoch[001/005] Iteration[2264/2674] Loss: 0.2429 Acc:75.41%\n",
            "Training: Epoch[001/005] Iteration[2272/2674] Loss: 0.3399 Acc:75.45%\n",
            "Training: Epoch[001/005] Iteration[2280/2674] Loss: 0.2838 Acc:75.50%\n",
            "Training: Epoch[001/005] Iteration[2288/2674] Loss: 0.5748 Acc:75.53%\n",
            "Training: Epoch[001/005] Iteration[2296/2674] Loss: 0.4341 Acc:75.57%\n",
            "Training: Epoch[001/005] Iteration[2304/2674] Loss: 0.3292 Acc:75.61%\n",
            "Training: Epoch[001/005] Iteration[2312/2674] Loss: 0.3904 Acc:75.63%\n",
            "Training: Epoch[001/005] Iteration[2320/2674] Loss: 0.5820 Acc:75.65%\n",
            "Training: Epoch[001/005] Iteration[2328/2674] Loss: 0.5406 Acc:75.67%\n",
            "Training: Epoch[001/005] Iteration[2336/2674] Loss: 0.8406 Acc:75.69%\n",
            "Training: Epoch[001/005] Iteration[2344/2674] Loss: 0.4987 Acc:75.71%\n",
            "Training: Epoch[001/005] Iteration[2352/2674] Loss: 0.7377 Acc:75.71%\n",
            "Training: Epoch[001/005] Iteration[2360/2674] Loss: 0.5295 Acc:75.73%\n",
            "Training: Epoch[001/005] Iteration[2368/2674] Loss: 0.4261 Acc:75.76%\n",
            "Training: Epoch[001/005] Iteration[2376/2674] Loss: 0.4208 Acc:75.79%\n",
            "Training: Epoch[001/005] Iteration[2384/2674] Loss: 0.3642 Acc:75.83%\n",
            "Training: Epoch[001/005] Iteration[2392/2674] Loss: 0.1177 Acc:75.90%\n",
            "Training: Epoch[001/005] Iteration[2400/2674] Loss: 0.2869 Acc:75.94%\n",
            "Training: Epoch[001/005] Iteration[2408/2674] Loss: 0.8226 Acc:75.94%\n",
            "Training: Epoch[001/005] Iteration[2416/2674] Loss: 0.6333 Acc:75.97%\n",
            "Training: Epoch[001/005] Iteration[2424/2674] Loss: 0.6279 Acc:75.97%\n",
            "Training: Epoch[001/005] Iteration[2432/2674] Loss: 0.7533 Acc:75.97%\n",
            "Training: Epoch[001/005] Iteration[2440/2674] Loss: 0.3743 Acc:75.99%\n",
            "Training: Epoch[001/005] Iteration[2448/2674] Loss: 0.3673 Acc:76.04%\n",
            "Training: Epoch[001/005] Iteration[2456/2674] Loss: 0.3856 Acc:76.08%\n",
            "Training: Epoch[001/005] Iteration[2464/2674] Loss: 0.6888 Acc:76.11%\n",
            "Training: Epoch[001/005] Iteration[2472/2674] Loss: 0.3959 Acc:76.15%\n",
            "Training: Epoch[001/005] Iteration[2480/2674] Loss: 0.2603 Acc:76.19%\n",
            "Training: Epoch[001/005] Iteration[2488/2674] Loss: 0.7476 Acc:76.20%\n",
            "Training: Epoch[001/005] Iteration[2496/2674] Loss: 0.3451 Acc:76.24%\n",
            "Training: Epoch[001/005] Iteration[2504/2674] Loss: 0.6146 Acc:76.26%\n",
            "Training: Epoch[001/005] Iteration[2512/2674] Loss: 0.9580 Acc:76.25%\n",
            "Training: Epoch[001/005] Iteration[2520/2674] Loss: 0.3109 Acc:76.29%\n",
            "Training: Epoch[001/005] Iteration[2528/2674] Loss: 0.4886 Acc:76.31%\n",
            "Training: Epoch[001/005] Iteration[2536/2674] Loss: 0.7373 Acc:76.31%\n",
            "Training: Epoch[001/005] Iteration[2544/2674] Loss: 0.5924 Acc:76.31%\n",
            "Training: Epoch[001/005] Iteration[2552/2674] Loss: 0.2951 Acc:76.36%\n",
            "Training: Epoch[001/005] Iteration[2560/2674] Loss: 0.4564 Acc:76.40%\n",
            "Training: Epoch[001/005] Iteration[2568/2674] Loss: 0.2873 Acc:76.44%\n",
            "Training: Epoch[001/005] Iteration[2576/2674] Loss: 0.4992 Acc:76.47%\n",
            "Training: Epoch[001/005] Iteration[2584/2674] Loss: 0.3482 Acc:76.52%\n",
            "Training: Epoch[001/005] Iteration[2592/2674] Loss: 0.5708 Acc:76.54%\n",
            "Training: Epoch[001/005] Iteration[2600/2674] Loss: 0.3075 Acc:76.57%\n",
            "Training: Epoch[001/005] Iteration[2608/2674] Loss: 0.5065 Acc:76.59%\n",
            "Training: Epoch[001/005] Iteration[2616/2674] Loss: 0.4667 Acc:76.62%\n",
            "Training: Epoch[001/005] Iteration[2624/2674] Loss: 0.4132 Acc:76.64%\n",
            "Training: Epoch[001/005] Iteration[2632/2674] Loss: 0.4252 Acc:76.68%\n",
            "Training: Epoch[001/005] Iteration[2640/2674] Loss: 0.4765 Acc:76.71%\n",
            "Training: Epoch[001/005] Iteration[2648/2674] Loss: 0.4552 Acc:76.74%\n",
            "Training: Epoch[001/005] Iteration[2656/2674] Loss: 0.5930 Acc:76.76%\n",
            "Training: Epoch[001/005] Iteration[2664/2674] Loss: 0.4768 Acc:76.77%\n",
            "Training: Epoch[001/005] Iteration[2672/2674] Loss: 0.5017 Acc:76.81%\n",
            "Training: Epoch[002/005] Iteration[008/2674] Loss: 0.3536 Acc:87.50%\n",
            "Training: Epoch[002/005] Iteration[016/2674] Loss: 0.3598 Acc:88.28%\n",
            "Training: Epoch[002/005] Iteration[024/2674] Loss: 0.5660 Acc:84.90%\n",
            "Training: Epoch[002/005] Iteration[032/2674] Loss: 0.3670 Acc:85.16%\n",
            "Training: Epoch[002/005] Iteration[040/2674] Loss: 0.2861 Acc:86.25%\n",
            "Training: Epoch[002/005] Iteration[048/2674] Loss: 0.4451 Acc:85.94%\n",
            "Training: Epoch[002/005] Iteration[056/2674] Loss: 0.3472 Acc:86.38%\n",
            "Training: Epoch[002/005] Iteration[064/2674] Loss: 0.5882 Acc:85.55%\n",
            "Training: Epoch[002/005] Iteration[072/2674] Loss: 0.2972 Acc:85.76%\n",
            "Training: Epoch[002/005] Iteration[080/2674] Loss: 0.4766 Acc:85.62%\n",
            "Training: Epoch[002/005] Iteration[088/2674] Loss: 0.5178 Acc:85.94%\n",
            "Training: Epoch[002/005] Iteration[096/2674] Loss: 0.3340 Acc:86.46%\n",
            "Training: Epoch[002/005] Iteration[104/2674] Loss: 0.4218 Acc:86.30%\n",
            "Training: Epoch[002/005] Iteration[112/2674] Loss: 0.4566 Acc:86.38%\n",
            "Training: Epoch[002/005] Iteration[120/2674] Loss: 0.3885 Acc:86.15%\n",
            "Training: Epoch[002/005] Iteration[128/2674] Loss: 0.2003 Acc:86.33%\n",
            "Training: Epoch[002/005] Iteration[136/2674] Loss: 0.5201 Acc:86.21%\n",
            "Training: Epoch[002/005] Iteration[144/2674] Loss: 0.3088 Acc:86.37%\n",
            "Training: Epoch[002/005] Iteration[152/2674] Loss: 0.3757 Acc:86.27%\n",
            "Training: Epoch[002/005] Iteration[160/2674] Loss: 0.4257 Acc:86.17%\n",
            "Training: Epoch[002/005] Iteration[168/2674] Loss: 0.3781 Acc:86.09%\n",
            "Training: Epoch[002/005] Iteration[176/2674] Loss: 0.3313 Acc:86.22%\n",
            "Training: Epoch[002/005] Iteration[184/2674] Loss: 0.5504 Acc:85.94%\n",
            "Training: Epoch[002/005] Iteration[192/2674] Loss: 0.4730 Acc:86.00%\n",
            "Training: Epoch[002/005] Iteration[200/2674] Loss: 0.2872 Acc:86.19%\n",
            "Training: Epoch[002/005] Iteration[208/2674] Loss: 0.3552 Acc:86.24%\n",
            "Training: Epoch[002/005] Iteration[216/2674] Loss: 0.5604 Acc:86.05%\n",
            "Training: Epoch[002/005] Iteration[224/2674] Loss: 0.8581 Acc:85.88%\n",
            "Training: Epoch[002/005] Iteration[232/2674] Loss: 0.3956 Acc:85.78%\n",
            "Training: Epoch[002/005] Iteration[240/2674] Loss: 0.5457 Acc:85.62%\n",
            "Training: Epoch[002/005] Iteration[248/2674] Loss: 0.6180 Acc:85.58%\n",
            "Training: Epoch[002/005] Iteration[256/2674] Loss: 0.4521 Acc:85.64%\n",
            "Training: Epoch[002/005] Iteration[264/2674] Loss: 0.1805 Acc:85.94%\n",
            "Training: Epoch[002/005] Iteration[272/2674] Loss: 0.4285 Acc:86.03%\n",
            "Training: Epoch[002/005] Iteration[280/2674] Loss: 0.2919 Acc:86.21%\n",
            "Training: Epoch[002/005] Iteration[288/2674] Loss: 0.5354 Acc:86.24%\n",
            "Training: Epoch[002/005] Iteration[296/2674] Loss: 0.2148 Acc:86.44%\n",
            "Training: Epoch[002/005] Iteration[304/2674] Loss: 0.2825 Acc:86.51%\n",
            "Training: Epoch[002/005] Iteration[312/2674] Loss: 0.2231 Acc:86.62%\n",
            "Training: Epoch[002/005] Iteration[320/2674] Loss: 0.5654 Acc:86.41%\n",
            "Training: Epoch[002/005] Iteration[328/2674] Loss: 0.3512 Acc:86.39%\n",
            "Training: Epoch[002/005] Iteration[336/2674] Loss: 0.4605 Acc:86.38%\n",
            "Training: Epoch[002/005] Iteration[344/2674] Loss: 0.3373 Acc:86.52%\n",
            "Training: Epoch[002/005] Iteration[352/2674] Loss: 0.5225 Acc:86.54%\n",
            "Training: Epoch[002/005] Iteration[360/2674] Loss: 0.3052 Acc:86.60%\n",
            "Training: Epoch[002/005] Iteration[368/2674] Loss: 0.3906 Acc:86.58%\n",
            "Training: Epoch[002/005] Iteration[376/2674] Loss: 0.3721 Acc:86.67%\n",
            "Training: Epoch[002/005] Iteration[384/2674] Loss: 0.5996 Acc:86.62%\n",
            "Training: Epoch[002/005] Iteration[392/2674] Loss: 0.4579 Acc:86.54%\n",
            "Training: Epoch[002/005] Iteration[400/2674] Loss: 0.5207 Acc:86.50%\n",
            "Training: Epoch[002/005] Iteration[408/2674] Loss: 0.3916 Acc:86.46%\n",
            "Training: Epoch[002/005] Iteration[416/2674] Loss: 0.6240 Acc:86.45%\n",
            "Training: Epoch[002/005] Iteration[424/2674] Loss: 0.3983 Acc:86.41%\n",
            "Training: Epoch[002/005] Iteration[432/2674] Loss: 0.3942 Acc:86.49%\n",
            "Training: Epoch[002/005] Iteration[440/2674] Loss: 0.2489 Acc:86.59%\n",
            "Training: Epoch[002/005] Iteration[448/2674] Loss: 0.4687 Acc:86.66%\n",
            "Training: Epoch[002/005] Iteration[456/2674] Loss: 0.4815 Acc:86.65%\n",
            "Training: Epoch[002/005] Iteration[464/2674] Loss: 0.2894 Acc:86.72%\n",
            "Training: Epoch[002/005] Iteration[472/2674] Loss: 0.2833 Acc:86.78%\n",
            "Training: Epoch[002/005] Iteration[480/2674] Loss: 0.3032 Acc:86.82%\n",
            "Training: Epoch[002/005] Iteration[488/2674] Loss: 0.4815 Acc:86.78%\n",
            "Training: Epoch[002/005] Iteration[496/2674] Loss: 0.6023 Acc:86.54%\n",
            "Training: Epoch[002/005] Iteration[504/2674] Loss: 0.3036 Acc:86.63%\n",
            "Training: Epoch[002/005] Iteration[512/2674] Loss: 0.4583 Acc:86.72%\n",
            "Training: Epoch[002/005] Iteration[520/2674] Loss: 0.2850 Acc:86.73%\n",
            "Training: Epoch[002/005] Iteration[528/2674] Loss: 0.3302 Acc:86.79%\n",
            "Training: Epoch[002/005] Iteration[536/2674] Loss: 0.2466 Acc:86.87%\n",
            "Training: Epoch[002/005] Iteration[544/2674] Loss: 0.3397 Acc:86.86%\n",
            "Training: Epoch[002/005] Iteration[552/2674] Loss: 0.2012 Acc:86.93%\n",
            "Training: Epoch[002/005] Iteration[560/2674] Loss: 0.4102 Acc:86.99%\n",
            "Training: Epoch[002/005] Iteration[568/2674] Loss: 0.4398 Acc:87.02%\n",
            "Training: Epoch[002/005] Iteration[576/2674] Loss: 0.4906 Acc:87.00%\n",
            "Training: Epoch[002/005] Iteration[584/2674] Loss: 0.4842 Acc:86.99%\n",
            "Training: Epoch[002/005] Iteration[592/2674] Loss: 0.3225 Acc:86.99%\n",
            "Training: Epoch[002/005] Iteration[600/2674] Loss: 0.2480 Acc:87.02%\n",
            "Training: Epoch[002/005] Iteration[608/2674] Loss: 0.2824 Acc:87.09%\n",
            "Training: Epoch[002/005] Iteration[616/2674] Loss: 0.4909 Acc:87.03%\n",
            "Training: Epoch[002/005] Iteration[624/2674] Loss: 0.2062 Acc:87.10%\n",
            "Training: Epoch[002/005] Iteration[632/2674] Loss: 0.3300 Acc:87.10%\n",
            "Training: Epoch[002/005] Iteration[640/2674] Loss: 0.4102 Acc:87.15%\n",
            "Training: Epoch[002/005] Iteration[648/2674] Loss: 0.4279 Acc:87.15%\n",
            "Training: Epoch[002/005] Iteration[656/2674] Loss: 0.5563 Acc:87.18%\n",
            "Training: Epoch[002/005] Iteration[664/2674] Loss: 0.2187 Acc:87.24%\n",
            "Training: Epoch[002/005] Iteration[672/2674] Loss: 0.2209 Acc:87.28%\n",
            "Training: Epoch[002/005] Iteration[680/2674] Loss: 0.4925 Acc:87.28%\n",
            "Training: Epoch[002/005] Iteration[688/2674] Loss: 0.4716 Acc:87.28%\n",
            "Training: Epoch[002/005] Iteration[696/2674] Loss: 0.3685 Acc:87.28%\n",
            "Training: Epoch[002/005] Iteration[704/2674] Loss: 0.1817 Acc:87.36%\n",
            "Training: Epoch[002/005] Iteration[712/2674] Loss: 0.2336 Acc:87.39%\n",
            "Training: Epoch[002/005] Iteration[720/2674] Loss: 0.3734 Acc:87.45%\n",
            "Training: Epoch[002/005] Iteration[728/2674] Loss: 0.1978 Acc:87.52%\n",
            "Training: Epoch[002/005] Iteration[736/2674] Loss: 0.4057 Acc:87.52%\n",
            "Training: Epoch[002/005] Iteration[744/2674] Loss: 0.8048 Acc:87.42%\n",
            "Training: Epoch[002/005] Iteration[752/2674] Loss: 0.2889 Acc:87.43%\n",
            "Training: Epoch[002/005] Iteration[760/2674] Loss: 0.3529 Acc:87.45%\n",
            "Training: Epoch[002/005] Iteration[768/2674] Loss: 0.2844 Acc:87.48%\n",
            "Training: Epoch[002/005] Iteration[776/2674] Loss: 0.4859 Acc:87.52%\n",
            "Training: Epoch[002/005] Iteration[784/2674] Loss: 0.4699 Acc:87.48%\n",
            "Training: Epoch[002/005] Iteration[792/2674] Loss: 0.7139 Acc:87.50%\n",
            "Training: Epoch[002/005] Iteration[800/2674] Loss: 0.2458 Acc:87.56%\n",
            "Training: Epoch[002/005] Iteration[808/2674] Loss: 0.1980 Acc:87.59%\n",
            "Training: Epoch[002/005] Iteration[816/2674] Loss: 0.6935 Acc:87.48%\n",
            "Training: Epoch[002/005] Iteration[824/2674] Loss: 0.4902 Acc:87.48%\n",
            "Training: Epoch[002/005] Iteration[832/2674] Loss: 0.3183 Acc:87.50%\n",
            "Training: Epoch[002/005] Iteration[840/2674] Loss: 0.1837 Acc:87.51%\n",
            "Training: Epoch[002/005] Iteration[848/2674] Loss: 0.2328 Acc:87.56%\n",
            "Training: Epoch[002/005] Iteration[856/2674] Loss: 0.2717 Acc:87.60%\n",
            "Training: Epoch[002/005] Iteration[864/2674] Loss: 0.2798 Acc:87.60%\n",
            "Training: Epoch[002/005] Iteration[872/2674] Loss: 0.4636 Acc:87.53%\n",
            "Training: Epoch[002/005] Iteration[880/2674] Loss: 0.8183 Acc:87.49%\n",
            "Training: Epoch[002/005] Iteration[888/2674] Loss: 0.5764 Acc:87.40%\n",
            "Training: Epoch[002/005] Iteration[896/2674] Loss: 0.4131 Acc:87.40%\n",
            "Training: Epoch[002/005] Iteration[904/2674] Loss: 0.3724 Acc:87.43%\n",
            "Training: Epoch[002/005] Iteration[912/2674] Loss: 0.1579 Acc:87.50%\n",
            "Training: Epoch[002/005] Iteration[920/2674] Loss: 0.5207 Acc:87.47%\n",
            "Training: Epoch[002/005] Iteration[928/2674] Loss: 0.5449 Acc:87.50%\n",
            "Training: Epoch[002/005] Iteration[936/2674] Loss: 0.3085 Acc:87.49%\n",
            "Training: Epoch[002/005] Iteration[944/2674] Loss: 0.3478 Acc:87.46%\n",
            "Training: Epoch[002/005] Iteration[952/2674] Loss: 0.2491 Acc:87.51%\n",
            "Training: Epoch[002/005] Iteration[960/2674] Loss: 0.3733 Acc:87.51%\n",
            "Training: Epoch[002/005] Iteration[968/2674] Loss: 0.1096 Acc:87.58%\n",
            "Training: Epoch[002/005] Iteration[976/2674] Loss: 0.2326 Acc:87.60%\n",
            "Training: Epoch[002/005] Iteration[984/2674] Loss: 0.1804 Acc:87.67%\n",
            "Training: Epoch[002/005] Iteration[992/2674] Loss: 0.2131 Acc:87.69%\n",
            "Training: Epoch[002/005] Iteration[1000/2674] Loss: 0.3402 Acc:87.70%\n",
            "Training: Epoch[002/005] Iteration[1008/2674] Loss: 0.3456 Acc:87.74%\n",
            "Training: Epoch[002/005] Iteration[1016/2674] Loss: 0.4063 Acc:87.75%\n",
            "Training: Epoch[002/005] Iteration[1024/2674] Loss: 0.2896 Acc:87.73%\n",
            "Training: Epoch[002/005] Iteration[1032/2674] Loss: 0.3712 Acc:87.72%\n",
            "Training: Epoch[002/005] Iteration[1040/2674] Loss: 0.2367 Acc:87.75%\n",
            "Training: Epoch[002/005] Iteration[1048/2674] Loss: 0.1243 Acc:87.81%\n",
            "Training: Epoch[002/005] Iteration[1056/2674] Loss: 0.2935 Acc:87.83%\n",
            "Training: Epoch[002/005] Iteration[1064/2674] Loss: 0.3554 Acc:87.82%\n",
            "Training: Epoch[002/005] Iteration[1072/2674] Loss: 0.4621 Acc:87.86%\n",
            "Training: Epoch[002/005] Iteration[1080/2674] Loss: 0.5068 Acc:87.87%\n",
            "Training: Epoch[002/005] Iteration[1088/2674] Loss: 0.3392 Acc:87.90%\n",
            "Training: Epoch[002/005] Iteration[1096/2674] Loss: 0.3069 Acc:87.90%\n",
            "Training: Epoch[002/005] Iteration[1104/2674] Loss: 0.3452 Acc:87.90%\n",
            "Training: Epoch[002/005] Iteration[1112/2674] Loss: 0.1817 Acc:87.94%\n",
            "Training: Epoch[002/005] Iteration[1120/2674] Loss: 0.5272 Acc:87.94%\n",
            "Training: Epoch[002/005] Iteration[1128/2674] Loss: 0.3537 Acc:87.95%\n",
            "Training: Epoch[002/005] Iteration[1136/2674] Loss: 0.1679 Acc:88.01%\n",
            "Training: Epoch[002/005] Iteration[1144/2674] Loss: 0.3491 Acc:87.99%\n",
            "Training: Epoch[002/005] Iteration[1152/2674] Loss: 0.2868 Acc:88.00%\n",
            "Training: Epoch[002/005] Iteration[1160/2674] Loss: 0.3662 Acc:88.01%\n",
            "Training: Epoch[002/005] Iteration[1168/2674] Loss: 0.4275 Acc:87.99%\n",
            "Training: Epoch[002/005] Iteration[1176/2674] Loss: 0.3706 Acc:88.00%\n",
            "Training: Epoch[002/005] Iteration[1184/2674] Loss: 0.2213 Acc:88.04%\n",
            "Training: Epoch[002/005] Iteration[1192/2674] Loss: 0.2599 Acc:88.09%\n",
            "Training: Epoch[002/005] Iteration[1200/2674] Loss: 0.5001 Acc:88.08%\n",
            "Training: Epoch[002/005] Iteration[1208/2674] Loss: 0.3005 Acc:88.10%\n",
            "Training: Epoch[002/005] Iteration[1216/2674] Loss: 0.3761 Acc:88.10%\n",
            "Training: Epoch[002/005] Iteration[1224/2674] Loss: 0.3351 Acc:88.09%\n",
            "Training: Epoch[002/005] Iteration[1232/2674] Loss: 0.2649 Acc:88.10%\n",
            "Training: Epoch[002/005] Iteration[1240/2674] Loss: 0.2817 Acc:88.10%\n",
            "Training: Epoch[002/005] Iteration[1248/2674] Loss: 0.4128 Acc:88.12%\n",
            "Training: Epoch[002/005] Iteration[1256/2674] Loss: 0.1463 Acc:88.18%\n",
            "Training: Epoch[002/005] Iteration[1264/2674] Loss: 0.2730 Acc:88.20%\n",
            "Training: Epoch[002/005] Iteration[1272/2674] Loss: 0.1855 Acc:88.24%\n",
            "Training: Epoch[002/005] Iteration[1280/2674] Loss: 0.2824 Acc:88.25%\n",
            "Training: Epoch[002/005] Iteration[1288/2674] Loss: 0.4139 Acc:88.27%\n",
            "Training: Epoch[002/005] Iteration[1296/2674] Loss: 0.3687 Acc:88.27%\n",
            "Training: Epoch[002/005] Iteration[1304/2674] Loss: 0.2711 Acc:88.27%\n",
            "Training: Epoch[002/005] Iteration[1312/2674] Loss: 0.4353 Acc:88.26%\n",
            "Training: Epoch[002/005] Iteration[1320/2674] Loss: 0.5393 Acc:88.25%\n",
            "Training: Epoch[002/005] Iteration[1328/2674] Loss: 0.1451 Acc:88.28%\n",
            "Training: Epoch[002/005] Iteration[1336/2674] Loss: 0.3804 Acc:88.27%\n",
            "Training: Epoch[002/005] Iteration[1344/2674] Loss: 0.2521 Acc:88.27%\n",
            "Training: Epoch[002/005] Iteration[1352/2674] Loss: 0.2342 Acc:88.29%\n",
            "Training: Epoch[002/005] Iteration[1360/2674] Loss: 0.6213 Acc:88.24%\n",
            "Training: Epoch[002/005] Iteration[1368/2674] Loss: 0.3736 Acc:88.23%\n",
            "Training: Epoch[002/005] Iteration[1376/2674] Loss: 0.4020 Acc:88.22%\n",
            "Training: Epoch[002/005] Iteration[1384/2674] Loss: 0.5275 Acc:88.21%\n",
            "Training: Epoch[002/005] Iteration[1392/2674] Loss: 0.5450 Acc:88.19%\n",
            "Training: Epoch[002/005] Iteration[1400/2674] Loss: 0.5774 Acc:88.19%\n",
            "Training: Epoch[002/005] Iteration[1408/2674] Loss: 0.7109 Acc:88.15%\n",
            "Training: Epoch[002/005] Iteration[1416/2674] Loss: 0.4510 Acc:88.13%\n",
            "Training: Epoch[002/005] Iteration[1424/2674] Loss: 0.2480 Acc:88.13%\n",
            "Training: Epoch[002/005] Iteration[1432/2674] Loss: 0.4343 Acc:88.12%\n",
            "Training: Epoch[002/005] Iteration[1440/2674] Loss: 0.2322 Acc:88.16%\n",
            "Training: Epoch[002/005] Iteration[1448/2674] Loss: 0.5022 Acc:88.17%\n",
            "Training: Epoch[002/005] Iteration[1456/2674] Loss: 0.7388 Acc:88.19%\n",
            "Training: Epoch[002/005] Iteration[1464/2674] Loss: 0.5400 Acc:88.15%\n",
            "Training: Epoch[002/005] Iteration[1472/2674] Loss: 0.1088 Acc:88.19%\n",
            "Training: Epoch[002/005] Iteration[1480/2674] Loss: 0.3891 Acc:88.19%\n",
            "Training: Epoch[002/005] Iteration[1488/2674] Loss: 0.6000 Acc:88.14%\n",
            "Training: Epoch[002/005] Iteration[1496/2674] Loss: 0.4569 Acc:88.10%\n",
            "Training: Epoch[002/005] Iteration[1504/2674] Loss: 0.3015 Acc:88.11%\n",
            "Training: Epoch[002/005] Iteration[1512/2674] Loss: 0.4098 Acc:88.08%\n",
            "Training: Epoch[002/005] Iteration[1520/2674] Loss: 0.2701 Acc:88.10%\n",
            "Training: Epoch[002/005] Iteration[1528/2674] Loss: 0.3270 Acc:88.11%\n",
            "Training: Epoch[002/005] Iteration[1536/2674] Loss: 0.4771 Acc:88.08%\n",
            "Training: Epoch[002/005] Iteration[1544/2674] Loss: 0.2472 Acc:88.09%\n",
            "Training: Epoch[002/005] Iteration[1552/2674] Loss: 0.4178 Acc:88.10%\n",
            "Training: Epoch[002/005] Iteration[1560/2674] Loss: 0.3960 Acc:88.09%\n",
            "Training: Epoch[002/005] Iteration[1568/2674] Loss: 0.4343 Acc:88.09%\n",
            "Training: Epoch[002/005] Iteration[1576/2674] Loss: 0.2798 Acc:88.11%\n",
            "Training: Epoch[002/005] Iteration[1584/2674] Loss: 0.6855 Acc:88.08%\n",
            "Training: Epoch[002/005] Iteration[1592/2674] Loss: 0.5484 Acc:88.07%\n",
            "Training: Epoch[002/005] Iteration[1600/2674] Loss: 0.5107 Acc:88.04%\n",
            "Training: Epoch[002/005] Iteration[1608/2674] Loss: 0.3931 Acc:88.03%\n",
            "Training: Epoch[002/005] Iteration[1616/2674] Loss: 0.5460 Acc:88.02%\n",
            "Training: Epoch[002/005] Iteration[1624/2674] Loss: 0.4422 Acc:88.00%\n",
            "Training: Epoch[002/005] Iteration[1632/2674] Loss: 0.2228 Acc:88.01%\n",
            "Training: Epoch[002/005] Iteration[1640/2674] Loss: 0.4303 Acc:87.97%\n",
            "Training: Epoch[002/005] Iteration[1648/2674] Loss: 0.4860 Acc:87.96%\n",
            "Training: Epoch[002/005] Iteration[1656/2674] Loss: 0.4674 Acc:87.94%\n",
            "Training: Epoch[002/005] Iteration[1664/2674] Loss: 0.4324 Acc:87.94%\n",
            "Training: Epoch[002/005] Iteration[1672/2674] Loss: 0.6892 Acc:87.95%\n",
            "Training: Epoch[002/005] Iteration[1680/2674] Loss: 0.4098 Acc:87.95%\n",
            "Training: Epoch[002/005] Iteration[1688/2674] Loss: 0.6342 Acc:87.94%\n",
            "Training: Epoch[002/005] Iteration[1696/2674] Loss: 0.6888 Acc:87.93%\n",
            "Training: Epoch[002/005] Iteration[1704/2674] Loss: 0.5027 Acc:87.93%\n",
            "Training: Epoch[002/005] Iteration[1712/2674] Loss: 0.6383 Acc:87.87%\n",
            "Training: Epoch[002/005] Iteration[1720/2674] Loss: 0.4328 Acc:87.87%\n",
            "Training: Epoch[002/005] Iteration[1728/2674] Loss: 0.5145 Acc:87.86%\n",
            "Training: Epoch[002/005] Iteration[1736/2674] Loss: 0.3573 Acc:87.87%\n",
            "Training: Epoch[002/005] Iteration[1744/2674] Loss: 0.2128 Acc:87.89%\n",
            "Training: Epoch[002/005] Iteration[1752/2674] Loss: 0.3867 Acc:87.89%\n",
            "Training: Epoch[002/005] Iteration[1760/2674] Loss: 0.1727 Acc:87.93%\n",
            "Training: Epoch[002/005] Iteration[1768/2674] Loss: 0.5456 Acc:87.91%\n",
            "Training: Epoch[002/005] Iteration[1776/2674] Loss: 0.3939 Acc:87.92%\n",
            "Training: Epoch[002/005] Iteration[1784/2674] Loss: 0.2955 Acc:87.93%\n",
            "Training: Epoch[002/005] Iteration[1792/2674] Loss: 0.4241 Acc:87.93%\n",
            "Training: Epoch[002/005] Iteration[1800/2674] Loss: 0.2817 Acc:87.92%\n",
            "Training: Epoch[002/005] Iteration[1808/2674] Loss: 0.5030 Acc:87.90%\n",
            "Training: Epoch[002/005] Iteration[1816/2674] Loss: 0.5332 Acc:87.91%\n",
            "Training: Epoch[002/005] Iteration[1824/2674] Loss: 0.4843 Acc:87.88%\n",
            "Training: Epoch[002/005] Iteration[1832/2674] Loss: 0.3931 Acc:87.88%\n",
            "Training: Epoch[002/005] Iteration[1840/2674] Loss: 0.2126 Acc:87.89%\n",
            "Training: Epoch[002/005] Iteration[1848/2674] Loss: 0.3930 Acc:87.90%\n",
            "Training: Epoch[002/005] Iteration[1856/2674] Loss: 0.4889 Acc:87.89%\n",
            "Training: Epoch[002/005] Iteration[1864/2674] Loss: 0.2209 Acc:87.90%\n",
            "Training: Epoch[002/005] Iteration[1872/2674] Loss: 0.5542 Acc:87.89%\n",
            "Training: Epoch[002/005] Iteration[1880/2674] Loss: 0.2124 Acc:87.91%\n",
            "Training: Epoch[002/005] Iteration[1888/2674] Loss: 0.2505 Acc:87.90%\n",
            "Training: Epoch[002/005] Iteration[1896/2674] Loss: 0.3818 Acc:87.92%\n",
            "Training: Epoch[002/005] Iteration[1904/2674] Loss: 0.1740 Acc:87.93%\n",
            "Training: Epoch[002/005] Iteration[1912/2674] Loss: 0.5049 Acc:87.94%\n",
            "Training: Epoch[002/005] Iteration[1920/2674] Loss: 0.5587 Acc:87.92%\n",
            "Training: Epoch[002/005] Iteration[1928/2674] Loss: 0.7658 Acc:87.88%\n",
            "Training: Epoch[002/005] Iteration[1936/2674] Loss: 0.3394 Acc:87.88%\n",
            "Training: Epoch[002/005] Iteration[1944/2674] Loss: 0.4732 Acc:87.89%\n",
            "Training: Epoch[002/005] Iteration[1952/2674] Loss: 0.5913 Acc:87.87%\n",
            "Training: Epoch[002/005] Iteration[1960/2674] Loss: 0.4420 Acc:87.86%\n",
            "Training: Epoch[002/005] Iteration[1968/2674] Loss: 0.3904 Acc:87.86%\n",
            "Training: Epoch[002/005] Iteration[1976/2674] Loss: 0.4981 Acc:87.84%\n",
            "Training: Epoch[002/005] Iteration[1984/2674] Loss: 0.2363 Acc:87.88%\n",
            "Training: Epoch[002/005] Iteration[1992/2674] Loss: 0.2756 Acc:87.89%\n",
            "Training: Epoch[002/005] Iteration[2000/2674] Loss: 0.3483 Acc:87.89%\n",
            "Training: Epoch[002/005] Iteration[2008/2674] Loss: 0.4735 Acc:87.89%\n",
            "Training: Epoch[002/005] Iteration[2016/2674] Loss: 0.2959 Acc:87.91%\n",
            "Training: Epoch[002/005] Iteration[2024/2674] Loss: 0.1262 Acc:87.94%\n",
            "Training: Epoch[002/005] Iteration[2032/2674] Loss: 0.3333 Acc:87.94%\n",
            "Training: Epoch[002/005] Iteration[2040/2674] Loss: 0.5732 Acc:87.95%\n",
            "Training: Epoch[002/005] Iteration[2048/2674] Loss: 0.2121 Acc:87.98%\n",
            "Training: Epoch[002/005] Iteration[2056/2674] Loss: 0.3446 Acc:87.97%\n",
            "Training: Epoch[002/005] Iteration[2064/2674] Loss: 0.6323 Acc:87.98%\n",
            "Training: Epoch[002/005] Iteration[2072/2674] Loss: 0.4407 Acc:87.95%\n",
            "Training: Epoch[002/005] Iteration[2080/2674] Loss: 0.3422 Acc:87.96%\n",
            "Training: Epoch[002/005] Iteration[2088/2674] Loss: 0.3439 Acc:87.97%\n",
            "Training: Epoch[002/005] Iteration[2096/2674] Loss: 0.1814 Acc:87.99%\n",
            "Training: Epoch[002/005] Iteration[2104/2674] Loss: 0.2866 Acc:88.00%\n",
            "Training: Epoch[002/005] Iteration[2112/2674] Loss: 0.2896 Acc:88.01%\n",
            "Training: Epoch[002/005] Iteration[2120/2674] Loss: 0.2120 Acc:88.02%\n",
            "Training: Epoch[002/005] Iteration[2128/2674] Loss: 0.4380 Acc:88.03%\n",
            "Training: Epoch[002/005] Iteration[2136/2674] Loss: 0.2300 Acc:88.04%\n",
            "Training: Epoch[002/005] Iteration[2144/2674] Loss: 0.5393 Acc:88.02%\n",
            "Training: Epoch[002/005] Iteration[2152/2674] Loss: 0.3762 Acc:88.03%\n",
            "Training: Epoch[002/005] Iteration[2160/2674] Loss: 0.2102 Acc:88.05%\n",
            "Training: Epoch[002/005] Iteration[2168/2674] Loss: 0.3194 Acc:88.05%\n",
            "Training: Epoch[002/005] Iteration[2176/2674] Loss: 0.6797 Acc:88.04%\n",
            "Training: Epoch[002/005] Iteration[2184/2674] Loss: 0.3018 Acc:88.06%\n",
            "Training: Epoch[002/005] Iteration[2192/2674] Loss: 0.5603 Acc:88.04%\n",
            "Training: Epoch[002/005] Iteration[2200/2674] Loss: 0.3146 Acc:88.05%\n",
            "Training: Epoch[002/005] Iteration[2208/2674] Loss: 0.4624 Acc:88.04%\n",
            "Training: Epoch[002/005] Iteration[2216/2674] Loss: 0.2880 Acc:88.05%\n",
            "Training: Epoch[002/005] Iteration[2224/2674] Loss: 0.4679 Acc:88.03%\n",
            "Training: Epoch[002/005] Iteration[2232/2674] Loss: 0.1265 Acc:88.05%\n",
            "Training: Epoch[002/005] Iteration[2240/2674] Loss: 0.3526 Acc:88.04%\n",
            "Training: Epoch[002/005] Iteration[2248/2674] Loss: 0.4791 Acc:88.04%\n",
            "Training: Epoch[002/005] Iteration[2256/2674] Loss: 0.4128 Acc:88.04%\n",
            "Training: Epoch[002/005] Iteration[2264/2674] Loss: 0.3526 Acc:88.05%\n",
            "Training: Epoch[002/005] Iteration[2272/2674] Loss: 0.2283 Acc:88.07%\n",
            "Training: Epoch[002/005] Iteration[2280/2674] Loss: 0.5365 Acc:88.08%\n",
            "Training: Epoch[002/005] Iteration[2288/2674] Loss: 0.4387 Acc:88.06%\n",
            "Training: Epoch[002/005] Iteration[2296/2674] Loss: 0.7336 Acc:88.02%\n",
            "Training: Epoch[002/005] Iteration[2304/2674] Loss: 0.1686 Acc:88.04%\n",
            "Training: Epoch[002/005] Iteration[2312/2674] Loss: 0.6654 Acc:88.04%\n",
            "Training: Epoch[002/005] Iteration[2320/2674] Loss: 0.4233 Acc:88.05%\n",
            "Training: Epoch[002/005] Iteration[2328/2674] Loss: 0.3013 Acc:88.05%\n",
            "Training: Epoch[002/005] Iteration[2336/2674] Loss: 0.3222 Acc:88.06%\n",
            "Training: Epoch[002/005] Iteration[2344/2674] Loss: 0.4852 Acc:88.05%\n",
            "Training: Epoch[002/005] Iteration[2352/2674] Loss: 0.3468 Acc:88.05%\n",
            "Training: Epoch[002/005] Iteration[2360/2674] Loss: 0.3033 Acc:88.06%\n",
            "Training: Epoch[002/005] Iteration[2368/2674] Loss: 0.4038 Acc:88.05%\n",
            "Training: Epoch[002/005] Iteration[2376/2674] Loss: 0.4787 Acc:88.06%\n",
            "Training: Epoch[002/005] Iteration[2384/2674] Loss: 0.2579 Acc:88.07%\n",
            "Training: Epoch[002/005] Iteration[2392/2674] Loss: 0.4418 Acc:88.07%\n",
            "Training: Epoch[002/005] Iteration[2400/2674] Loss: 0.5390 Acc:88.06%\n",
            "Training: Epoch[002/005] Iteration[2408/2674] Loss: 0.2233 Acc:88.07%\n",
            "Training: Epoch[002/005] Iteration[2416/2674] Loss: 0.2201 Acc:88.07%\n",
            "Training: Epoch[002/005] Iteration[2424/2674] Loss: 0.3325 Acc:88.08%\n",
            "Training: Epoch[002/005] Iteration[2432/2674] Loss: 0.2033 Acc:88.10%\n",
            "Training: Epoch[002/005] Iteration[2440/2674] Loss: 0.4786 Acc:88.09%\n",
            "Training: Epoch[002/005] Iteration[2448/2674] Loss: 0.3298 Acc:88.09%\n",
            "Training: Epoch[002/005] Iteration[2456/2674] Loss: 0.4163 Acc:88.09%\n",
            "Training: Epoch[002/005] Iteration[2464/2674] Loss: 0.1703 Acc:88.10%\n",
            "Training: Epoch[002/005] Iteration[2472/2674] Loss: 0.2549 Acc:88.11%\n",
            "Training: Epoch[002/005] Iteration[2480/2674] Loss: 0.2350 Acc:88.13%\n",
            "Training: Epoch[002/005] Iteration[2488/2674] Loss: 0.1626 Acc:88.14%\n",
            "Training: Epoch[002/005] Iteration[2496/2674] Loss: 0.3931 Acc:88.14%\n",
            "Training: Epoch[002/005] Iteration[2504/2674] Loss: 0.5340 Acc:88.14%\n",
            "Training: Epoch[002/005] Iteration[2512/2674] Loss: 0.1685 Acc:88.17%\n",
            "Training: Epoch[002/005] Iteration[2520/2674] Loss: 0.2258 Acc:88.18%\n",
            "Training: Epoch[002/005] Iteration[2528/2674] Loss: 0.4858 Acc:88.17%\n",
            "Training: Epoch[002/005] Iteration[2536/2674] Loss: 0.9318 Acc:88.16%\n",
            "Training: Epoch[002/005] Iteration[2544/2674] Loss: 0.3698 Acc:88.15%\n",
            "Training: Epoch[002/005] Iteration[2552/2674] Loss: 0.4585 Acc:88.13%\n",
            "Training: Epoch[002/005] Iteration[2560/2674] Loss: 0.4427 Acc:88.12%\n",
            "Training: Epoch[002/005] Iteration[2568/2674] Loss: 0.4366 Acc:88.12%\n",
            "Training: Epoch[002/005] Iteration[2576/2674] Loss: 0.6808 Acc:88.10%\n",
            "Training: Epoch[002/005] Iteration[2584/2674] Loss: 0.3831 Acc:88.10%\n",
            "Training: Epoch[002/005] Iteration[2592/2674] Loss: 0.3453 Acc:88.10%\n",
            "Training: Epoch[002/005] Iteration[2600/2674] Loss: 0.2899 Acc:88.11%\n",
            "Training: Epoch[002/005] Iteration[2608/2674] Loss: 0.1765 Acc:88.13%\n",
            "Training: Epoch[002/005] Iteration[2616/2674] Loss: 0.3944 Acc:88.12%\n",
            "Training: Epoch[002/005] Iteration[2624/2674] Loss: 0.2528 Acc:88.13%\n",
            "Training: Epoch[002/005] Iteration[2632/2674] Loss: 0.2697 Acc:88.14%\n",
            "Training: Epoch[002/005] Iteration[2640/2674] Loss: 0.2836 Acc:88.14%\n",
            "Training: Epoch[002/005] Iteration[2648/2674] Loss: 0.3423 Acc:88.16%\n",
            "Training: Epoch[002/005] Iteration[2656/2674] Loss: 0.3617 Acc:88.16%\n",
            "Training: Epoch[002/005] Iteration[2664/2674] Loss: 0.1430 Acc:88.19%\n",
            "Training: Epoch[002/005] Iteration[2672/2674] Loss: 0.7994 Acc:88.18%\n",
            "Training: Epoch[003/005] Iteration[008/2674] Loss: 0.3611 Acc:89.06%\n",
            "Training: Epoch[003/005] Iteration[016/2674] Loss: 0.2218 Acc:88.28%\n",
            "Training: Epoch[003/005] Iteration[024/2674] Loss: 0.4905 Acc:90.10%\n",
            "Training: Epoch[003/005] Iteration[032/2674] Loss: 0.8263 Acc:87.89%\n",
            "Training: Epoch[003/005] Iteration[040/2674] Loss: 0.5190 Acc:86.56%\n",
            "Training: Epoch[003/005] Iteration[048/2674] Loss: 0.3581 Acc:86.72%\n",
            "Training: Epoch[003/005] Iteration[056/2674] Loss: 0.3755 Acc:87.05%\n",
            "Training: Epoch[003/005] Iteration[064/2674] Loss: 0.3947 Acc:87.11%\n",
            "Training: Epoch[003/005] Iteration[072/2674] Loss: 0.3828 Acc:87.50%\n",
            "Training: Epoch[003/005] Iteration[080/2674] Loss: 0.1374 Acc:88.44%\n",
            "Training: Epoch[003/005] Iteration[088/2674] Loss: 0.2437 Acc:88.92%\n",
            "Training: Epoch[003/005] Iteration[096/2674] Loss: 0.3299 Acc:89.19%\n",
            "Training: Epoch[003/005] Iteration[104/2674] Loss: 0.3433 Acc:89.30%\n",
            "Training: Epoch[003/005] Iteration[112/2674] Loss: 0.1835 Acc:89.62%\n",
            "Training: Epoch[003/005] Iteration[120/2674] Loss: 0.2589 Acc:89.79%\n",
            "Training: Epoch[003/005] Iteration[128/2674] Loss: 0.4960 Acc:89.45%\n",
            "Training: Epoch[003/005] Iteration[136/2674] Loss: 0.1677 Acc:89.71%\n",
            "Training: Epoch[003/005] Iteration[144/2674] Loss: 0.2715 Acc:89.93%\n",
            "Training: Epoch[003/005] Iteration[152/2674] Loss: 0.1351 Acc:90.21%\n",
            "Training: Epoch[003/005] Iteration[160/2674] Loss: 0.2653 Acc:90.47%\n",
            "Training: Epoch[003/005] Iteration[168/2674] Loss: 0.3695 Acc:90.33%\n",
            "Training: Epoch[003/005] Iteration[176/2674] Loss: 0.1567 Acc:90.55%\n",
            "Training: Epoch[003/005] Iteration[184/2674] Loss: 0.2104 Acc:90.76%\n",
            "Training: Epoch[003/005] Iteration[192/2674] Loss: 0.2921 Acc:90.62%\n",
            "Training: Epoch[003/005] Iteration[200/2674] Loss: 0.3429 Acc:90.81%\n",
            "Training: Epoch[003/005] Iteration[208/2674] Loss: 0.4143 Acc:90.69%\n",
            "Training: Epoch[003/005] Iteration[216/2674] Loss: 0.1126 Acc:90.86%\n",
            "Training: Epoch[003/005] Iteration[224/2674] Loss: 0.6617 Acc:90.62%\n",
            "Training: Epoch[003/005] Iteration[232/2674] Loss: 0.0970 Acc:90.84%\n",
            "Training: Epoch[003/005] Iteration[240/2674] Loss: 0.4067 Acc:90.78%\n",
            "Training: Epoch[003/005] Iteration[248/2674] Loss: 0.4282 Acc:90.62%\n",
            "Training: Epoch[003/005] Iteration[256/2674] Loss: 0.2397 Acc:90.67%\n",
            "Training: Epoch[003/005] Iteration[264/2674] Loss: 0.3230 Acc:90.58%\n",
            "Training: Epoch[003/005] Iteration[272/2674] Loss: 0.2924 Acc:90.58%\n",
            "Training: Epoch[003/005] Iteration[280/2674] Loss: 0.3922 Acc:90.31%\n",
            "Training: Epoch[003/005] Iteration[288/2674] Loss: 0.2145 Acc:90.36%\n",
            "Training: Epoch[003/005] Iteration[296/2674] Loss: 0.1847 Acc:90.41%\n",
            "Training: Epoch[003/005] Iteration[304/2674] Loss: 0.2143 Acc:90.46%\n",
            "Training: Epoch[003/005] Iteration[312/2674] Loss: 0.1880 Acc:90.58%\n",
            "Training: Epoch[003/005] Iteration[320/2674] Loss: 0.2980 Acc:90.55%\n",
            "Training: Epoch[003/005] Iteration[328/2674] Loss: 0.2553 Acc:90.55%\n",
            "Training: Epoch[003/005] Iteration[336/2674] Loss: 0.2924 Acc:90.51%\n",
            "Training: Epoch[003/005] Iteration[344/2674] Loss: 0.6927 Acc:90.41%\n",
            "Training: Epoch[003/005] Iteration[352/2674] Loss: 0.2047 Acc:90.45%\n",
            "Training: Epoch[003/005] Iteration[360/2674] Loss: 0.5242 Acc:90.31%\n",
            "Training: Epoch[003/005] Iteration[368/2674] Loss: 0.3311 Acc:90.29%\n",
            "Training: Epoch[003/005] Iteration[376/2674] Loss: 0.2202 Acc:90.29%\n",
            "Training: Epoch[003/005] Iteration[384/2674] Loss: 0.4924 Acc:90.27%\n",
            "Training: Epoch[003/005] Iteration[392/2674] Loss: 0.0871 Acc:90.43%\n",
            "Training: Epoch[003/005] Iteration[400/2674] Loss: 0.1477 Acc:90.50%\n",
            "Training: Epoch[003/005] Iteration[408/2674] Loss: 0.5003 Acc:90.38%\n",
            "Training: Epoch[003/005] Iteration[416/2674] Loss: 0.4107 Acc:90.32%\n",
            "Training: Epoch[003/005] Iteration[424/2674] Loss: 0.4199 Acc:90.30%\n",
            "Training: Epoch[003/005] Iteration[432/2674] Loss: 0.5702 Acc:90.22%\n",
            "Training: Epoch[003/005] Iteration[440/2674] Loss: 0.7296 Acc:90.17%\n",
            "Training: Epoch[003/005] Iteration[448/2674] Loss: 0.4060 Acc:90.15%\n",
            "Training: Epoch[003/005] Iteration[456/2674] Loss: 0.4720 Acc:90.19%\n",
            "Training: Epoch[003/005] Iteration[464/2674] Loss: 0.5630 Acc:90.11%\n",
            "Training: Epoch[003/005] Iteration[472/2674] Loss: 0.5164 Acc:90.04%\n",
            "Training: Epoch[003/005] Iteration[480/2674] Loss: 0.4735 Acc:90.03%\n",
            "Training: Epoch[003/005] Iteration[488/2674] Loss: 0.3002 Acc:90.04%\n",
            "Training: Epoch[003/005] Iteration[496/2674] Loss: 0.4383 Acc:89.99%\n",
            "Training: Epoch[003/005] Iteration[504/2674] Loss: 0.3018 Acc:89.98%\n",
            "Training: Epoch[003/005] Iteration[512/2674] Loss: 0.4340 Acc:89.97%\n",
            "Training: Epoch[003/005] Iteration[520/2674] Loss: 0.3780 Acc:89.93%\n",
            "Training: Epoch[003/005] Iteration[528/2674] Loss: 0.3035 Acc:89.94%\n",
            "Training: Epoch[003/005] Iteration[536/2674] Loss: 0.4430 Acc:89.93%\n",
            "Training: Epoch[003/005] Iteration[544/2674] Loss: 0.2598 Acc:89.94%\n",
            "Training: Epoch[003/005] Iteration[552/2674] Loss: 0.3456 Acc:89.99%\n",
            "Training: Epoch[003/005] Iteration[560/2674] Loss: 0.1715 Acc:90.07%\n",
            "Training: Epoch[003/005] Iteration[568/2674] Loss: 0.2637 Acc:90.07%\n",
            "Training: Epoch[003/005] Iteration[576/2674] Loss: 0.3626 Acc:90.10%\n",
            "Training: Epoch[003/005] Iteration[584/2674] Loss: 0.3125 Acc:90.09%\n",
            "Training: Epoch[003/005] Iteration[592/2674] Loss: 0.2158 Acc:90.14%\n",
            "Training: Epoch[003/005] Iteration[600/2674] Loss: 0.2976 Acc:90.12%\n",
            "Training: Epoch[003/005] Iteration[608/2674] Loss: 0.4831 Acc:90.07%\n",
            "Training: Epoch[003/005] Iteration[616/2674] Loss: 0.2815 Acc:90.08%\n",
            "Training: Epoch[003/005] Iteration[624/2674] Loss: 0.1778 Acc:90.12%\n",
            "Training: Epoch[003/005] Iteration[632/2674] Loss: 0.2769 Acc:90.15%\n",
            "Training: Epoch[003/005] Iteration[640/2674] Loss: 0.1917 Acc:90.16%\n",
            "Training: Epoch[003/005] Iteration[648/2674] Loss: 0.2709 Acc:90.14%\n",
            "Training: Epoch[003/005] Iteration[656/2674] Loss: 0.0364 Acc:90.26%\n",
            "Training: Epoch[003/005] Iteration[664/2674] Loss: 0.3814 Acc:90.27%\n",
            "Training: Epoch[003/005] Iteration[672/2674] Loss: 0.3420 Acc:90.29%\n",
            "Training: Epoch[003/005] Iteration[680/2674] Loss: 0.4648 Acc:90.22%\n",
            "Training: Epoch[003/005] Iteration[688/2674] Loss: 0.3400 Acc:90.17%\n",
            "Training: Epoch[003/005] Iteration[696/2674] Loss: 0.3561 Acc:90.16%\n",
            "Training: Epoch[003/005] Iteration[704/2674] Loss: 0.3047 Acc:90.16%\n",
            "Training: Epoch[003/005] Iteration[712/2674] Loss: 0.1528 Acc:90.26%\n",
            "Training: Epoch[003/005] Iteration[720/2674] Loss: 0.1018 Acc:90.33%\n",
            "Training: Epoch[003/005] Iteration[728/2674] Loss: 0.3468 Acc:90.33%\n",
            "Training: Epoch[003/005] Iteration[736/2674] Loss: 0.4359 Acc:90.30%\n",
            "Training: Epoch[003/005] Iteration[744/2674] Loss: 0.5323 Acc:90.26%\n",
            "Training: Epoch[003/005] Iteration[752/2674] Loss: 0.2499 Acc:90.26%\n",
            "Training: Epoch[003/005] Iteration[760/2674] Loss: 0.3285 Acc:90.23%\n",
            "Training: Epoch[003/005] Iteration[768/2674] Loss: 0.1473 Acc:90.28%\n",
            "Training: Epoch[003/005] Iteration[776/2674] Loss: 0.3508 Acc:90.22%\n",
            "Training: Epoch[003/005] Iteration[784/2674] Loss: 0.3468 Acc:90.21%\n",
            "Training: Epoch[003/005] Iteration[792/2674] Loss: 0.1769 Acc:90.21%\n",
            "Training: Epoch[003/005] Iteration[800/2674] Loss: 0.1617 Acc:90.23%\n",
            "Training: Epoch[003/005] Iteration[808/2674] Loss: 0.1004 Acc:90.28%\n",
            "Training: Epoch[003/005] Iteration[816/2674] Loss: 0.2332 Acc:90.32%\n",
            "Training: Epoch[003/005] Iteration[824/2674] Loss: 0.1131 Acc:90.37%\n",
            "Training: Epoch[003/005] Iteration[832/2674] Loss: 0.1368 Acc:90.40%\n",
            "Training: Epoch[003/005] Iteration[840/2674] Loss: 0.4607 Acc:90.40%\n",
            "Training: Epoch[003/005] Iteration[848/2674] Loss: 0.3749 Acc:90.42%\n",
            "Training: Epoch[003/005] Iteration[856/2674] Loss: 0.1993 Acc:90.46%\n",
            "Training: Epoch[003/005] Iteration[864/2674] Loss: 0.2460 Acc:90.47%\n",
            "Training: Epoch[003/005] Iteration[872/2674] Loss: 0.2295 Acc:90.51%\n",
            "Training: Epoch[003/005] Iteration[880/2674] Loss: 0.3999 Acc:90.50%\n",
            "Training: Epoch[003/005] Iteration[888/2674] Loss: 0.4301 Acc:90.51%\n",
            "Training: Epoch[003/005] Iteration[896/2674] Loss: 0.3947 Acc:90.47%\n",
            "Training: Epoch[003/005] Iteration[904/2674] Loss: 0.3475 Acc:90.45%\n",
            "Training: Epoch[003/005] Iteration[912/2674] Loss: 0.3153 Acc:90.43%\n",
            "Training: Epoch[003/005] Iteration[920/2674] Loss: 0.2471 Acc:90.43%\n",
            "Training: Epoch[003/005] Iteration[928/2674] Loss: 0.0839 Acc:90.48%\n",
            "Training: Epoch[003/005] Iteration[936/2674] Loss: 0.3974 Acc:90.44%\n",
            "Training: Epoch[003/005] Iteration[944/2674] Loss: 0.5026 Acc:90.43%\n",
            "Training: Epoch[003/005] Iteration[952/2674] Loss: 0.3127 Acc:90.40%\n",
            "Training: Epoch[003/005] Iteration[960/2674] Loss: 0.3169 Acc:90.38%\n",
            "Training: Epoch[003/005] Iteration[968/2674] Loss: 0.3209 Acc:90.38%\n",
            "Training: Epoch[003/005] Iteration[976/2674] Loss: 0.3949 Acc:90.39%\n",
            "Training: Epoch[003/005] Iteration[984/2674] Loss: 0.2163 Acc:90.42%\n",
            "Training: Epoch[003/005] Iteration[992/2674] Loss: 0.6435 Acc:90.40%\n",
            "Training: Epoch[003/005] Iteration[1000/2674] Loss: 0.3055 Acc:90.38%\n",
            "Training: Epoch[003/005] Iteration[1008/2674] Loss: 0.2287 Acc:90.38%\n",
            "Training: Epoch[003/005] Iteration[1016/2674] Loss: 0.8159 Acc:90.35%\n",
            "Training: Epoch[003/005] Iteration[1024/2674] Loss: 0.4348 Acc:90.33%\n",
            "Training: Epoch[003/005] Iteration[1032/2674] Loss: 0.2555 Acc:90.32%\n",
            "Training: Epoch[003/005] Iteration[1040/2674] Loss: 0.5816 Acc:90.29%\n",
            "Training: Epoch[003/005] Iteration[1048/2674] Loss: 0.4150 Acc:90.30%\n",
            "Training: Epoch[003/005] Iteration[1056/2674] Loss: 0.2815 Acc:90.33%\n",
            "Training: Epoch[003/005] Iteration[1064/2674] Loss: 0.5626 Acc:90.30%\n",
            "Training: Epoch[003/005] Iteration[1072/2674] Loss: 0.4975 Acc:90.25%\n",
            "Training: Epoch[003/005] Iteration[1080/2674] Loss: 0.4727 Acc:90.23%\n",
            "Training: Epoch[003/005] Iteration[1088/2674] Loss: 0.0946 Acc:90.27%\n",
            "Training: Epoch[003/005] Iteration[1096/2674] Loss: 0.3863 Acc:90.26%\n",
            "Training: Epoch[003/005] Iteration[1104/2674] Loss: 0.4846 Acc:90.23%\n",
            "Training: Epoch[003/005] Iteration[1112/2674] Loss: 0.4586 Acc:90.16%\n",
            "Training: Epoch[003/005] Iteration[1120/2674] Loss: 0.1950 Acc:90.17%\n",
            "Training: Epoch[003/005] Iteration[1128/2674] Loss: 0.1982 Acc:90.20%\n",
            "Training: Epoch[003/005] Iteration[1136/2674] Loss: 0.1510 Acc:90.24%\n",
            "Training: Epoch[003/005] Iteration[1144/2674] Loss: 0.1535 Acc:90.28%\n",
            "Training: Epoch[003/005] Iteration[1152/2674] Loss: 0.3002 Acc:90.26%\n",
            "Training: Epoch[003/005] Iteration[1160/2674] Loss: 0.4267 Acc:90.27%\n",
            "Training: Epoch[003/005] Iteration[1168/2674] Loss: 0.6004 Acc:90.24%\n",
            "Training: Epoch[003/005] Iteration[1176/2674] Loss: 0.2266 Acc:90.24%\n",
            "Training: Epoch[003/005] Iteration[1184/2674] Loss: 0.1973 Acc:90.22%\n",
            "Training: Epoch[003/005] Iteration[1192/2674] Loss: 0.1607 Acc:90.25%\n",
            "Training: Epoch[003/005] Iteration[1200/2674] Loss: 0.3120 Acc:90.26%\n",
            "Training: Epoch[003/005] Iteration[1208/2674] Loss: 0.4158 Acc:90.27%\n",
            "Training: Epoch[003/005] Iteration[1216/2674] Loss: 0.1318 Acc:90.31%\n",
            "Training: Epoch[003/005] Iteration[1224/2674] Loss: 0.2284 Acc:90.35%\n",
            "Training: Epoch[003/005] Iteration[1232/2674] Loss: 0.1499 Acc:90.39%\n",
            "Training: Epoch[003/005] Iteration[1240/2674] Loss: 0.4267 Acc:90.40%\n",
            "Training: Epoch[003/005] Iteration[1248/2674] Loss: 0.1166 Acc:90.43%\n",
            "Training: Epoch[003/005] Iteration[1256/2674] Loss: 0.6515 Acc:90.38%\n",
            "Training: Epoch[003/005] Iteration[1264/2674] Loss: 0.4433 Acc:90.33%\n",
            "Training: Epoch[003/005] Iteration[1272/2674] Loss: 0.2587 Acc:90.34%\n",
            "Training: Epoch[003/005] Iteration[1280/2674] Loss: 0.1119 Acc:90.38%\n",
            "Training: Epoch[003/005] Iteration[1288/2674] Loss: 0.6342 Acc:90.36%\n",
            "Training: Epoch[003/005] Iteration[1296/2674] Loss: 0.3049 Acc:90.37%\n",
            "Training: Epoch[003/005] Iteration[1304/2674] Loss: 0.5161 Acc:90.37%\n",
            "Training: Epoch[003/005] Iteration[1312/2674] Loss: 0.2575 Acc:90.36%\n",
            "Training: Epoch[003/005] Iteration[1320/2674] Loss: 0.2432 Acc:90.36%\n",
            "Training: Epoch[003/005] Iteration[1328/2674] Loss: 0.2102 Acc:90.38%\n",
            "Training: Epoch[003/005] Iteration[1336/2674] Loss: 0.3706 Acc:90.38%\n",
            "Training: Epoch[003/005] Iteration[1344/2674] Loss: 0.2094 Acc:90.37%\n",
            "Training: Epoch[003/005] Iteration[1352/2674] Loss: 0.1889 Acc:90.39%\n",
            "Training: Epoch[003/005] Iteration[1360/2674] Loss: 0.1944 Acc:90.41%\n",
            "Training: Epoch[003/005] Iteration[1368/2674] Loss: 0.2454 Acc:90.41%\n",
            "Training: Epoch[003/005] Iteration[1376/2674] Loss: 0.1652 Acc:90.44%\n",
            "Training: Epoch[003/005] Iteration[1384/2674] Loss: 0.1492 Acc:90.46%\n",
            "Training: Epoch[003/005] Iteration[1392/2674] Loss: 0.2449 Acc:90.45%\n",
            "Training: Epoch[003/005] Iteration[1400/2674] Loss: 0.3056 Acc:90.45%\n",
            "Training: Epoch[003/005] Iteration[1408/2674] Loss: 0.1203 Acc:90.47%\n",
            "Training: Epoch[003/005] Iteration[1416/2674] Loss: 0.1238 Acc:90.50%\n",
            "Training: Epoch[003/005] Iteration[1424/2674] Loss: 0.2678 Acc:90.52%\n",
            "Training: Epoch[003/005] Iteration[1432/2674] Loss: 0.2949 Acc:90.53%\n",
            "Training: Epoch[003/005] Iteration[1440/2674] Loss: 0.4420 Acc:90.52%\n",
            "Training: Epoch[003/005] Iteration[1448/2674] Loss: 0.1431 Acc:90.54%\n",
            "Training: Epoch[003/005] Iteration[1456/2674] Loss: 0.2895 Acc:90.53%\n",
            "Training: Epoch[003/005] Iteration[1464/2674] Loss: 0.4400 Acc:90.51%\n",
            "Training: Epoch[003/005] Iteration[1472/2674] Loss: 0.2028 Acc:90.51%\n",
            "Training: Epoch[003/005] Iteration[1480/2674] Loss: 0.5832 Acc:90.50%\n",
            "Training: Epoch[003/005] Iteration[1488/2674] Loss: 0.5178 Acc:90.48%\n",
            "Training: Epoch[003/005] Iteration[1496/2674] Loss: 0.3490 Acc:90.49%\n",
            "Training: Epoch[003/005] Iteration[1504/2674] Loss: 0.3152 Acc:90.49%\n",
            "Training: Epoch[003/005] Iteration[1512/2674] Loss: 0.3098 Acc:90.49%\n",
            "Training: Epoch[003/005] Iteration[1520/2674] Loss: 0.5612 Acc:90.49%\n",
            "Training: Epoch[003/005] Iteration[1528/2674] Loss: 0.2066 Acc:90.49%\n",
            "Training: Epoch[003/005] Iteration[1536/2674] Loss: 0.4448 Acc:90.48%\n",
            "Training: Epoch[003/005] Iteration[1544/2674] Loss: 0.3123 Acc:90.49%\n",
            "Training: Epoch[003/005] Iteration[1552/2674] Loss: 0.6719 Acc:90.47%\n",
            "Training: Epoch[003/005] Iteration[1560/2674] Loss: 0.4440 Acc:90.49%\n",
            "Training: Epoch[003/005] Iteration[1568/2674] Loss: 0.3292 Acc:90.47%\n",
            "Training: Epoch[003/005] Iteration[1576/2674] Loss: 0.1763 Acc:90.49%\n",
            "Training: Epoch[003/005] Iteration[1584/2674] Loss: 0.0687 Acc:90.53%\n",
            "Training: Epoch[003/005] Iteration[1592/2674] Loss: 0.4878 Acc:90.55%\n",
            "Training: Epoch[003/005] Iteration[1600/2674] Loss: 0.2842 Acc:90.56%\n",
            "Training: Epoch[003/005] Iteration[1608/2674] Loss: 0.1868 Acc:90.59%\n",
            "Training: Epoch[003/005] Iteration[1616/2674] Loss: 0.3364 Acc:90.58%\n",
            "Training: Epoch[003/005] Iteration[1624/2674] Loss: 0.3504 Acc:90.58%\n",
            "Training: Epoch[003/005] Iteration[1632/2674] Loss: 0.2581 Acc:90.57%\n",
            "Training: Epoch[003/005] Iteration[1640/2674] Loss: 0.4003 Acc:90.57%\n",
            "Training: Epoch[003/005] Iteration[1648/2674] Loss: 0.2685 Acc:90.56%\n",
            "Training: Epoch[003/005] Iteration[1656/2674] Loss: 0.2787 Acc:90.58%\n",
            "Training: Epoch[003/005] Iteration[1664/2674] Loss: 0.5714 Acc:90.54%\n",
            "Training: Epoch[003/005] Iteration[1672/2674] Loss: 0.1317 Acc:90.56%\n",
            "Training: Epoch[003/005] Iteration[1680/2674] Loss: 0.6668 Acc:90.51%\n",
            "Training: Epoch[003/005] Iteration[1688/2674] Loss: 0.2855 Acc:90.51%\n",
            "Training: Epoch[003/005] Iteration[1696/2674] Loss: 0.2259 Acc:90.52%\n",
            "Training: Epoch[003/005] Iteration[1704/2674] Loss: 0.3272 Acc:90.53%\n",
            "Training: Epoch[003/005] Iteration[1712/2674] Loss: 0.1631 Acc:90.54%\n",
            "Training: Epoch[003/005] Iteration[1720/2674] Loss: 0.1893 Acc:90.56%\n",
            "Training: Epoch[003/005] Iteration[1728/2674] Loss: 0.5217 Acc:90.57%\n",
            "Training: Epoch[003/005] Iteration[1736/2674] Loss: 0.2171 Acc:90.57%\n",
            "Training: Epoch[003/005] Iteration[1744/2674] Loss: 0.3458 Acc:90.57%\n",
            "Training: Epoch[003/005] Iteration[1752/2674] Loss: 0.4710 Acc:90.58%\n",
            "Training: Epoch[003/005] Iteration[1760/2674] Loss: 0.1723 Acc:90.60%\n",
            "Training: Epoch[003/005] Iteration[1768/2674] Loss: 0.2032 Acc:90.60%\n",
            "Training: Epoch[003/005] Iteration[1776/2674] Loss: 0.4369 Acc:90.57%\n",
            "Training: Epoch[003/005] Iteration[1784/2674] Loss: 0.2452 Acc:90.58%\n",
            "Training: Epoch[003/005] Iteration[1792/2674] Loss: 0.2419 Acc:90.58%\n",
            "Training: Epoch[003/005] Iteration[1800/2674] Loss: 0.6742 Acc:90.54%\n",
            "Training: Epoch[003/005] Iteration[1808/2674] Loss: 0.1771 Acc:90.56%\n",
            "Training: Epoch[003/005] Iteration[1816/2674] Loss: 0.2964 Acc:90.58%\n",
            "Training: Epoch[003/005] Iteration[1824/2674] Loss: 0.6074 Acc:90.56%\n",
            "Training: Epoch[003/005] Iteration[1832/2674] Loss: 0.1897 Acc:90.56%\n",
            "Training: Epoch[003/005] Iteration[1840/2674] Loss: 0.7863 Acc:90.53%\n",
            "Training: Epoch[003/005] Iteration[1848/2674] Loss: 0.2508 Acc:90.54%\n",
            "Training: Epoch[003/005] Iteration[1856/2674] Loss: 0.4666 Acc:90.51%\n",
            "Training: Epoch[003/005] Iteration[1864/2674] Loss: 0.2772 Acc:90.52%\n",
            "Training: Epoch[003/005] Iteration[1872/2674] Loss: 0.1090 Acc:90.53%\n",
            "Training: Epoch[003/005] Iteration[1880/2674] Loss: 0.6143 Acc:90.51%\n",
            "Training: Epoch[003/005] Iteration[1888/2674] Loss: 0.4033 Acc:90.49%\n",
            "Training: Epoch[003/005] Iteration[1896/2674] Loss: 0.1752 Acc:90.51%\n",
            "Training: Epoch[003/005] Iteration[1904/2674] Loss: 0.3970 Acc:90.51%\n",
            "Training: Epoch[003/005] Iteration[1912/2674] Loss: 0.3496 Acc:90.50%\n",
            "Training: Epoch[003/005] Iteration[1920/2674] Loss: 0.2655 Acc:90.49%\n",
            "Training: Epoch[003/005] Iteration[1928/2674] Loss: 0.2994 Acc:90.48%\n",
            "Training: Epoch[003/005] Iteration[1936/2674] Loss: 0.3735 Acc:90.47%\n",
            "Training: Epoch[003/005] Iteration[1944/2674] Loss: 0.4130 Acc:90.46%\n",
            "Training: Epoch[003/005] Iteration[1952/2674] Loss: 0.4254 Acc:90.46%\n",
            "Training: Epoch[003/005] Iteration[1960/2674] Loss: 0.6464 Acc:90.46%\n",
            "Training: Epoch[003/005] Iteration[1968/2674] Loss: 0.5151 Acc:90.43%\n",
            "Training: Epoch[003/005] Iteration[1976/2674] Loss: 0.2064 Acc:90.43%\n",
            "Training: Epoch[003/005] Iteration[1984/2674] Loss: 0.3073 Acc:90.44%\n",
            "Training: Epoch[003/005] Iteration[1992/2674] Loss: 0.3848 Acc:90.43%\n",
            "Training: Epoch[003/005] Iteration[2000/2674] Loss: 0.2467 Acc:90.43%\n",
            "Training: Epoch[003/005] Iteration[2008/2674] Loss: 0.4354 Acc:90.43%\n",
            "Training: Epoch[003/005] Iteration[2016/2674] Loss: 0.1494 Acc:90.45%\n",
            "Training: Epoch[003/005] Iteration[2024/2674] Loss: 0.1545 Acc:90.45%\n",
            "Training: Epoch[003/005] Iteration[2032/2674] Loss: 0.2651 Acc:90.45%\n",
            "Training: Epoch[003/005] Iteration[2040/2674] Loss: 0.4036 Acc:90.44%\n",
            "Training: Epoch[003/005] Iteration[2048/2674] Loss: 0.1225 Acc:90.47%\n",
            "Training: Epoch[003/005] Iteration[2056/2674] Loss: 0.5070 Acc:90.45%\n",
            "Training: Epoch[003/005] Iteration[2064/2674] Loss: 0.3149 Acc:90.46%\n",
            "Training: Epoch[003/005] Iteration[2072/2674] Loss: 0.4535 Acc:90.46%\n",
            "Training: Epoch[003/005] Iteration[2080/2674] Loss: 0.3498 Acc:90.46%\n",
            "Training: Epoch[003/005] Iteration[2088/2674] Loss: 0.5835 Acc:90.46%\n",
            "Training: Epoch[003/005] Iteration[2096/2674] Loss: 0.1798 Acc:90.48%\n",
            "Training: Epoch[003/005] Iteration[2104/2674] Loss: 0.6845 Acc:90.45%\n",
            "Training: Epoch[003/005] Iteration[2112/2674] Loss: 0.2798 Acc:90.46%\n",
            "Training: Epoch[003/005] Iteration[2120/2674] Loss: 0.3135 Acc:90.45%\n",
            "Training: Epoch[003/005] Iteration[2128/2674] Loss: 0.2421 Acc:90.46%\n",
            "Training: Epoch[003/005] Iteration[2136/2674] Loss: 0.2706 Acc:90.47%\n",
            "Training: Epoch[003/005] Iteration[2144/2674] Loss: 0.1935 Acc:90.48%\n",
            "Training: Epoch[003/005] Iteration[2152/2674] Loss: 0.3212 Acc:90.48%\n",
            "Training: Epoch[003/005] Iteration[2160/2674] Loss: 0.5166 Acc:90.48%\n",
            "Training: Epoch[003/005] Iteration[2168/2674] Loss: 0.4250 Acc:90.48%\n",
            "Training: Epoch[003/005] Iteration[2176/2674] Loss: 0.4432 Acc:90.45%\n",
            "Training: Epoch[003/005] Iteration[2184/2674] Loss: 0.2214 Acc:90.46%\n",
            "Training: Epoch[003/005] Iteration[2192/2674] Loss: 0.3716 Acc:90.45%\n",
            "Training: Epoch[003/005] Iteration[2200/2674] Loss: 0.2912 Acc:90.45%\n",
            "Training: Epoch[003/005] Iteration[2208/2674] Loss: 0.1571 Acc:90.47%\n",
            "Training: Epoch[003/005] Iteration[2216/2674] Loss: 0.1575 Acc:90.49%\n",
            "Training: Epoch[003/005] Iteration[2224/2674] Loss: 0.3718 Acc:90.48%\n",
            "Training: Epoch[003/005] Iteration[2232/2674] Loss: 0.5911 Acc:90.47%\n",
            "Training: Epoch[003/005] Iteration[2240/2674] Loss: 0.1697 Acc:90.49%\n",
            "Training: Epoch[003/005] Iteration[2248/2674] Loss: 0.5893 Acc:90.49%\n",
            "Training: Epoch[003/005] Iteration[2256/2674] Loss: 0.5893 Acc:90.48%\n",
            "Training: Epoch[003/005] Iteration[2264/2674] Loss: 0.6062 Acc:90.44%\n",
            "Training: Epoch[003/005] Iteration[2272/2674] Loss: 0.1221 Acc:90.46%\n",
            "Training: Epoch[003/005] Iteration[2280/2674] Loss: 0.2079 Acc:90.48%\n",
            "Training: Epoch[003/005] Iteration[2288/2674] Loss: 0.5142 Acc:90.47%\n",
            "Training: Epoch[003/005] Iteration[2296/2674] Loss: 0.3244 Acc:90.48%\n",
            "Training: Epoch[003/005] Iteration[2304/2674] Loss: 0.4293 Acc:90.46%\n",
            "Training: Epoch[003/005] Iteration[2312/2674] Loss: 0.1836 Acc:90.47%\n",
            "Training: Epoch[003/005] Iteration[2320/2674] Loss: 0.1023 Acc:90.49%\n",
            "Training: Epoch[003/005] Iteration[2328/2674] Loss: 0.1653 Acc:90.51%\n",
            "Training: Epoch[003/005] Iteration[2336/2674] Loss: 0.7165 Acc:90.50%\n",
            "Training: Epoch[003/005] Iteration[2344/2674] Loss: 0.1629 Acc:90.52%\n",
            "Training: Epoch[003/005] Iteration[2352/2674] Loss: 0.3630 Acc:90.52%\n",
            "Training: Epoch[003/005] Iteration[2360/2674] Loss: 0.2254 Acc:90.52%\n",
            "Training: Epoch[003/005] Iteration[2368/2674] Loss: 0.5570 Acc:90.52%\n",
            "Training: Epoch[003/005] Iteration[2376/2674] Loss: 0.3600 Acc:90.52%\n",
            "Training: Epoch[003/005] Iteration[2384/2674] Loss: 0.4832 Acc:90.49%\n",
            "Training: Epoch[003/005] Iteration[2392/2674] Loss: 0.2030 Acc:90.50%\n",
            "Training: Epoch[003/005] Iteration[2400/2674] Loss: 0.1566 Acc:90.51%\n",
            "Training: Epoch[003/005] Iteration[2408/2674] Loss: 0.3442 Acc:90.52%\n",
            "Training: Epoch[003/005] Iteration[2416/2674] Loss: 0.1794 Acc:90.53%\n",
            "Training: Epoch[003/005] Iteration[2424/2674] Loss: 0.2629 Acc:90.53%\n",
            "Training: Epoch[003/005] Iteration[2432/2674] Loss: 0.1985 Acc:90.53%\n",
            "Training: Epoch[003/005] Iteration[2440/2674] Loss: 0.4263 Acc:90.54%\n",
            "Training: Epoch[003/005] Iteration[2448/2674] Loss: 0.2635 Acc:90.54%\n",
            "Training: Epoch[003/005] Iteration[2456/2674] Loss: 0.5146 Acc:90.54%\n",
            "Training: Epoch[003/005] Iteration[2464/2674] Loss: 0.4105 Acc:90.53%\n",
            "Training: Epoch[003/005] Iteration[2472/2674] Loss: 0.2162 Acc:90.54%\n",
            "Training: Epoch[003/005] Iteration[2480/2674] Loss: 0.4016 Acc:90.52%\n",
            "Training: Epoch[003/005] Iteration[2488/2674] Loss: 0.3109 Acc:90.51%\n",
            "Training: Epoch[003/005] Iteration[2496/2674] Loss: 0.3311 Acc:90.52%\n",
            "Training: Epoch[003/005] Iteration[2504/2674] Loss: 0.1457 Acc:90.54%\n",
            "Training: Epoch[003/005] Iteration[2512/2674] Loss: 0.1342 Acc:90.56%\n",
            "Training: Epoch[003/005] Iteration[2520/2674] Loss: 0.4201 Acc:90.57%\n",
            "Training: Epoch[003/005] Iteration[2528/2674] Loss: 0.2900 Acc:90.56%\n",
            "Training: Epoch[003/005] Iteration[2536/2674] Loss: 0.1977 Acc:90.57%\n",
            "Training: Epoch[003/005] Iteration[2544/2674] Loss: 0.3403 Acc:90.57%\n",
            "Training: Epoch[003/005] Iteration[2552/2674] Loss: 0.2807 Acc:90.58%\n",
            "Training: Epoch[003/005] Iteration[2560/2674] Loss: 0.2935 Acc:90.58%\n",
            "Training: Epoch[003/005] Iteration[2568/2674] Loss: 0.2948 Acc:90.59%\n",
            "Training: Epoch[003/005] Iteration[2576/2674] Loss: 0.4727 Acc:90.57%\n",
            "Training: Epoch[003/005] Iteration[2584/2674] Loss: 0.3989 Acc:90.56%\n",
            "Training: Epoch[003/005] Iteration[2592/2674] Loss: 0.3347 Acc:90.56%\n",
            "Training: Epoch[003/005] Iteration[2600/2674] Loss: 0.5374 Acc:90.53%\n",
            "Training: Epoch[003/005] Iteration[2608/2674] Loss: 0.8569 Acc:90.50%\n",
            "Training: Epoch[003/005] Iteration[2616/2674] Loss: 0.2141 Acc:90.51%\n",
            "Training: Epoch[003/005] Iteration[2624/2674] Loss: 0.2464 Acc:90.51%\n",
            "Training: Epoch[003/005] Iteration[2632/2674] Loss: 0.3329 Acc:90.52%\n",
            "Training: Epoch[003/005] Iteration[2640/2674] Loss: 0.5539 Acc:90.50%\n",
            "Training: Epoch[003/005] Iteration[2648/2674] Loss: 0.1469 Acc:90.51%\n",
            "Training: Epoch[003/005] Iteration[2656/2674] Loss: 0.2868 Acc:90.51%\n",
            "Training: Epoch[003/005] Iteration[2664/2674] Loss: 0.2431 Acc:90.51%\n",
            "Training: Epoch[003/005] Iteration[2672/2674] Loss: 0.2753 Acc:90.51%\n",
            "Training: Epoch[004/005] Iteration[008/2674] Loss: 0.1095 Acc:98.44%\n",
            "Training: Epoch[004/005] Iteration[016/2674] Loss: 0.2287 Acc:96.88%\n",
            "Training: Epoch[004/005] Iteration[024/2674] Loss: 0.2706 Acc:95.31%\n",
            "Training: Epoch[004/005] Iteration[032/2674] Loss: 0.1765 Acc:94.53%\n",
            "Training: Epoch[004/005] Iteration[040/2674] Loss: 0.2089 Acc:94.69%\n",
            "Training: Epoch[004/005] Iteration[048/2674] Loss: 0.2402 Acc:94.27%\n",
            "Training: Epoch[004/005] Iteration[056/2674] Loss: 0.2633 Acc:93.97%\n",
            "Training: Epoch[004/005] Iteration[064/2674] Loss: 0.1069 Acc:94.34%\n",
            "Training: Epoch[004/005] Iteration[072/2674] Loss: 0.2014 Acc:94.27%\n",
            "Training: Epoch[004/005] Iteration[080/2674] Loss: 0.2556 Acc:94.22%\n",
            "Training: Epoch[004/005] Iteration[088/2674] Loss: 0.3420 Acc:94.18%\n",
            "Training: Epoch[004/005] Iteration[096/2674] Loss: 0.2041 Acc:94.14%\n",
            "Training: Epoch[004/005] Iteration[104/2674] Loss: 0.3915 Acc:94.11%\n",
            "Training: Epoch[004/005] Iteration[112/2674] Loss: 0.2893 Acc:94.08%\n",
            "Training: Epoch[004/005] Iteration[120/2674] Loss: 0.1972 Acc:94.06%\n",
            "Training: Epoch[004/005] Iteration[128/2674] Loss: 0.2176 Acc:93.85%\n",
            "Training: Epoch[004/005] Iteration[136/2674] Loss: 0.3064 Acc:93.66%\n",
            "Training: Epoch[004/005] Iteration[144/2674] Loss: 0.3965 Acc:93.66%\n",
            "Training: Epoch[004/005] Iteration[152/2674] Loss: 0.7079 Acc:93.34%\n",
            "Training: Epoch[004/005] Iteration[160/2674] Loss: 0.3302 Acc:93.12%\n",
            "Training: Epoch[004/005] Iteration[168/2674] Loss: 0.2054 Acc:93.08%\n",
            "Training: Epoch[004/005] Iteration[176/2674] Loss: 0.2216 Acc:93.18%\n",
            "Training: Epoch[004/005] Iteration[184/2674] Loss: 0.1589 Acc:93.27%\n",
            "Training: Epoch[004/005] Iteration[192/2674] Loss: 0.2535 Acc:93.23%\n",
            "Training: Epoch[004/005] Iteration[200/2674] Loss: 0.3023 Acc:93.12%\n",
            "Training: Epoch[004/005] Iteration[208/2674] Loss: 0.4828 Acc:93.15%\n",
            "Training: Epoch[004/005] Iteration[216/2674] Loss: 0.4165 Acc:93.00%\n",
            "Training: Epoch[004/005] Iteration[224/2674] Loss: 0.2492 Acc:92.97%\n",
            "Training: Epoch[004/005] Iteration[232/2674] Loss: 0.0712 Acc:93.10%\n",
            "Training: Epoch[004/005] Iteration[240/2674] Loss: 0.4411 Acc:92.86%\n",
            "Training: Epoch[004/005] Iteration[248/2674] Loss: 0.1716 Acc:92.74%\n",
            "Training: Epoch[004/005] Iteration[256/2674] Loss: 0.3512 Acc:92.48%\n",
            "Training: Epoch[004/005] Iteration[264/2674] Loss: 0.3019 Acc:92.47%\n",
            "Training: Epoch[004/005] Iteration[272/2674] Loss: 0.2286 Acc:92.33%\n",
            "Training: Epoch[004/005] Iteration[280/2674] Loss: 0.3612 Acc:92.23%\n",
            "Training: Epoch[004/005] Iteration[288/2674] Loss: 0.1770 Acc:92.19%\n",
            "Training: Epoch[004/005] Iteration[296/2674] Loss: 0.1065 Acc:92.19%\n",
            "Training: Epoch[004/005] Iteration[304/2674] Loss: 0.1623 Acc:92.31%\n",
            "Training: Epoch[004/005] Iteration[312/2674] Loss: 0.4472 Acc:92.27%\n",
            "Training: Epoch[004/005] Iteration[320/2674] Loss: 0.4791 Acc:92.23%\n",
            "Training: Epoch[004/005] Iteration[328/2674] Loss: 0.3554 Acc:92.19%\n",
            "Training: Epoch[004/005] Iteration[336/2674] Loss: 0.5903 Acc:92.11%\n",
            "Training: Epoch[004/005] Iteration[344/2674] Loss: 0.2960 Acc:92.11%\n",
            "Training: Epoch[004/005] Iteration[352/2674] Loss: 0.2145 Acc:92.12%\n",
            "Training: Epoch[004/005] Iteration[360/2674] Loss: 0.3697 Acc:92.05%\n",
            "Training: Epoch[004/005] Iteration[368/2674] Loss: 0.3739 Acc:92.02%\n",
            "Training: Epoch[004/005] Iteration[376/2674] Loss: 0.1483 Acc:92.05%\n",
            "Training: Epoch[004/005] Iteration[384/2674] Loss: 0.3371 Acc:92.09%\n",
            "Training: Epoch[004/005] Iteration[392/2674] Loss: 0.1895 Acc:92.16%\n",
            "Training: Epoch[004/005] Iteration[400/2674] Loss: 0.2527 Acc:92.22%\n",
            "Training: Epoch[004/005] Iteration[408/2674] Loss: 0.2948 Acc:92.22%\n",
            "Training: Epoch[004/005] Iteration[416/2674] Loss: 0.3541 Acc:92.19%\n",
            "Training: Epoch[004/005] Iteration[424/2674] Loss: 0.1524 Acc:92.25%\n",
            "Training: Epoch[004/005] Iteration[432/2674] Loss: 0.3641 Acc:92.22%\n",
            "Training: Epoch[004/005] Iteration[440/2674] Loss: 0.1763 Acc:92.24%\n",
            "Training: Epoch[004/005] Iteration[448/2674] Loss: 0.4130 Acc:92.22%\n",
            "Training: Epoch[004/005] Iteration[456/2674] Loss: 0.5065 Acc:92.13%\n",
            "Training: Epoch[004/005] Iteration[464/2674] Loss: 0.3646 Acc:92.13%\n",
            "Training: Epoch[004/005] Iteration[472/2674] Loss: 0.2219 Acc:92.13%\n",
            "Training: Epoch[004/005] Iteration[480/2674] Loss: 0.3028 Acc:92.08%\n",
            "Training: Epoch[004/005] Iteration[488/2674] Loss: 0.2482 Acc:92.06%\n",
            "Training: Epoch[004/005] Iteration[496/2674] Loss: 0.5276 Acc:92.04%\n",
            "Training: Epoch[004/005] Iteration[504/2674] Loss: 0.2773 Acc:92.06%\n",
            "Training: Epoch[004/005] Iteration[512/2674] Loss: 0.6461 Acc:92.02%\n",
            "Training: Epoch[004/005] Iteration[520/2674] Loss: 0.2182 Acc:92.04%\n",
            "Training: Epoch[004/005] Iteration[528/2674] Loss: 0.2118 Acc:92.09%\n",
            "Training: Epoch[004/005] Iteration[536/2674] Loss: 0.4149 Acc:92.05%\n",
            "Training: Epoch[004/005] Iteration[544/2674] Loss: 0.2056 Acc:92.12%\n",
            "Training: Epoch[004/005] Iteration[552/2674] Loss: 0.2269 Acc:92.07%\n",
            "Training: Epoch[004/005] Iteration[560/2674] Loss: 0.1801 Acc:92.08%\n",
            "Training: Epoch[004/005] Iteration[568/2674] Loss: 0.3445 Acc:92.03%\n",
            "Training: Epoch[004/005] Iteration[576/2674] Loss: 0.0700 Acc:92.12%\n",
            "Training: Epoch[004/005] Iteration[584/2674] Loss: 0.2537 Acc:92.14%\n",
            "Training: Epoch[004/005] Iteration[592/2674] Loss: 0.1244 Acc:92.21%\n",
            "Training: Epoch[004/005] Iteration[600/2674] Loss: 0.2931 Acc:92.21%\n",
            "Training: Epoch[004/005] Iteration[608/2674] Loss: 0.3064 Acc:92.19%\n",
            "Training: Epoch[004/005] Iteration[616/2674] Loss: 0.3879 Acc:92.23%\n",
            "Training: Epoch[004/005] Iteration[624/2674] Loss: 0.3555 Acc:92.23%\n",
            "Training: Epoch[004/005] Iteration[632/2674] Loss: 0.2331 Acc:92.21%\n",
            "Training: Epoch[004/005] Iteration[640/2674] Loss: 0.3959 Acc:92.13%\n",
            "Training: Epoch[004/005] Iteration[648/2674] Loss: 0.1660 Acc:92.11%\n",
            "Training: Epoch[004/005] Iteration[656/2674] Loss: 0.2763 Acc:92.13%\n",
            "Training: Epoch[004/005] Iteration[664/2674] Loss: 0.2673 Acc:92.17%\n",
            "Training: Epoch[004/005] Iteration[672/2674] Loss: 0.0675 Acc:92.22%\n",
            "Training: Epoch[004/005] Iteration[680/2674] Loss: 0.1811 Acc:92.22%\n",
            "Training: Epoch[004/005] Iteration[688/2674] Loss: 0.0631 Acc:92.30%\n",
            "Training: Epoch[004/005] Iteration[696/2674] Loss: 0.1172 Acc:92.31%\n",
            "Training: Epoch[004/005] Iteration[704/2674] Loss: 0.2246 Acc:92.33%\n",
            "Training: Epoch[004/005] Iteration[712/2674] Loss: 0.1429 Acc:92.40%\n",
            "Training: Epoch[004/005] Iteration[720/2674] Loss: 0.0798 Acc:92.43%\n",
            "Training: Epoch[004/005] Iteration[728/2674] Loss: 0.2527 Acc:92.45%\n",
            "Training: Epoch[004/005] Iteration[736/2674] Loss: 0.1861 Acc:92.49%\n",
            "Training: Epoch[004/005] Iteration[744/2674] Loss: 0.3990 Acc:92.49%\n",
            "Training: Epoch[004/005] Iteration[752/2674] Loss: 0.1975 Acc:92.49%\n",
            "Training: Epoch[004/005] Iteration[760/2674] Loss: 0.3677 Acc:92.48%\n",
            "Training: Epoch[004/005] Iteration[768/2674] Loss: 0.1478 Acc:92.51%\n",
            "Training: Epoch[004/005] Iteration[776/2674] Loss: 0.2574 Acc:92.53%\n",
            "Training: Epoch[004/005] Iteration[784/2674] Loss: 0.0984 Acc:92.59%\n",
            "Training: Epoch[004/005] Iteration[792/2674] Loss: 0.1191 Acc:92.61%\n",
            "Training: Epoch[004/005] Iteration[800/2674] Loss: 0.1837 Acc:92.66%\n",
            "Training: Epoch[004/005] Iteration[808/2674] Loss: 0.1210 Acc:92.67%\n",
            "Training: Epoch[004/005] Iteration[816/2674] Loss: 0.1615 Acc:92.71%\n",
            "Training: Epoch[004/005] Iteration[824/2674] Loss: 0.3701 Acc:92.70%\n",
            "Training: Epoch[004/005] Iteration[832/2674] Loss: 0.1044 Acc:92.74%\n",
            "Training: Epoch[004/005] Iteration[840/2674] Loss: 0.1613 Acc:92.77%\n",
            "Training: Epoch[004/005] Iteration[848/2674] Loss: 0.1073 Acc:92.81%\n",
            "Training: Epoch[004/005] Iteration[856/2674] Loss: 0.1917 Acc:92.82%\n",
            "Training: Epoch[004/005] Iteration[864/2674] Loss: 0.1064 Acc:92.87%\n",
            "Training: Epoch[004/005] Iteration[872/2674] Loss: 0.3599 Acc:92.86%\n",
            "Training: Epoch[004/005] Iteration[880/2674] Loss: 0.2963 Acc:92.87%\n",
            "Training: Epoch[004/005] Iteration[888/2674] Loss: 0.3992 Acc:92.84%\n",
            "Training: Epoch[004/005] Iteration[896/2674] Loss: 0.7514 Acc:92.83%\n",
            "Training: Epoch[004/005] Iteration[904/2674] Loss: 0.6390 Acc:92.78%\n",
            "Training: Epoch[004/005] Iteration[912/2674] Loss: 0.2288 Acc:92.78%\n",
            "Training: Epoch[004/005] Iteration[920/2674] Loss: 0.4681 Acc:92.76%\n",
            "Training: Epoch[004/005] Iteration[928/2674] Loss: 0.2562 Acc:92.78%\n",
            "Training: Epoch[004/005] Iteration[936/2674] Loss: 0.3345 Acc:92.76%\n",
            "Training: Epoch[004/005] Iteration[944/2674] Loss: 0.4478 Acc:92.72%\n",
            "Training: Epoch[004/005] Iteration[952/2674] Loss: 0.3320 Acc:92.71%\n",
            "Training: Epoch[004/005] Iteration[960/2674] Loss: 0.6874 Acc:92.68%\n",
            "Training: Epoch[004/005] Iteration[968/2674] Loss: 0.1989 Acc:92.68%\n",
            "Training: Epoch[004/005] Iteration[976/2674] Loss: 0.3609 Acc:92.60%\n",
            "Training: Epoch[004/005] Iteration[984/2674] Loss: 0.1286 Acc:92.63%\n",
            "Training: Epoch[004/005] Iteration[992/2674] Loss: 0.3224 Acc:92.60%\n",
            "Training: Epoch[004/005] Iteration[1000/2674] Loss: 0.1340 Acc:92.64%\n",
            "Training: Epoch[004/005] Iteration[1008/2674] Loss: 0.2503 Acc:92.63%\n",
            "Training: Epoch[004/005] Iteration[1016/2674] Loss: 0.0798 Acc:92.67%\n",
            "Training: Epoch[004/005] Iteration[1024/2674] Loss: 0.1340 Acc:92.69%\n",
            "Training: Epoch[004/005] Iteration[1032/2674] Loss: 0.3102 Acc:92.68%\n",
            "Training: Epoch[004/005] Iteration[1040/2674] Loss: 0.2502 Acc:92.69%\n",
            "Training: Epoch[004/005] Iteration[1048/2674] Loss: 0.4219 Acc:92.69%\n",
            "Training: Epoch[004/005] Iteration[1056/2674] Loss: 0.0490 Acc:92.74%\n",
            "Training: Epoch[004/005] Iteration[1064/2674] Loss: 0.3640 Acc:92.73%\n",
            "Training: Epoch[004/005] Iteration[1072/2674] Loss: 0.3688 Acc:92.74%\n",
            "Training: Epoch[004/005] Iteration[1080/2674] Loss: 0.1372 Acc:92.77%\n",
            "Training: Epoch[004/005] Iteration[1088/2674] Loss: 0.4007 Acc:92.75%\n",
            "Training: Epoch[004/005] Iteration[1096/2674] Loss: 0.4495 Acc:92.70%\n",
            "Training: Epoch[004/005] Iteration[1104/2674] Loss: 0.3001 Acc:92.66%\n",
            "Training: Epoch[004/005] Iteration[1112/2674] Loss: 0.2348 Acc:92.68%\n",
            "Training: Epoch[004/005] Iteration[1120/2674] Loss: 0.0464 Acc:92.72%\n",
            "Training: Epoch[004/005] Iteration[1128/2674] Loss: 0.2033 Acc:92.74%\n",
            "Training: Epoch[004/005] Iteration[1136/2674] Loss: 0.6626 Acc:92.73%\n",
            "Training: Epoch[004/005] Iteration[1144/2674] Loss: 0.1518 Acc:92.74%\n",
            "Training: Epoch[004/005] Iteration[1152/2674] Loss: 0.1512 Acc:92.77%\n",
            "Training: Epoch[004/005] Iteration[1160/2674] Loss: 0.0553 Acc:92.80%\n",
            "Training: Epoch[004/005] Iteration[1168/2674] Loss: 0.1799 Acc:92.80%\n",
            "Training: Epoch[004/005] Iteration[1176/2674] Loss: 1.0103 Acc:92.75%\n",
            "Training: Epoch[004/005] Iteration[1184/2674] Loss: 0.5244 Acc:92.72%\n",
            "Training: Epoch[004/005] Iteration[1192/2674] Loss: 0.2163 Acc:92.71%\n",
            "Training: Epoch[004/005] Iteration[1200/2674] Loss: 0.4742 Acc:92.68%\n",
            "Training: Epoch[004/005] Iteration[1208/2674] Loss: 0.3230 Acc:92.68%\n",
            "Training: Epoch[004/005] Iteration[1216/2674] Loss: 0.1762 Acc:92.70%\n",
            "Training: Epoch[004/005] Iteration[1224/2674] Loss: 0.2842 Acc:92.71%\n",
            "Training: Epoch[004/005] Iteration[1232/2674] Loss: 0.4309 Acc:92.69%\n",
            "Training: Epoch[004/005] Iteration[1240/2674] Loss: 0.3988 Acc:92.69%\n",
            "Training: Epoch[004/005] Iteration[1248/2674] Loss: 0.0545 Acc:92.73%\n",
            "Training: Epoch[004/005] Iteration[1256/2674] Loss: 0.0614 Acc:92.75%\n",
            "Training: Epoch[004/005] Iteration[1264/2674] Loss: 0.0896 Acc:92.78%\n",
            "Training: Epoch[004/005] Iteration[1272/2674] Loss: 0.1880 Acc:92.76%\n",
            "Training: Epoch[004/005] Iteration[1280/2674] Loss: 0.3229 Acc:92.76%\n",
            "Training: Epoch[004/005] Iteration[1288/2674] Loss: 0.2842 Acc:92.76%\n",
            "Training: Epoch[004/005] Iteration[1296/2674] Loss: 0.1859 Acc:92.77%\n",
            "Training: Epoch[004/005] Iteration[1304/2674] Loss: 0.1282 Acc:92.79%\n",
            "Training: Epoch[004/005] Iteration[1312/2674] Loss: 0.4611 Acc:92.75%\n",
            "Training: Epoch[004/005] Iteration[1320/2674] Loss: 0.4708 Acc:92.73%\n",
            "Training: Epoch[004/005] Iteration[1328/2674] Loss: 0.2436 Acc:92.72%\n",
            "Training: Epoch[004/005] Iteration[1336/2674] Loss: 0.3830 Acc:92.71%\n",
            "Training: Epoch[004/005] Iteration[1344/2674] Loss: 0.0147 Acc:92.75%\n",
            "Training: Epoch[004/005] Iteration[1352/2674] Loss: 0.1042 Acc:92.77%\n",
            "Training: Epoch[004/005] Iteration[1360/2674] Loss: 0.0638 Acc:92.79%\n",
            "Training: Epoch[004/005] Iteration[1368/2674] Loss: 0.3148 Acc:92.77%\n",
            "Training: Epoch[004/005] Iteration[1376/2674] Loss: 0.2445 Acc:92.77%\n",
            "Training: Epoch[004/005] Iteration[1384/2674] Loss: 0.1557 Acc:92.77%\n",
            "Training: Epoch[004/005] Iteration[1392/2674] Loss: 0.2308 Acc:92.78%\n",
            "Training: Epoch[004/005] Iteration[1400/2674] Loss: 0.2525 Acc:92.75%\n",
            "Training: Epoch[004/005] Iteration[1408/2674] Loss: 0.3731 Acc:92.74%\n",
            "Training: Epoch[004/005] Iteration[1416/2674] Loss: 0.2173 Acc:92.74%\n",
            "Training: Epoch[004/005] Iteration[1424/2674] Loss: 0.1190 Acc:92.76%\n",
            "Training: Epoch[004/005] Iteration[1432/2674] Loss: 0.1255 Acc:92.78%\n",
            "Training: Epoch[004/005] Iteration[1440/2674] Loss: 0.1165 Acc:92.80%\n",
            "Training: Epoch[004/005] Iteration[1448/2674] Loss: 0.1838 Acc:92.80%\n",
            "Training: Epoch[004/005] Iteration[1456/2674] Loss: 0.0675 Acc:92.82%\n",
            "Training: Epoch[004/005] Iteration[1464/2674] Loss: 0.1994 Acc:92.84%\n",
            "Training: Epoch[004/005] Iteration[1472/2674] Loss: 0.2034 Acc:92.86%\n",
            "Training: Epoch[004/005] Iteration[1480/2674] Loss: 0.4115 Acc:92.83%\n",
            "Training: Epoch[004/005] Iteration[1488/2674] Loss: 0.2114 Acc:92.84%\n",
            "Training: Epoch[004/005] Iteration[1496/2674] Loss: 0.1612 Acc:92.85%\n",
            "Training: Epoch[004/005] Iteration[1504/2674] Loss: 0.1701 Acc:92.86%\n",
            "Training: Epoch[004/005] Iteration[1512/2674] Loss: 0.6183 Acc:92.83%\n",
            "Training: Epoch[004/005] Iteration[1520/2674] Loss: 0.1174 Acc:92.85%\n",
            "Training: Epoch[004/005] Iteration[1528/2674] Loss: 0.2371 Acc:92.85%\n",
            "Training: Epoch[004/005] Iteration[1536/2674] Loss: 0.2570 Acc:92.86%\n",
            "Training: Epoch[004/005] Iteration[1544/2674] Loss: 0.2043 Acc:92.85%\n",
            "Training: Epoch[004/005] Iteration[1552/2674] Loss: 0.1148 Acc:92.87%\n",
            "Training: Epoch[004/005] Iteration[1560/2674] Loss: 0.3486 Acc:92.86%\n",
            "Training: Epoch[004/005] Iteration[1568/2674] Loss: 0.4767 Acc:92.87%\n",
            "Training: Epoch[004/005] Iteration[1576/2674] Loss: 0.1177 Acc:92.89%\n",
            "Training: Epoch[004/005] Iteration[1584/2674] Loss: 0.2298 Acc:92.89%\n",
            "Training: Epoch[004/005] Iteration[1592/2674] Loss: 0.2539 Acc:92.88%\n",
            "Training: Epoch[004/005] Iteration[1600/2674] Loss: 0.4512 Acc:92.84%\n",
            "Training: Epoch[004/005] Iteration[1608/2674] Loss: 0.2217 Acc:92.84%\n",
            "Training: Epoch[004/005] Iteration[1616/2674] Loss: 0.5791 Acc:92.84%\n",
            "Training: Epoch[004/005] Iteration[1624/2674] Loss: 0.1162 Acc:92.85%\n",
            "Training: Epoch[004/005] Iteration[1632/2674] Loss: 0.2783 Acc:92.83%\n",
            "Training: Epoch[004/005] Iteration[1640/2674] Loss: 0.0453 Acc:92.87%\n",
            "Training: Epoch[004/005] Iteration[1648/2674] Loss: 0.3247 Acc:92.85%\n",
            "Training: Epoch[004/005] Iteration[1656/2674] Loss: 0.0837 Acc:92.87%\n",
            "Training: Epoch[004/005] Iteration[1664/2674] Loss: 0.5947 Acc:92.86%\n",
            "Training: Epoch[004/005] Iteration[1672/2674] Loss: 0.1767 Acc:92.88%\n",
            "Training: Epoch[004/005] Iteration[1680/2674] Loss: 0.3642 Acc:92.83%\n",
            "Training: Epoch[004/005] Iteration[1688/2674] Loss: 0.1933 Acc:92.82%\n",
            "Training: Epoch[004/005] Iteration[1696/2674] Loss: 0.3227 Acc:92.80%\n",
            "Training: Epoch[004/005] Iteration[1704/2674] Loss: 0.1625 Acc:92.80%\n",
            "Training: Epoch[004/005] Iteration[1712/2674] Loss: 0.3073 Acc:92.79%\n",
            "Training: Epoch[004/005] Iteration[1720/2674] Loss: 0.1731 Acc:92.78%\n",
            "Training: Epoch[004/005] Iteration[1728/2674] Loss: 0.3972 Acc:92.76%\n",
            "Training: Epoch[004/005] Iteration[1736/2674] Loss: 0.3271 Acc:92.76%\n",
            "Training: Epoch[004/005] Iteration[1744/2674] Loss: 1.0471 Acc:92.74%\n",
            "Training: Epoch[004/005] Iteration[1752/2674] Loss: 0.2647 Acc:92.73%\n",
            "Training: Epoch[004/005] Iteration[1760/2674] Loss: 0.5083 Acc:92.71%\n",
            "Training: Epoch[004/005] Iteration[1768/2674] Loss: 0.3423 Acc:92.70%\n",
            "Training: Epoch[004/005] Iteration[1776/2674] Loss: 0.4702 Acc:92.71%\n",
            "Training: Epoch[004/005] Iteration[1784/2674] Loss: 0.3513 Acc:92.71%\n",
            "Training: Epoch[004/005] Iteration[1792/2674] Loss: 0.2667 Acc:92.72%\n",
            "Training: Epoch[004/005] Iteration[1800/2674] Loss: 0.2546 Acc:92.72%\n",
            "Training: Epoch[004/005] Iteration[1808/2674] Loss: 0.1728 Acc:92.73%\n",
            "Training: Epoch[004/005] Iteration[1816/2674] Loss: 0.3870 Acc:92.71%\n",
            "Training: Epoch[004/005] Iteration[1824/2674] Loss: 0.4858 Acc:92.69%\n",
            "Training: Epoch[004/005] Iteration[1832/2674] Loss: 0.7382 Acc:92.69%\n",
            "Training: Epoch[004/005] Iteration[1840/2674] Loss: 0.3354 Acc:92.68%\n",
            "Training: Epoch[004/005] Iteration[1848/2674] Loss: 0.2292 Acc:92.66%\n",
            "Training: Epoch[004/005] Iteration[1856/2674] Loss: 0.1488 Acc:92.67%\n",
            "Training: Epoch[004/005] Iteration[1864/2674] Loss: 0.2008 Acc:92.68%\n",
            "Training: Epoch[004/005] Iteration[1872/2674] Loss: 0.1226 Acc:92.69%\n",
            "Training: Epoch[004/005] Iteration[1880/2674] Loss: 0.1358 Acc:92.69%\n",
            "Training: Epoch[004/005] Iteration[1888/2674] Loss: 0.2004 Acc:92.70%\n",
            "Training: Epoch[004/005] Iteration[1896/2674] Loss: 0.1746 Acc:92.70%\n",
            "Training: Epoch[004/005] Iteration[1904/2674] Loss: 0.3037 Acc:92.67%\n",
            "Training: Epoch[004/005] Iteration[1912/2674] Loss: 0.1243 Acc:92.68%\n",
            "Training: Epoch[004/005] Iteration[1920/2674] Loss: 0.2082 Acc:92.69%\n",
            "Training: Epoch[004/005] Iteration[1928/2674] Loss: 0.1174 Acc:92.69%\n",
            "Training: Epoch[004/005] Iteration[1936/2674] Loss: 0.3788 Acc:92.67%\n",
            "Training: Epoch[004/005] Iteration[1944/2674] Loss: 0.3256 Acc:92.67%\n",
            "Training: Epoch[004/005] Iteration[1952/2674] Loss: 0.4220 Acc:92.65%\n",
            "Training: Epoch[004/005] Iteration[1960/2674] Loss: 0.3825 Acc:92.64%\n",
            "Training: Epoch[004/005] Iteration[1968/2674] Loss: 0.3294 Acc:92.64%\n",
            "Training: Epoch[004/005] Iteration[1976/2674] Loss: 0.2248 Acc:92.65%\n",
            "Training: Epoch[004/005] Iteration[1984/2674] Loss: 0.2895 Acc:92.66%\n",
            "Training: Epoch[004/005] Iteration[1992/2674] Loss: 0.3583 Acc:92.65%\n",
            "Training: Epoch[004/005] Iteration[2000/2674] Loss: 0.7598 Acc:92.63%\n",
            "Training: Epoch[004/005] Iteration[2008/2674] Loss: 0.1433 Acc:92.64%\n",
            "Training: Epoch[004/005] Iteration[2016/2674] Loss: 0.6899 Acc:92.62%\n",
            "Training: Epoch[004/005] Iteration[2024/2674] Loss: 0.0725 Acc:92.63%\n",
            "Training: Epoch[004/005] Iteration[2032/2674] Loss: 0.4856 Acc:92.62%\n",
            "Training: Epoch[004/005] Iteration[2040/2674] Loss: 0.5692 Acc:92.60%\n",
            "Training: Epoch[004/005] Iteration[2048/2674] Loss: 0.2778 Acc:92.61%\n",
            "Training: Epoch[004/005] Iteration[2056/2674] Loss: 0.1993 Acc:92.61%\n",
            "Training: Epoch[004/005] Iteration[2064/2674] Loss: 0.1163 Acc:92.62%\n",
            "Training: Epoch[004/005] Iteration[2072/2674] Loss: 0.3535 Acc:92.62%\n",
            "Training: Epoch[004/005] Iteration[2080/2674] Loss: 0.4910 Acc:92.62%\n",
            "Training: Epoch[004/005] Iteration[2088/2674] Loss: 0.1502 Acc:92.64%\n",
            "Training: Epoch[004/005] Iteration[2096/2674] Loss: 0.3354 Acc:92.63%\n",
            "Training: Epoch[004/005] Iteration[2104/2674] Loss: 0.1147 Acc:92.64%\n",
            "Training: Epoch[004/005] Iteration[2112/2674] Loss: 0.0413 Acc:92.66%\n",
            "Training: Epoch[004/005] Iteration[2120/2674] Loss: 0.3998 Acc:92.65%\n",
            "Training: Epoch[004/005] Iteration[2128/2674] Loss: 0.3672 Acc:92.65%\n",
            "Training: Epoch[004/005] Iteration[2136/2674] Loss: 0.3078 Acc:92.64%\n",
            "Training: Epoch[004/005] Iteration[2144/2674] Loss: 0.2238 Acc:92.64%\n",
            "Training: Epoch[004/005] Iteration[2152/2674] Loss: 0.3909 Acc:92.65%\n",
            "Training: Epoch[004/005] Iteration[2160/2674] Loss: 0.5050 Acc:92.62%\n",
            "Training: Epoch[004/005] Iteration[2168/2674] Loss: 0.0953 Acc:92.63%\n",
            "Training: Epoch[004/005] Iteration[2176/2674] Loss: 0.2442 Acc:92.62%\n",
            "Training: Epoch[004/005] Iteration[2184/2674] Loss: 0.2924 Acc:92.63%\n",
            "Training: Epoch[004/005] Iteration[2192/2674] Loss: 0.4182 Acc:92.60%\n",
            "Training: Epoch[004/005] Iteration[2200/2674] Loss: 0.4324 Acc:92.57%\n",
            "Training: Epoch[004/005] Iteration[2208/2674] Loss: 0.1342 Acc:92.57%\n",
            "Training: Epoch[004/005] Iteration[2216/2674] Loss: 0.0840 Acc:92.58%\n",
            "Training: Epoch[004/005] Iteration[2224/2674] Loss: 0.0719 Acc:92.60%\n",
            "Training: Epoch[004/005] Iteration[2232/2674] Loss: 0.4952 Acc:92.57%\n",
            "Training: Epoch[004/005] Iteration[2240/2674] Loss: 0.1956 Acc:92.58%\n",
            "Training: Epoch[004/005] Iteration[2248/2674] Loss: 0.1276 Acc:92.58%\n",
            "Training: Epoch[004/005] Iteration[2256/2674] Loss: 0.3418 Acc:92.57%\n",
            "Training: Epoch[004/005] Iteration[2264/2674] Loss: 0.3204 Acc:92.56%\n",
            "Training: Epoch[004/005] Iteration[2272/2674] Loss: 0.3816 Acc:92.56%\n",
            "Training: Epoch[004/005] Iteration[2280/2674] Loss: 0.2365 Acc:92.56%\n",
            "Training: Epoch[004/005] Iteration[2288/2674] Loss: 0.1156 Acc:92.57%\n",
            "Training: Epoch[004/005] Iteration[2296/2674] Loss: 0.7500 Acc:92.55%\n",
            "Training: Epoch[004/005] Iteration[2304/2674] Loss: 0.1449 Acc:92.56%\n",
            "Training: Epoch[004/005] Iteration[2312/2674] Loss: 0.2824 Acc:92.55%\n",
            "Training: Epoch[004/005] Iteration[2320/2674] Loss: 0.6533 Acc:92.54%\n",
            "Training: Epoch[004/005] Iteration[2328/2674] Loss: 0.5236 Acc:92.53%\n",
            "Training: Epoch[004/005] Iteration[2336/2674] Loss: 0.6287 Acc:92.51%\n",
            "Training: Epoch[004/005] Iteration[2344/2674] Loss: 0.3597 Acc:92.50%\n",
            "Training: Epoch[004/005] Iteration[2352/2674] Loss: 0.2298 Acc:92.49%\n",
            "Training: Epoch[004/005] Iteration[2360/2674] Loss: 0.2899 Acc:92.48%\n",
            "Training: Epoch[004/005] Iteration[2368/2674] Loss: 0.1913 Acc:92.49%\n",
            "Training: Epoch[004/005] Iteration[2376/2674] Loss: 0.1179 Acc:92.50%\n",
            "Training: Epoch[004/005] Iteration[2384/2674] Loss: 0.2485 Acc:92.50%\n",
            "Training: Epoch[004/005] Iteration[2392/2674] Loss: 0.1309 Acc:92.51%\n",
            "Training: Epoch[004/005] Iteration[2400/2674] Loss: 0.3384 Acc:92.52%\n",
            "Training: Epoch[004/005] Iteration[2408/2674] Loss: 0.1412 Acc:92.52%\n",
            "Training: Epoch[004/005] Iteration[2416/2674] Loss: 0.1166 Acc:92.53%\n",
            "Training: Epoch[004/005] Iteration[2424/2674] Loss: 0.3428 Acc:92.51%\n",
            "Training: Epoch[004/005] Iteration[2432/2674] Loss: 0.1643 Acc:92.52%\n",
            "Training: Epoch[004/005] Iteration[2440/2674] Loss: 0.0564 Acc:92.54%\n",
            "Training: Epoch[004/005] Iteration[2448/2674] Loss: 0.5056 Acc:92.53%\n",
            "Training: Epoch[004/005] Iteration[2456/2674] Loss: 0.3972 Acc:92.53%\n",
            "Training: Epoch[004/005] Iteration[2464/2674] Loss: 0.1618 Acc:92.53%\n",
            "Training: Epoch[004/005] Iteration[2472/2674] Loss: 0.0541 Acc:92.55%\n",
            "Training: Epoch[004/005] Iteration[2480/2674] Loss: 0.1063 Acc:92.55%\n",
            "Training: Epoch[004/005] Iteration[2488/2674] Loss: 0.0904 Acc:92.56%\n",
            "Training: Epoch[004/005] Iteration[2496/2674] Loss: 0.1576 Acc:92.58%\n",
            "Training: Epoch[004/005] Iteration[2504/2674] Loss: 0.0773 Acc:92.60%\n",
            "Training: Epoch[004/005] Iteration[2512/2674] Loss: 0.1413 Acc:92.61%\n",
            "Training: Epoch[004/005] Iteration[2520/2674] Loss: 0.2498 Acc:92.61%\n",
            "Training: Epoch[004/005] Iteration[2528/2674] Loss: 0.6385 Acc:92.60%\n",
            "Training: Epoch[004/005] Iteration[2536/2674] Loss: 0.0911 Acc:92.62%\n",
            "Training: Epoch[004/005] Iteration[2544/2674] Loss: 0.1399 Acc:92.63%\n",
            "Training: Epoch[004/005] Iteration[2552/2674] Loss: 0.1990 Acc:92.63%\n",
            "Training: Epoch[004/005] Iteration[2560/2674] Loss: 0.3296 Acc:92.62%\n",
            "Training: Epoch[004/005] Iteration[2568/2674] Loss: 0.2263 Acc:92.63%\n",
            "Training: Epoch[004/005] Iteration[2576/2674] Loss: 0.1221 Acc:92.64%\n",
            "Training: Epoch[004/005] Iteration[2584/2674] Loss: 0.3876 Acc:92.64%\n",
            "Training: Epoch[004/005] Iteration[2592/2674] Loss: 0.5576 Acc:92.63%\n",
            "Training: Epoch[004/005] Iteration[2600/2674] Loss: 0.3298 Acc:92.63%\n",
            "Training: Epoch[004/005] Iteration[2608/2674] Loss: 0.1733 Acc:92.64%\n",
            "Training: Epoch[004/005] Iteration[2616/2674] Loss: 0.3778 Acc:92.63%\n",
            "Training: Epoch[004/005] Iteration[2624/2674] Loss: 0.3703 Acc:92.64%\n",
            "Training: Epoch[004/005] Iteration[2632/2674] Loss: 0.1531 Acc:92.64%\n",
            "Training: Epoch[004/005] Iteration[2640/2674] Loss: 0.2285 Acc:92.65%\n",
            "Training: Epoch[004/005] Iteration[2648/2674] Loss: 0.2598 Acc:92.64%\n",
            "Training: Epoch[004/005] Iteration[2656/2674] Loss: 0.2305 Acc:92.62%\n",
            "Training: Epoch[004/005] Iteration[2664/2674] Loss: 0.3225 Acc:92.61%\n",
            "Training: Epoch[004/005] Iteration[2672/2674] Loss: 0.1910 Acc:92.63%\n",
            "Training: Epoch[005/005] Iteration[008/2674] Loss: 0.1379 Acc:95.31%\n",
            "Training: Epoch[005/005] Iteration[016/2674] Loss: 0.1352 Acc:94.53%\n",
            "Training: Epoch[005/005] Iteration[024/2674] Loss: 0.1841 Acc:94.27%\n",
            "Training: Epoch[005/005] Iteration[032/2674] Loss: 0.2908 Acc:93.75%\n",
            "Training: Epoch[005/005] Iteration[040/2674] Loss: 0.1152 Acc:94.38%\n",
            "Training: Epoch[005/005] Iteration[048/2674] Loss: 0.2361 Acc:94.01%\n",
            "Training: Epoch[005/005] Iteration[056/2674] Loss: 0.0638 Acc:94.64%\n",
            "Training: Epoch[005/005] Iteration[064/2674] Loss: 0.4581 Acc:94.92%\n",
            "Training: Epoch[005/005] Iteration[072/2674] Loss: 0.1792 Acc:94.27%\n",
            "Training: Epoch[005/005] Iteration[080/2674] Loss: 0.1923 Acc:94.53%\n",
            "Training: Epoch[005/005] Iteration[088/2674] Loss: 0.1434 Acc:94.60%\n",
            "Training: Epoch[005/005] Iteration[096/2674] Loss: 0.0873 Acc:94.79%\n",
            "Training: Epoch[005/005] Iteration[104/2674] Loss: 0.0199 Acc:95.19%\n",
            "Training: Epoch[005/005] Iteration[112/2674] Loss: 0.0356 Acc:95.31%\n",
            "Training: Epoch[005/005] Iteration[120/2674] Loss: 0.2424 Acc:95.31%\n",
            "Training: Epoch[005/005] Iteration[128/2674] Loss: 0.0918 Acc:95.31%\n",
            "Training: Epoch[005/005] Iteration[136/2674] Loss: 0.1567 Acc:95.31%\n",
            "Training: Epoch[005/005] Iteration[144/2674] Loss: 0.0888 Acc:95.40%\n",
            "Training: Epoch[005/005] Iteration[152/2674] Loss: 0.2214 Acc:95.31%\n",
            "Training: Epoch[005/005] Iteration[160/2674] Loss: 0.2358 Acc:95.31%\n",
            "Training: Epoch[005/005] Iteration[168/2674] Loss: 0.2494 Acc:95.16%\n",
            "Training: Epoch[005/005] Iteration[176/2674] Loss: 0.1191 Acc:95.17%\n",
            "Training: Epoch[005/005] Iteration[184/2674] Loss: 0.2347 Acc:95.11%\n",
            "Training: Epoch[005/005] Iteration[192/2674] Loss: 0.1965 Acc:94.99%\n",
            "Training: Epoch[005/005] Iteration[200/2674] Loss: 0.0810 Acc:95.06%\n",
            "Training: Epoch[005/005] Iteration[208/2674] Loss: 0.1091 Acc:95.07%\n",
            "Training: Epoch[005/005] Iteration[216/2674] Loss: 0.1145 Acc:95.14%\n",
            "Training: Epoch[005/005] Iteration[224/2674] Loss: 0.2502 Acc:95.09%\n",
            "Training: Epoch[005/005] Iteration[232/2674] Loss: 0.2199 Acc:95.10%\n",
            "Training: Epoch[005/005] Iteration[240/2674] Loss: 0.0559 Acc:95.16%\n",
            "Training: Epoch[005/005] Iteration[248/2674] Loss: 0.1772 Acc:95.11%\n",
            "Training: Epoch[005/005] Iteration[256/2674] Loss: 0.4386 Acc:95.12%\n",
            "Training: Epoch[005/005] Iteration[264/2674] Loss: 0.1396 Acc:95.17%\n",
            "Training: Epoch[005/005] Iteration[272/2674] Loss: 0.0679 Acc:95.27%\n",
            "Training: Epoch[005/005] Iteration[280/2674] Loss: 0.1488 Acc:95.22%\n",
            "Training: Epoch[005/005] Iteration[288/2674] Loss: 0.2163 Acc:95.18%\n",
            "Training: Epoch[005/005] Iteration[296/2674] Loss: 0.0489 Acc:95.27%\n",
            "Training: Epoch[005/005] Iteration[304/2674] Loss: 0.0583 Acc:95.31%\n",
            "Training: Epoch[005/005] Iteration[312/2674] Loss: 0.0893 Acc:95.35%\n",
            "Training: Epoch[005/005] Iteration[320/2674] Loss: 0.0783 Acc:95.39%\n",
            "Training: Epoch[005/005] Iteration[328/2674] Loss: 0.0644 Acc:95.43%\n",
            "Training: Epoch[005/005] Iteration[336/2674] Loss: 0.2080 Acc:95.39%\n",
            "Training: Epoch[005/005] Iteration[344/2674] Loss: 0.1728 Acc:95.35%\n",
            "Training: Epoch[005/005] Iteration[352/2674] Loss: 0.0772 Acc:95.38%\n",
            "Training: Epoch[005/005] Iteration[360/2674] Loss: 0.1126 Acc:95.38%\n",
            "Training: Epoch[005/005] Iteration[368/2674] Loss: 0.1499 Acc:95.38%\n",
            "Training: Epoch[005/005] Iteration[376/2674] Loss: 0.0480 Acc:95.45%\n",
            "Training: Epoch[005/005] Iteration[384/2674] Loss: 0.0506 Acc:95.51%\n",
            "Training: Epoch[005/005] Iteration[392/2674] Loss: 0.0851 Acc:95.50%\n",
            "Training: Epoch[005/005] Iteration[400/2674] Loss: 0.1991 Acc:95.44%\n",
            "Training: Epoch[005/005] Iteration[408/2674] Loss: 0.0828 Acc:95.47%\n",
            "Training: Epoch[005/005] Iteration[416/2674] Loss: 0.2591 Acc:95.43%\n",
            "Training: Epoch[005/005] Iteration[424/2674] Loss: 0.0607 Acc:95.46%\n",
            "Training: Epoch[005/005] Iteration[432/2674] Loss: 0.1677 Acc:95.40%\n",
            "Training: Epoch[005/005] Iteration[440/2674] Loss: 0.1509 Acc:95.43%\n",
            "Training: Epoch[005/005] Iteration[448/2674] Loss: 0.2652 Acc:95.40%\n",
            "Training: Epoch[005/005] Iteration[456/2674] Loss: 0.1347 Acc:95.37%\n",
            "Training: Epoch[005/005] Iteration[464/2674] Loss: 0.0994 Acc:95.37%\n",
            "Training: Epoch[005/005] Iteration[472/2674] Loss: 0.1379 Acc:95.39%\n",
            "Training: Epoch[005/005] Iteration[480/2674] Loss: 0.0768 Acc:95.44%\n",
            "Training: Epoch[005/005] Iteration[488/2674] Loss: 0.1172 Acc:95.47%\n",
            "Training: Epoch[005/005] Iteration[496/2674] Loss: 0.0104 Acc:95.54%\n",
            "Training: Epoch[005/005] Iteration[504/2674] Loss: 0.2646 Acc:95.49%\n",
            "Training: Epoch[005/005] Iteration[512/2674] Loss: 0.0752 Acc:95.51%\n",
            "Training: Epoch[005/005] Iteration[520/2674] Loss: 0.0247 Acc:95.58%\n",
            "Training: Epoch[005/005] Iteration[528/2674] Loss: 0.0812 Acc:95.57%\n",
            "Training: Epoch[005/005] Iteration[536/2674] Loss: 0.2106 Acc:95.57%\n",
            "Training: Epoch[005/005] Iteration[544/2674] Loss: 0.0584 Acc:95.61%\n",
            "Training: Epoch[005/005] Iteration[552/2674] Loss: 0.0031 Acc:95.67%\n",
            "Training: Epoch[005/005] Iteration[560/2674] Loss: 0.0322 Acc:95.74%\n",
            "Training: Epoch[005/005] Iteration[568/2674] Loss: 0.2421 Acc:95.73%\n",
            "Training: Epoch[005/005] Iteration[576/2674] Loss: 0.0410 Acc:95.79%\n",
            "Training: Epoch[005/005] Iteration[584/2674] Loss: 0.0242 Acc:95.85%\n",
            "Training: Epoch[005/005] Iteration[592/2674] Loss: 0.1299 Acc:95.88%\n",
            "Training: Epoch[005/005] Iteration[600/2674] Loss: 0.0310 Acc:95.94%\n",
            "Training: Epoch[005/005] Iteration[608/2674] Loss: 0.1081 Acc:95.93%\n",
            "Training: Epoch[005/005] Iteration[616/2674] Loss: 0.0964 Acc:95.92%\n",
            "Training: Epoch[005/005] Iteration[624/2674] Loss: 0.0640 Acc:95.93%\n",
            "Training: Epoch[005/005] Iteration[632/2674] Loss: 0.0260 Acc:95.97%\n",
            "Training: Epoch[005/005] Iteration[640/2674] Loss: 0.1898 Acc:95.96%\n",
            "Training: Epoch[005/005] Iteration[648/2674] Loss: 0.0318 Acc:95.99%\n",
            "Training: Epoch[005/005] Iteration[656/2674] Loss: 0.0571 Acc:96.02%\n",
            "Training: Epoch[005/005] Iteration[664/2674] Loss: 0.0754 Acc:96.03%\n",
            "Training: Epoch[005/005] Iteration[672/2674] Loss: 0.1502 Acc:96.04%\n",
            "Training: Epoch[005/005] Iteration[680/2674] Loss: 0.1415 Acc:96.05%\n",
            "Training: Epoch[005/005] Iteration[688/2674] Loss: 0.0534 Acc:96.08%\n",
            "Training: Epoch[005/005] Iteration[696/2674] Loss: 0.0516 Acc:96.10%\n",
            "Training: Epoch[005/005] Iteration[704/2674] Loss: 0.1031 Acc:96.11%\n",
            "Training: Epoch[005/005] Iteration[712/2674] Loss: 0.1487 Acc:96.10%\n",
            "Training: Epoch[005/005] Iteration[720/2674] Loss: 0.0259 Acc:96.13%\n",
            "Training: Epoch[005/005] Iteration[728/2674] Loss: 0.0657 Acc:96.15%\n",
            "Training: Epoch[005/005] Iteration[736/2674] Loss: 0.0854 Acc:96.14%\n",
            "Training: Epoch[005/005] Iteration[744/2674] Loss: 0.0745 Acc:96.15%\n",
            "Training: Epoch[005/005] Iteration[752/2674] Loss: 0.1811 Acc:96.16%\n",
            "Training: Epoch[005/005] Iteration[760/2674] Loss: 0.1093 Acc:96.18%\n",
            "Training: Epoch[005/005] Iteration[768/2674] Loss: 0.0649 Acc:96.21%\n",
            "Training: Epoch[005/005] Iteration[776/2674] Loss: 0.1200 Acc:96.21%\n",
            "Training: Epoch[005/005] Iteration[784/2674] Loss: 0.1579 Acc:96.21%\n",
            "Training: Epoch[005/005] Iteration[792/2674] Loss: 0.0210 Acc:96.23%\n",
            "Training: Epoch[005/005] Iteration[800/2674] Loss: 0.1407 Acc:96.20%\n",
            "Training: Epoch[005/005] Iteration[808/2674] Loss: 0.0730 Acc:96.21%\n",
            "Training: Epoch[005/005] Iteration[816/2674] Loss: 0.1188 Acc:96.22%\n",
            "Training: Epoch[005/005] Iteration[824/2674] Loss: 0.1140 Acc:96.18%\n",
            "Training: Epoch[005/005] Iteration[832/2674] Loss: 0.0815 Acc:96.18%\n",
            "Training: Epoch[005/005] Iteration[840/2674] Loss: 0.0759 Acc:96.21%\n",
            "Training: Epoch[005/005] Iteration[848/2674] Loss: 0.2230 Acc:96.18%\n",
            "Training: Epoch[005/005] Iteration[856/2674] Loss: 0.1230 Acc:96.19%\n",
            "Training: Epoch[005/005] Iteration[864/2674] Loss: 0.0969 Acc:96.21%\n",
            "Training: Epoch[005/005] Iteration[872/2674] Loss: 0.0660 Acc:96.22%\n",
            "Training: Epoch[005/005] Iteration[880/2674] Loss: 0.1982 Acc:96.22%\n",
            "Training: Epoch[005/005] Iteration[888/2674] Loss: 0.0907 Acc:96.21%\n",
            "Training: Epoch[005/005] Iteration[896/2674] Loss: 0.1282 Acc:96.21%\n",
            "Training: Epoch[005/005] Iteration[904/2674] Loss: 0.0441 Acc:96.23%\n",
            "Training: Epoch[005/005] Iteration[912/2674] Loss: 0.0486 Acc:96.24%\n",
            "Training: Epoch[005/005] Iteration[920/2674] Loss: 0.0807 Acc:96.26%\n",
            "Training: Epoch[005/005] Iteration[928/2674] Loss: 0.0922 Acc:96.27%\n",
            "Training: Epoch[005/005] Iteration[936/2674] Loss: 0.0912 Acc:96.26%\n",
            "Training: Epoch[005/005] Iteration[944/2674] Loss: 0.2305 Acc:96.24%\n",
            "Training: Epoch[005/005] Iteration[952/2674] Loss: 0.0610 Acc:96.26%\n",
            "Training: Epoch[005/005] Iteration[960/2674] Loss: 0.0692 Acc:96.26%\n",
            "Training: Epoch[005/005] Iteration[968/2674] Loss: 0.1190 Acc:96.24%\n",
            "Training: Epoch[005/005] Iteration[976/2674] Loss: 0.0196 Acc:96.27%\n",
            "Training: Epoch[005/005] Iteration[984/2674] Loss: 0.0731 Acc:96.28%\n",
            "Training: Epoch[005/005] Iteration[992/2674] Loss: 0.0668 Acc:96.30%\n",
            "Training: Epoch[005/005] Iteration[1000/2674] Loss: 0.1873 Acc:96.26%\n",
            "Training: Epoch[005/005] Iteration[1008/2674] Loss: 0.0222 Acc:96.28%\n",
            "Training: Epoch[005/005] Iteration[1016/2674] Loss: 0.0067 Acc:96.31%\n",
            "Training: Epoch[005/005] Iteration[1024/2674] Loss: 0.0507 Acc:96.33%\n",
            "Training: Epoch[005/005] Iteration[1032/2674] Loss: 0.1197 Acc:96.33%\n",
            "Training: Epoch[005/005] Iteration[1040/2674] Loss: 0.0677 Acc:96.35%\n",
            "Training: Epoch[005/005] Iteration[1048/2674] Loss: 0.0265 Acc:96.36%\n",
            "Training: Epoch[005/005] Iteration[1056/2674] Loss: 0.0557 Acc:96.37%\n",
            "Training: Epoch[005/005] Iteration[1064/2674] Loss: 0.1382 Acc:96.38%\n",
            "Training: Epoch[005/005] Iteration[1072/2674] Loss: 0.0332 Acc:96.41%\n",
            "Training: Epoch[005/005] Iteration[1080/2674] Loss: 0.0271 Acc:96.42%\n",
            "Training: Epoch[005/005] Iteration[1088/2674] Loss: 0.1829 Acc:96.42%\n",
            "Training: Epoch[005/005] Iteration[1096/2674] Loss: 0.1852 Acc:96.41%\n",
            "Training: Epoch[005/005] Iteration[1104/2674] Loss: 0.0603 Acc:96.41%\n",
            "Training: Epoch[005/005] Iteration[1112/2674] Loss: 0.0359 Acc:96.43%\n",
            "Training: Epoch[005/005] Iteration[1120/2674] Loss: 0.0677 Acc:96.44%\n",
            "Training: Epoch[005/005] Iteration[1128/2674] Loss: 0.0237 Acc:96.46%\n",
            "Training: Epoch[005/005] Iteration[1136/2674] Loss: 0.0070 Acc:96.49%\n",
            "Training: Epoch[005/005] Iteration[1144/2674] Loss: 0.0263 Acc:96.50%\n",
            "Training: Epoch[005/005] Iteration[1152/2674] Loss: 0.0352 Acc:96.53%\n",
            "Training: Epoch[005/005] Iteration[1160/2674] Loss: 0.0701 Acc:96.53%\n",
            "Training: Epoch[005/005] Iteration[1168/2674] Loss: 0.1759 Acc:96.54%\n",
            "Training: Epoch[005/005] Iteration[1176/2674] Loss: 0.0397 Acc:96.56%\n",
            "Training: Epoch[005/005] Iteration[1184/2674] Loss: 0.2564 Acc:96.54%\n",
            "Training: Epoch[005/005] Iteration[1192/2674] Loss: 0.0112 Acc:96.56%\n",
            "Training: Epoch[005/005] Iteration[1200/2674] Loss: 0.0508 Acc:96.57%\n",
            "Training: Epoch[005/005] Iteration[1208/2674] Loss: 0.1405 Acc:96.57%\n",
            "Training: Epoch[005/005] Iteration[1216/2674] Loss: 0.2405 Acc:96.56%\n",
            "Training: Epoch[005/005] Iteration[1224/2674] Loss: 0.0731 Acc:96.56%\n",
            "Training: Epoch[005/005] Iteration[1232/2674] Loss: 0.0448 Acc:96.57%\n",
            "Training: Epoch[005/005] Iteration[1240/2674] Loss: 0.0962 Acc:96.57%\n",
            "Training: Epoch[005/005] Iteration[1248/2674] Loss: 0.0550 Acc:96.58%\n",
            "Training: Epoch[005/005] Iteration[1256/2674] Loss: 0.0115 Acc:96.61%\n",
            "Training: Epoch[005/005] Iteration[1264/2674] Loss: 0.0240 Acc:96.62%\n",
            "Training: Epoch[005/005] Iteration[1272/2674] Loss: 0.0374 Acc:96.63%\n",
            "Training: Epoch[005/005] Iteration[1280/2674] Loss: 0.0123 Acc:96.65%\n",
            "Training: Epoch[005/005] Iteration[1288/2674] Loss: 0.1782 Acc:96.64%\n",
            "Training: Epoch[005/005] Iteration[1296/2674] Loss: 0.1968 Acc:96.63%\n",
            "Training: Epoch[005/005] Iteration[1304/2674] Loss: 0.0195 Acc:96.65%\n",
            "Training: Epoch[005/005] Iteration[1312/2674] Loss: 0.0521 Acc:96.67%\n",
            "Training: Epoch[005/005] Iteration[1320/2674] Loss: 0.3082 Acc:96.66%\n",
            "Training: Epoch[005/005] Iteration[1328/2674] Loss: 0.1391 Acc:96.66%\n",
            "Training: Epoch[005/005] Iteration[1336/2674] Loss: 0.2405 Acc:96.61%\n",
            "Training: Epoch[005/005] Iteration[1344/2674] Loss: 0.2261 Acc:96.60%\n",
            "Training: Epoch[005/005] Iteration[1352/2674] Loss: 0.0957 Acc:96.59%\n",
            "Training: Epoch[005/005] Iteration[1360/2674] Loss: 0.0870 Acc:96.59%\n",
            "Training: Epoch[005/005] Iteration[1368/2674] Loss: 0.0950 Acc:96.58%\n",
            "Training: Epoch[005/005] Iteration[1376/2674] Loss: 0.1695 Acc:96.58%\n",
            "Training: Epoch[005/005] Iteration[1384/2674] Loss: 0.0444 Acc:96.60%\n",
            "Training: Epoch[005/005] Iteration[1392/2674] Loss: 0.0196 Acc:96.61%\n",
            "Training: Epoch[005/005] Iteration[1400/2674] Loss: 0.0183 Acc:96.62%\n",
            "Training: Epoch[005/005] Iteration[1408/2674] Loss: 0.0229 Acc:96.64%\n",
            "Training: Epoch[005/005] Iteration[1416/2674] Loss: 0.0997 Acc:96.64%\n",
            "Training: Epoch[005/005] Iteration[1424/2674] Loss: 0.0717 Acc:96.65%\n",
            "Training: Epoch[005/005] Iteration[1432/2674] Loss: 0.2533 Acc:96.65%\n",
            "Training: Epoch[005/005] Iteration[1440/2674] Loss: 0.2939 Acc:96.65%\n",
            "Training: Epoch[005/005] Iteration[1448/2674] Loss: 0.1154 Acc:96.63%\n",
            "Training: Epoch[005/005] Iteration[1456/2674] Loss: 0.0673 Acc:96.64%\n",
            "Training: Epoch[005/005] Iteration[1464/2674] Loss: 0.1175 Acc:96.65%\n",
            "Training: Epoch[005/005] Iteration[1472/2674] Loss: 0.2047 Acc:96.64%\n",
            "Training: Epoch[005/005] Iteration[1480/2674] Loss: 0.0758 Acc:96.63%\n",
            "Training: Epoch[005/005] Iteration[1488/2674] Loss: 0.0382 Acc:96.64%\n",
            "Training: Epoch[005/005] Iteration[1496/2674] Loss: 0.1133 Acc:96.63%\n",
            "Training: Epoch[005/005] Iteration[1504/2674] Loss: 0.0688 Acc:96.64%\n",
            "Training: Epoch[005/005] Iteration[1512/2674] Loss: 0.1348 Acc:96.65%\n",
            "Training: Epoch[005/005] Iteration[1520/2674] Loss: 0.0834 Acc:96.66%\n",
            "Training: Epoch[005/005] Iteration[1528/2674] Loss: 0.1668 Acc:96.65%\n",
            "Training: Epoch[005/005] Iteration[1536/2674] Loss: 0.0133 Acc:96.67%\n",
            "Training: Epoch[005/005] Iteration[1544/2674] Loss: 0.0816 Acc:96.67%\n",
            "Training: Epoch[005/005] Iteration[1552/2674] Loss: 0.0960 Acc:96.67%\n",
            "Training: Epoch[005/005] Iteration[1560/2674] Loss: 0.0908 Acc:96.67%\n",
            "Training: Epoch[005/005] Iteration[1568/2674] Loss: 0.0348 Acc:96.68%\n",
            "Training: Epoch[005/005] Iteration[1576/2674] Loss: 0.2311 Acc:96.68%\n",
            "Training: Epoch[005/005] Iteration[1584/2674] Loss: 0.0215 Acc:96.70%\n",
            "Training: Epoch[005/005] Iteration[1592/2674] Loss: 0.0127 Acc:96.72%\n",
            "Training: Epoch[005/005] Iteration[1600/2674] Loss: 0.0959 Acc:96.73%\n",
            "Training: Epoch[005/005] Iteration[1608/2674] Loss: 0.0993 Acc:96.72%\n",
            "Training: Epoch[005/005] Iteration[1616/2674] Loss: 0.0567 Acc:96.72%\n",
            "Training: Epoch[005/005] Iteration[1624/2674] Loss: 0.1670 Acc:96.71%\n",
            "Training: Epoch[005/005] Iteration[1632/2674] Loss: 0.0298 Acc:96.72%\n",
            "Training: Epoch[005/005] Iteration[1640/2674] Loss: 0.0666 Acc:96.71%\n",
            "Training: Epoch[005/005] Iteration[1648/2674] Loss: 0.2330 Acc:96.70%\n",
            "Training: Epoch[005/005] Iteration[1656/2674] Loss: 0.0712 Acc:96.70%\n",
            "Training: Epoch[005/005] Iteration[1664/2674] Loss: 0.2485 Acc:96.68%\n",
            "Training: Epoch[005/005] Iteration[1672/2674] Loss: 0.1672 Acc:96.67%\n",
            "Training: Epoch[005/005] Iteration[1680/2674] Loss: 0.0253 Acc:96.67%\n",
            "Training: Epoch[005/005] Iteration[1688/2674] Loss: 0.0477 Acc:96.68%\n",
            "Training: Epoch[005/005] Iteration[1696/2674] Loss: 0.0208 Acc:96.69%\n",
            "Training: Epoch[005/005] Iteration[1704/2674] Loss: 0.0017 Acc:96.71%\n",
            "Training: Epoch[005/005] Iteration[1712/2674] Loss: 0.0936 Acc:96.71%\n",
            "Training: Epoch[005/005] Iteration[1720/2674] Loss: 0.0156 Acc:96.72%\n",
            "Training: Epoch[005/005] Iteration[1728/2674] Loss: 0.0741 Acc:96.72%\n",
            "Training: Epoch[005/005] Iteration[1736/2674] Loss: 0.1062 Acc:96.73%\n",
            "Training: Epoch[005/005] Iteration[1744/2674] Loss: 0.0060 Acc:96.75%\n",
            "Training: Epoch[005/005] Iteration[1752/2674] Loss: 0.1876 Acc:96.74%\n",
            "Training: Epoch[005/005] Iteration[1760/2674] Loss: 0.1191 Acc:96.75%\n",
            "Training: Epoch[005/005] Iteration[1768/2674] Loss: 0.0864 Acc:96.75%\n",
            "Training: Epoch[005/005] Iteration[1776/2674] Loss: 0.1089 Acc:96.76%\n",
            "Training: Epoch[005/005] Iteration[1784/2674] Loss: 0.0322 Acc:96.76%\n",
            "Training: Epoch[005/005] Iteration[1792/2674] Loss: 0.1507 Acc:96.76%\n",
            "Training: Epoch[005/005] Iteration[1800/2674] Loss: 0.0374 Acc:96.78%\n",
            "Training: Epoch[005/005] Iteration[1808/2674] Loss: 0.1794 Acc:96.76%\n",
            "Training: Epoch[005/005] Iteration[1816/2674] Loss: 0.0927 Acc:96.76%\n",
            "Training: Epoch[005/005] Iteration[1824/2674] Loss: 0.1532 Acc:96.75%\n",
            "Training: Epoch[005/005] Iteration[1832/2674] Loss: 0.1687 Acc:96.75%\n",
            "Training: Epoch[005/005] Iteration[1840/2674] Loss: 0.1924 Acc:96.73%\n",
            "Training: Epoch[005/005] Iteration[1848/2674] Loss: 0.1271 Acc:96.71%\n",
            "Training: Epoch[005/005] Iteration[1856/2674] Loss: 0.1135 Acc:96.71%\n",
            "Training: Epoch[005/005] Iteration[1864/2674] Loss: 0.1653 Acc:96.70%\n",
            "Training: Epoch[005/005] Iteration[1872/2674] Loss: 0.1431 Acc:96.71%\n",
            "Training: Epoch[005/005] Iteration[1880/2674] Loss: 0.1329 Acc:96.71%\n",
            "Training: Epoch[005/005] Iteration[1888/2674] Loss: 0.0197 Acc:96.72%\n",
            "Training: Epoch[005/005] Iteration[1896/2674] Loss: 0.0632 Acc:96.72%\n",
            "Training: Epoch[005/005] Iteration[1904/2674] Loss: 0.0106 Acc:96.73%\n",
            "Training: Epoch[005/005] Iteration[1912/2674] Loss: 0.0818 Acc:96.73%\n",
            "Training: Epoch[005/005] Iteration[1920/2674] Loss: 0.1460 Acc:96.73%\n",
            "Training: Epoch[005/005] Iteration[1928/2674] Loss: 0.0728 Acc:96.73%\n",
            "Training: Epoch[005/005] Iteration[1936/2674] Loss: 0.1393 Acc:96.73%\n",
            "Training: Epoch[005/005] Iteration[1944/2674] Loss: 0.1308 Acc:96.73%\n",
            "Training: Epoch[005/005] Iteration[1952/2674] Loss: 0.1509 Acc:96.72%\n",
            "Training: Epoch[005/005] Iteration[1960/2674] Loss: 0.1914 Acc:96.70%\n",
            "Training: Epoch[005/005] Iteration[1968/2674] Loss: 0.2546 Acc:96.70%\n",
            "Training: Epoch[005/005] Iteration[1976/2674] Loss: 0.0634 Acc:96.69%\n",
            "Training: Epoch[005/005] Iteration[1984/2674] Loss: 0.1139 Acc:96.70%\n",
            "Training: Epoch[005/005] Iteration[1992/2674] Loss: 0.1529 Acc:96.69%\n",
            "Training: Epoch[005/005] Iteration[2000/2674] Loss: 0.0300 Acc:96.70%\n",
            "Training: Epoch[005/005] Iteration[2008/2674] Loss: 0.0892 Acc:96.71%\n",
            "Training: Epoch[005/005] Iteration[2016/2674] Loss: 0.0352 Acc:96.71%\n",
            "Training: Epoch[005/005] Iteration[2024/2674] Loss: 0.0279 Acc:96.72%\n",
            "Training: Epoch[005/005] Iteration[2032/2674] Loss: 0.0112 Acc:96.73%\n",
            "Training: Epoch[005/005] Iteration[2040/2674] Loss: 0.0713 Acc:96.73%\n",
            "Training: Epoch[005/005] Iteration[2048/2674] Loss: 0.0829 Acc:96.74%\n",
            "Training: Epoch[005/005] Iteration[2056/2674] Loss: 0.1565 Acc:96.75%\n",
            "Training: Epoch[005/005] Iteration[2064/2674] Loss: 0.0333 Acc:96.75%\n",
            "Training: Epoch[005/005] Iteration[2072/2674] Loss: 0.0250 Acc:96.76%\n",
            "Training: Epoch[005/005] Iteration[2080/2674] Loss: 0.3848 Acc:96.74%\n",
            "Training: Epoch[005/005] Iteration[2088/2674] Loss: 0.0490 Acc:96.74%\n",
            "Training: Epoch[005/005] Iteration[2096/2674] Loss: 0.1596 Acc:96.74%\n",
            "Training: Epoch[005/005] Iteration[2104/2674] Loss: 0.0966 Acc:96.74%\n",
            "Training: Epoch[005/005] Iteration[2112/2674] Loss: 0.1424 Acc:96.74%\n",
            "Training: Epoch[005/005] Iteration[2120/2674] Loss: 0.0271 Acc:96.75%\n",
            "Training: Epoch[005/005] Iteration[2128/2674] Loss: 0.0414 Acc:96.75%\n",
            "Training: Epoch[005/005] Iteration[2136/2674] Loss: 0.0735 Acc:96.75%\n",
            "Training: Epoch[005/005] Iteration[2144/2674] Loss: 0.1919 Acc:96.74%\n",
            "Training: Epoch[005/005] Iteration[2152/2674] Loss: 0.0704 Acc:96.74%\n",
            "Training: Epoch[005/005] Iteration[2160/2674] Loss: 0.0465 Acc:96.75%\n",
            "Training: Epoch[005/005] Iteration[2168/2674] Loss: 0.2014 Acc:96.74%\n",
            "Training: Epoch[005/005] Iteration[2176/2674] Loss: 0.1614 Acc:96.73%\n",
            "Training: Epoch[005/005] Iteration[2184/2674] Loss: 0.0314 Acc:96.74%\n",
            "Training: Epoch[005/005] Iteration[2192/2674] Loss: 0.0076 Acc:96.75%\n",
            "Training: Epoch[005/005] Iteration[2200/2674] Loss: 0.0566 Acc:96.76%\n",
            "Training: Epoch[005/005] Iteration[2208/2674] Loss: 0.0817 Acc:96.76%\n",
            "Training: Epoch[005/005] Iteration[2216/2674] Loss: 0.0964 Acc:96.75%\n",
            "Training: Epoch[005/005] Iteration[2224/2674] Loss: 0.1529 Acc:96.75%\n",
            "Training: Epoch[005/005] Iteration[2232/2674] Loss: 0.1429 Acc:96.75%\n",
            "Training: Epoch[005/005] Iteration[2240/2674] Loss: 0.0408 Acc:96.76%\n",
            "Training: Epoch[005/005] Iteration[2248/2674] Loss: 0.3046 Acc:96.76%\n",
            "Training: Epoch[005/005] Iteration[2256/2674] Loss: 0.0354 Acc:96.76%\n",
            "Training: Epoch[005/005] Iteration[2264/2674] Loss: 0.0888 Acc:96.76%\n",
            "Training: Epoch[005/005] Iteration[2272/2674] Loss: 0.1347 Acc:96.76%\n",
            "Training: Epoch[005/005] Iteration[2280/2674] Loss: 0.0208 Acc:96.77%\n",
            "Training: Epoch[005/005] Iteration[2288/2674] Loss: 0.0417 Acc:96.77%\n",
            "Training: Epoch[005/005] Iteration[2296/2674] Loss: 0.1062 Acc:96.77%\n",
            "Training: Epoch[005/005] Iteration[2304/2674] Loss: 0.0850 Acc:96.78%\n",
            "Training: Epoch[005/005] Iteration[2312/2674] Loss: 0.0942 Acc:96.78%\n",
            "Training: Epoch[005/005] Iteration[2320/2674] Loss: 0.1713 Acc:96.78%\n",
            "Training: Epoch[005/005] Iteration[2328/2674] Loss: 0.0079 Acc:96.79%\n",
            "Training: Epoch[005/005] Iteration[2336/2674] Loss: 0.3516 Acc:96.78%\n",
            "Training: Epoch[005/005] Iteration[2344/2674] Loss: 0.0691 Acc:96.78%\n",
            "Training: Epoch[005/005] Iteration[2352/2674] Loss: 0.2236 Acc:96.77%\n",
            "Training: Epoch[005/005] Iteration[2360/2674] Loss: 0.2353 Acc:96.77%\n",
            "Training: Epoch[005/005] Iteration[2368/2674] Loss: 0.0425 Acc:96.77%\n",
            "Training: Epoch[005/005] Iteration[2376/2674] Loss: 0.2000 Acc:96.76%\n",
            "Training: Epoch[005/005] Iteration[2384/2674] Loss: 0.0906 Acc:96.77%\n",
            "Training: Epoch[005/005] Iteration[2392/2674] Loss: 0.0254 Acc:96.78%\n",
            "Training: Epoch[005/005] Iteration[2400/2674] Loss: 0.1723 Acc:96.79%\n",
            "Training: Epoch[005/005] Iteration[2408/2674] Loss: 0.0138 Acc:96.80%\n",
            "Training: Epoch[005/005] Iteration[2416/2674] Loss: 0.0242 Acc:96.80%\n",
            "Training: Epoch[005/005] Iteration[2424/2674] Loss: 0.0481 Acc:96.81%\n",
            "Training: Epoch[005/005] Iteration[2432/2674] Loss: 0.0285 Acc:96.81%\n",
            "Training: Epoch[005/005] Iteration[2440/2674] Loss: 0.0272 Acc:96.82%\n",
            "Training: Epoch[005/005] Iteration[2448/2674] Loss: 0.2608 Acc:96.80%\n",
            "Training: Epoch[005/005] Iteration[2456/2674] Loss: 0.0896 Acc:96.80%\n",
            "Training: Epoch[005/005] Iteration[2464/2674] Loss: 0.1407 Acc:96.80%\n",
            "Training: Epoch[005/005] Iteration[2472/2674] Loss: 0.2159 Acc:96.79%\n",
            "Training: Epoch[005/005] Iteration[2480/2674] Loss: 0.1072 Acc:96.79%\n",
            "Training: Epoch[005/005] Iteration[2488/2674] Loss: 0.0261 Acc:96.79%\n",
            "Training: Epoch[005/005] Iteration[2496/2674] Loss: 0.1864 Acc:96.79%\n",
            "Training: Epoch[005/005] Iteration[2504/2674] Loss: 0.0305 Acc:96.80%\n",
            "Training: Epoch[005/005] Iteration[2512/2674] Loss: 0.0870 Acc:96.80%\n",
            "Training: Epoch[005/005] Iteration[2520/2674] Loss: 0.0017 Acc:96.81%\n",
            "Training: Epoch[005/005] Iteration[2528/2674] Loss: 0.1168 Acc:96.81%\n",
            "Training: Epoch[005/005] Iteration[2536/2674] Loss: 0.1656 Acc:96.80%\n",
            "Training: Epoch[005/005] Iteration[2544/2674] Loss: 0.0862 Acc:96.81%\n",
            "Training: Epoch[005/005] Iteration[2552/2674] Loss: 0.0153 Acc:96.81%\n",
            "Training: Epoch[005/005] Iteration[2560/2674] Loss: 0.2173 Acc:96.81%\n",
            "Training: Epoch[005/005] Iteration[2568/2674] Loss: 0.0272 Acc:96.81%\n",
            "Training: Epoch[005/005] Iteration[2576/2674] Loss: 0.2226 Acc:96.81%\n",
            "Training: Epoch[005/005] Iteration[2584/2674] Loss: 0.1003 Acc:96.81%\n",
            "Training: Epoch[005/005] Iteration[2592/2674] Loss: 0.1143 Acc:96.80%\n",
            "Training: Epoch[005/005] Iteration[2600/2674] Loss: 0.2932 Acc:96.80%\n",
            "Training: Epoch[005/005] Iteration[2608/2674] Loss: 0.0487 Acc:96.80%\n",
            "Training: Epoch[005/005] Iteration[2616/2674] Loss: 0.1224 Acc:96.80%\n",
            "Training: Epoch[005/005] Iteration[2624/2674] Loss: 0.0052 Acc:96.81%\n",
            "Training: Epoch[005/005] Iteration[2632/2674] Loss: 0.0638 Acc:96.82%\n",
            "Training: Epoch[005/005] Iteration[2640/2674] Loss: 0.0749 Acc:96.82%\n",
            "Training: Epoch[005/005] Iteration[2648/2674] Loss: 0.2301 Acc:96.81%\n",
            "Training: Epoch[005/005] Iteration[2656/2674] Loss: 0.0205 Acc:96.82%\n",
            "Training: Epoch[005/005] Iteration[2664/2674] Loss: 0.0898 Acc:96.81%\n",
            "Training: Epoch[005/005] Iteration[2672/2674] Loss: 0.0975 Acc:96.80%\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------- Save Model and Plot Confusion Matrix -------------------------\n",
        "# Save the trained model\n",
        "cnn_save_path = os.path.join(log_dir, 'net_params.pkl')\n",
        "torch.save(cnn.state_dict(), cnn_save_path)\n",
        "\n",
        "# Validate on training and testing sets and obtain confusion matrices\n",
        "conf_mat_train, train_acc = validate(cnn, train_loader, 'train', classes_name)\n",
        "#conf_mat_valid, valid_acc = validate(cnn, test_loader, 'test', classes_name)\n",
        "\n",
        "# Plot and save confusion matrices\n",
        "show_confMat(conf_mat_train, classes_name, 'train', log_dir)\n",
        "#show_confMat(conf_mat_valid, classes_name, 'valid', log_dir)\n"
      ],
      "metadata": {
        "id": "iIpnsu9LmQI2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4dbbf39-6362-4c67-868d-d1496d0055e2"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "class:0         , total num:3316.0, correct num:3308.0  Recall: 99.73% Precision: 99.76%\n",
            "class:1         , total num:9325.0, correct num:9323.0  Recall: 99.97% Precision: 99.79%\n",
            "class:2         , total num:1050.0, correct num:1031.0  Recall: 98.10% Precision: 98.47%\n",
            "class:3         , total num:1532.0, correct num:1524.0  Recall: 99.41% Precision: 99.74%\n",
            "class:4         , total num:673.0 , correct num:673.0  Recall: 99.85% Precision: 99.85%\n",
            "class:5         , total num:2515.0, correct num:2505.0  Recall: 99.56% Precision: 99.84%\n",
            "class:6         , total num:665.0 , correct num:648.0  Recall: 97.30% Precision: 98.93%\n",
            "class:7         , total num:1841.0, correct num:1836.0  Recall: 99.67% Precision: 99.73%\n",
            "class:8         , total num:474.0 , correct num:474.0  Recall: 99.79% Precision: 97.33%\n",
            "train set Accuracy:99.68%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-ddeaf4e06936>:50: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n",
            "  cmap = plt.cm.get_cmap('Greys')\n"
          ]
        }
      ]
    }
  ]
}